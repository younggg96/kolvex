"""
å°çº¢ä¹¦å¸–å­ API è·¯ç”±
èŽ·å–çˆ¬å–çš„å¸–å­æ•°æ®ï¼ˆåŒ…å«å®Œæ•´åª’ä½“å’Œ AI åˆ†æžï¼‰
"""

from fastapi import APIRouter, HTTPException, Query
from typing import Dict, List, Optional
from datetime import datetime, timezone, timedelta

from app.services.xiaohongshu import get_supabase_client

router = APIRouter()


def _format_post(post: Dict) -> Dict:
    """
    æ ¼å¼åŒ–å•ä¸ªå¸–å­æ•°æ®ï¼Œç¡®ä¿è¿”å›žå®Œæ•´ç»“æž„
    """

    # å¤„ç† JSONB å­—æ®µ
    def parse_jsonb(value):
        if value is None:
            return []
        if isinstance(value, str):
            import json

            try:
                return json.loads(value)
            except:
                return []
        return value if isinstance(value, list) else []

    return {
        # === åŸºç¡€ä¿¡æ¯ ===
        "id": post.get("id"),
        "note_id": post.get("note_id"),
        "post_hash": post.get("post_hash"),
        "title": post.get("title"),
        "content": post.get("content"),
        "note_type": post.get("note_type", "normal"),
        "permalink": post.get("permalink"),
        # === ä½œè€…ä¿¡æ¯ ===
        "author_name": post.get("author_name"),
        "author_id": post.get("author_id"),
        "author_avatar": post.get("author_avatar"),
        # === åª’ä½“èµ„æº ===
        "cover_url": post.get("cover_url"),
        "image_urls": parse_jsonb(post.get("image_urls")),
        "video_url": post.get("video_url"),
        # === äº’åŠ¨æ•°æ® ===
        "like_count": post.get("like_count", 0),
        "collect_count": post.get("collect_count", 0),
        "comment_count": post.get("comment_count", 0),
        "share_count": post.get("share_count", 0),
        # === æ ‡ç­¾ ===
        "tags": parse_jsonb(post.get("tags")),
        "search_keyword": post.get("search_keyword"),
        # === AI åˆ†æžç»“æžœ ===
        "ai_sentiment": post.get("ai_sentiment"),
        "ai_sentiment_confidence": float(post.get("ai_sentiment_confidence") or 0),
        "ai_sentiment_reasoning": post.get("ai_sentiment_reasoning"),
        "ai_tickers": parse_jsonb(post.get("ai_tickers")),
        "ai_tags": parse_jsonb(post.get("ai_tags")),
        "ai_summary": post.get("ai_summary"),
        "ai_trading_signal": post.get("ai_trading_signal"),
        "ai_is_stock_related": post.get("ai_is_stock_related", False),
        "ai_stock_related_confidence": float(
            post.get("ai_stock_related_confidence") or 0
        ),
        "ai_stock_related_reason": post.get("ai_stock_related_reason"),
        "ai_analyzed_at": post.get("ai_analyzed_at"),
        "ai_model": post.get("ai_model"),
        # === æ—¶é—´æˆ³ ===
        "created_at": post.get("created_at"),
        "scraped_at": post.get("scraped_at"),
        "updated_at": post.get("updated_at"),
    }


@router.get("/posts", response_model=Dict)
def get_xhs_posts(
    # åˆ†é¡µ
    limit: int = Query(50, ge=1, le=200, description="è¿”å›žæ•°é‡"),
    offset: int = Query(0, ge=0, description="åç§»é‡ï¼ˆç”¨äºŽåˆ†é¡µï¼‰"),
    # ç­›é€‰æ¡ä»¶
    keyword: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    ticker: Optional[str] = Query(None, description="è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ NVDAï¼‰"),
    sentiment: Optional[str] = Query(None, description="æƒ…æ„Ÿ: bullish/bearish/neutral"),
    stock_related: Optional[bool] = Query(None, description="æ˜¯å¦è‚¡ç¥¨ç›¸å…³"),
    has_images: Optional[bool] = Query(None, description="æ˜¯å¦æœ‰å›¾ç‰‡"),
    has_video: Optional[bool] = Query(None, description="æ˜¯å¦æœ‰è§†é¢‘"),
    # æŽ’åº
    sort_by: str = Query("scraped_at", description="æŽ’åºå­—æ®µ"),
    sort_desc: bool = Query(True, description="æ˜¯å¦é™åº"),
):
    """
    ðŸ“‹ èŽ·å–å°çº¢ä¹¦å¸–å­åˆ—è¡¨

    è¿”å›žçˆ¬å–çš„å¸–å­æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„åª’ä½“èµ„æºå’Œ AI åˆ†æžç»“æžœã€‚

    ### ç­›é€‰å‚æ•°
    - `keyword`: æŒ‰æœç´¢å…³é”®è¯ç­›é€‰
    - `ticker`: æŒ‰è‚¡ç¥¨ä»£ç ç­›é€‰ï¼ˆå¦‚ NVDA, TSLAï¼‰
    - `sentiment`: æŒ‰ AI æƒ…æ„Ÿåˆ†æžç»“æžœç­›é€‰ï¼ˆbullish/bearish/neutralï¼‰
    - `stock_related`: æ˜¯å¦åªè¿”å›žè‚¡ç¥¨ç›¸å…³å¸–å­
    - `has_images`: æ˜¯å¦æœ‰å›¾ç‰‡
    - `has_video`: æ˜¯å¦æœ‰è§†é¢‘

    ### è¿”å›žæ•°æ®
    æ¯ä¸ªå¸–å­åŒ…å«ï¼š
    - åŸºç¡€ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å†…å®¹ã€é“¾æŽ¥ï¼‰
    - ä½œè€…ä¿¡æ¯ï¼ˆåç§°ã€å¤´åƒï¼‰
    - åª’ä½“èµ„æºï¼ˆå°é¢å›¾ã€å›¾ç‰‡åˆ—è¡¨ã€è§†é¢‘ï¼‰
    - äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµžã€æ”¶è—ã€è¯„è®ºã€åˆ†äº«ï¼‰
    - AI åˆ†æžï¼ˆæƒ…æ„Ÿã€è‚¡ç¥¨ä»£ç ã€æ‘˜è¦ã€äº¤æ˜“ä¿¡å·ï¼‰
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªè¿žæŽ¥")

    try:
        # æž„å»ºæŸ¥è¯¢
        query = supabase.table("xhs_posts").select("*", count="exact")

        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if keyword:
            query = query.eq("search_keyword", keyword)

        if ticker:
            query = query.contains("ai_tickers", [ticker.upper()])

        if sentiment:
            query = query.eq("ai_sentiment", sentiment)

        if stock_related is True:
            query = query.eq("ai_is_stock_related", True)
        elif stock_related is False:
            query = query.eq("ai_is_stock_related", False)

        if has_video is True:
            query = query.not_.is_("video_url", "null")
        elif has_video is False:
            query = query.is_("video_url", "null")

        # æŽ’åº
        query = query.order(sort_by, desc=sort_desc)

        # åˆ†é¡µ
        query = query.range(offset, offset + limit - 1)

        # æ‰§è¡ŒæŸ¥è¯¢
        result = query.execute()
        posts = result.data or []
        total = result.count or 0

        # å†…å­˜ä¸­è¿‡æ»¤ has_imagesï¼ˆJSONB æ•°ç»„éžç©ºæŸ¥è¯¢å¤æ‚ï¼‰
        if has_images is not None:
            formatted = []
            for post in posts:
                image_urls = post.get("image_urls")
                has_img = bool(image_urls and len(image_urls) > 0)
                if has_images == has_img:
                    formatted.append(_format_post(post))
            posts = formatted
        else:
            posts = [_format_post(p) for p in posts]

        return {
            "success": True,
            "data": posts,
            "pagination": {
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_more": offset + len(posts) < total,
            },
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/posts/{note_id}", response_model=Dict)
def get_xhs_post_detail(note_id: str):
    """
    ðŸ“„ èŽ·å–å•ä¸ªå¸–å­è¯¦æƒ…

    æ ¹æ®å°çº¢ä¹¦ç¬”è®° ID èŽ·å–å®Œæ•´å¸–å­æ•°æ®ã€‚
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªè¿žæŽ¥")

    try:
        result = (
            supabase.table("xhs_posts")
            .select("*")
            .eq("note_id", note_id)
            .limit(1)
            .execute()
        )

        if not result.data:
            raise HTTPException(status_code=404, detail=f"å¸–å­ä¸å­˜åœ¨: {note_id}")

        return {
            "success": True,
            "data": _format_post(result.data[0]),
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
