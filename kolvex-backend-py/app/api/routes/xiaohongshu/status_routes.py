"""
å°çº¢ä¹¦çˆ¬è™«çŠ¶æ€ä¸ç»Ÿè®¡ API è·¯ç”±
æä¾›çˆ¬è™«çŠ¶æ€æ£€æŸ¥å’Œæ•°æ®åº“ç»Ÿè®¡åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException
from typing import Dict
from datetime import datetime, timezone, timedelta

from app.services.xiaohongshu import (
    get_supabase_client,
    get_stats,
    COOKIES_FILE,
    DEFAULT_KEYWORDS,
)
from app.services.xiaohongshu.scraper import load_cookies

router = APIRouter()


# ============================================================
# çŠ¶æ€ä¸ç»Ÿè®¡ç«¯ç‚¹
# ============================================================


@router.get("/status", response_model=Dict)
def get_scraper_status():
    """
    ğŸ“Š è·å–å°çº¢ä¹¦çˆ¬è™«çŠ¶æ€

    æ£€æŸ¥ï¼š
    - Cookies æ˜¯å¦å­˜åœ¨ï¼ˆç™»å½•çŠ¶æ€ï¼‰
    - Supabase è¿æ¥çŠ¶æ€
    - æ•°æ®åº“ä¸­çš„å¸–å­æ•°é‡
    - é»˜è®¤å…³é”®è¯åˆ—è¡¨
    """
    cookies = load_cookies()
    supabase = get_supabase_client()
    
    # æ£€æŸ¥ç™»å½•çŠ¶æ€
    is_logged_in = cookies is not None and len(cookies) > 0

    # è·å–æ•°æ®åº“ä¸­çš„å¸–å­æ•°é‡
    posts_count = 0
    stock_related_count = 0
    if supabase:
        try:
            result = (
                supabase.table("xhs_posts")
                .select("id", count="exact")
                .execute()
            )
            posts_count = result.count or 0

            # è‚¡ç¥¨ç›¸å…³å¸–å­æ•°é‡
            result_stock = (
                supabase.table("xhs_posts")
                .select("id", count="exact")
                .eq("ai_is_stock_related", True)
                .execute()
            )
            stock_related_count = result_stock.count or 0
        except:
            pass

    return {
        "platform": "xiaohongshu",
        "is_logged_in": is_logged_in,
        "cookies_available": cookies is not None,
        "cookies_count": len(cookies) if cookies else 0,
        "cookies_file": str(COOKIES_FILE),
        "supabase_connected": supabase is not None,
        "total_posts": posts_count,
        "stock_related_posts": stock_related_count,
        "default_keywords": DEFAULT_KEYWORDS,
        "login_required_message": None if is_logged_in else "è¯·å…ˆåœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œç™»å½•å‘½ä»¤: python -m app.services.xiaohongshu --login",
    }


@router.get("/login-status", response_model=Dict)
def check_login_status():
    """
    ğŸ”‘ æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€
    
    è¿”å›ï¼š
    - is_logged_in: æ˜¯å¦å·²ç™»å½•
    - message: çŠ¶æ€ä¿¡æ¯
    - login_command: ç™»å½•å‘½ä»¤ï¼ˆå¦‚æœéœ€è¦ç™»å½•ï¼‰
    """
    cookies = load_cookies()
    is_logged_in = cookies is not None and len(cookies) > 0
    
    if is_logged_in:
        return {
            "is_logged_in": True,
            "message": "âœ… å·²ç™»å½•ï¼Œå¯ä»¥å¼€å§‹çˆ¬å–",
            "cookies_count": len(cookies),
            "login_command": None,
        }
    else:
        return {
            "is_logged_in": False,
            "message": "âŒ æœªç™»å½•ï¼Œè¯·å…ˆæ‰§è¡Œç™»å½•å‘½ä»¤",
            "cookies_count": 0,
            "login_command": "python -m app.services.xiaohongshu --login",
        }


@router.get("/stats", response_model=Dict)
def get_database_stats():
    """
    ğŸ“Š è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    è¿”å›ï¼š
    - æ€»å¸–å­æ•°
    - è‚¡ç¥¨ç›¸å…³å¸–å­æ•°
    - æŒ‰å…³é”®è¯ç»Ÿè®¡
    - æŒ‰æƒ…æ„Ÿç»Ÿè®¡
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    stats = get_stats(supabase)

    # è·å–æƒ…æ„Ÿåˆ†æç»Ÿè®¡
    try:
        sentiment_result = (
            supabase.table("xhs_posts")
            .select("ai_sentiment")
            .execute()
        )
        sentiment_counts = {}
        for post in sentiment_result.data:
            sentiment = post.get("ai_sentiment") or "æœªåˆ†æ"
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        stats["by_sentiment"] = sentiment_counts
    except Exception:
        stats["by_sentiment"] = {}

    # è·å–ä½œè€…ç»Ÿè®¡
    try:
        author_result = (
            supabase.table("xhs_posts")
            .select("author_name")
            .execute()
        )
        author_counts = {}
        for post in author_result.data:
            author = post.get("author_name") or "æœªçŸ¥"
            author_counts[author] = author_counts.get(author, 0) + 1
        # åªè¿”å›å‰ 20 ä¸ªä½œè€…
        stats["top_authors"] = dict(
            sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        )
    except Exception:
        stats["top_authors"] = {}

    return stats


@router.get("/posts/date-stats", response_model=Dict)
def get_posts_date_stats():
    """
    ğŸ“Š æŸ¥çœ‹å¸–å­æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡

    ç”¨äºè¯Šæ–­æ•°æ®åº“ä¸­å¸–å­çš„æ—¶é—´åˆ†å¸ƒ
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    try:
        # è·å–æ‰€æœ‰å¸–å­çš„ scraped_at
        result = supabase.table("xhs_posts").select("scraped_at, created_at").execute()
        posts = result.data or []

        # ç»Ÿè®¡
        total = len(posts)
        null_dates = 0
        by_date = {}
        recent_7_days = 0
        recent_30_days = 0

        now = datetime.now(timezone.utc)
        cutoff_7 = now - timedelta(days=7)
        cutoff_30 = now - timedelta(days=30)

        for post in posts:
            scraped_at = post.get("scraped_at")
            if not scraped_at:
                null_dates += 1
                continue

            try:
                # è§£ææ—¶é—´
                if isinstance(scraped_at, str):
                    post_time = datetime.fromisoformat(
                        scraped_at.replace("Z", "+00:00")
                    )
                else:
                    post_time = scraped_at

                date_str = post_time.strftime("%Y-%m-%d")
                by_date[date_str] = by_date.get(date_str, 0) + 1

                if post_time.tzinfo is None:
                    post_time = post_time.replace(tzinfo=timezone.utc)

                if post_time >= cutoff_7:
                    recent_7_days += 1
                if post_time >= cutoff_30:
                    recent_30_days += 1
            except:
                null_dates += 1

        # æŒ‰æ—¥æœŸæ’åº
        by_date_sorted = dict(sorted(by_date.items(), reverse=True)[:30])

        return {
            "total_posts": total,
            "null_dates": null_dates,
            "recent_7_days": recent_7_days,
            "recent_30_days": recent_30_days,
            "older_than_7_days": total - recent_7_days - null_dates,
            "by_date": by_date_sorted,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/tickers/stats", response_model=Dict)
def get_tickers_stats():
    """
    ğŸ“ˆ è·å–è‚¡ç¥¨ä»£ç ç»Ÿè®¡

    è¿”å›è¢«æåŠæœ€å¤šçš„è‚¡ç¥¨ä»£ç 
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    try:
        result = (
            supabase.table("xhs_posts")
            .select("ai_tickers")
            .eq("ai_is_stock_related", True)
            .execute()
        )
        posts = result.data or []

        ticker_counts = {}
        for post in posts:
            tickers = post.get("ai_tickers") or []
            if isinstance(tickers, list):
                for ticker in tickers:
                    ticker_counts[ticker] = ticker_counts.get(ticker, 0) + 1

        # æŒ‰æ•°é‡æ’åº
        sorted_tickers = dict(
            sorted(ticker_counts.items(), key=lambda x: x[1], reverse=True)[:30]
        )

        return {
            "total_stock_related_posts": len(posts),
            "unique_tickers": len(ticker_counts),
            "top_tickers": sorted_tickers,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

