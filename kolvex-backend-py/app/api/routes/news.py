"""
é‡‘èæ–°é—» API è·¯ç”±
æä¾› Benzinga æ–°é—»æ•°æ®çš„è·å–å’Œå­˜å‚¨åŠŸèƒ½

åŠŸèƒ½ï¼š
1. è·å–å•åªè‚¡ç¥¨çš„æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“
2. å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶è·å–æ‰€æœ‰ KOL æåˆ°è¿‡çš„æ ‡çš„çš„æ–°é—»
"""

from fastapi import APIRouter, Query, HTTPException
from typing import Optional, List
from pydantic import BaseModel, Field
from datetime import datetime, date, timedelta, timezone
import logging
import asyncio

from app.core.supabase import get_supabase_service
from app.services.benzinga import BenzingaClient, NewsArticle

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/news", tags=["News"])


# ============================================================
# å…¨å±€çŠ¶æ€ - è®°å½•å®šæ—¶ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
# ============================================================


class SchedulerStatus:
    """KOL æ ‡çš„æ–°é—»å®šæ—¶ä»»åŠ¡çŠ¶æ€"""

    last_run_at: Optional[datetime] = None
    last_run_tickers: List[str] = []
    last_run_articles_saved: int = 0
    last_run_duration_seconds: float = 0.0
    is_running: bool = False
    next_run_at: Optional[datetime] = None
    error_message: Optional[str] = None


scheduler_status = SchedulerStatus()


class BulkNewsSchedulerStatus:
    """æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡çŠ¶æ€"""

    is_enabled: bool = False
    is_running: bool = False
    last_run_at: Optional[datetime] = None
    last_run_fetched: int = 0
    last_run_saved: int = 0
    last_run_duration_seconds: float = 0.0
    next_run_at: Optional[datetime] = None
    error_message: Optional[str] = None
    interval_hours: int = 1


bulk_news_scheduler_status = BulkNewsSchedulerStatus()


# ============================================================
# Pydantic æ¨¡å‹
# ============================================================


class NewsArticleResponse(BaseModel):
    """æ–°é—»æ–‡ç« å“åº”æ¨¡å‹"""

    id: Optional[int] = None
    published_at: str = Field(..., description="å‘å¸ƒæ—¶é—´ (ISO æ ¼å¼)")
    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜")
    summary: str = Field(..., description="æ–‡ç« æ‘˜è¦")
    url: str = Field(..., description="æ–‡ç«  URL")
    tags: List[str] = Field(default_factory=list, description="æ ‡ç­¾åˆ—è¡¨")
    tickers: List[str] = Field(default_factory=list, description="ç›¸å…³è‚¡ç¥¨ä»£ç ")
    source: str = Field(default="benzinga", description="æ–°é—»æ¥æº")
    created_at: Optional[datetime] = None


class NewsListResponse(BaseModel):
    """æ–°é—»åˆ—è¡¨å“åº”"""

    articles: List[NewsArticleResponse]
    total: int
    page: int
    page_size: int
    has_more: bool


class FetchNewsResponse(BaseModel):
    """è·å–æ–°é—»å“åº”"""

    success: bool
    message: str
    ticker: str
    articles_fetched: int
    articles_saved: int
    fetch_time: str  # ISO æ ¼å¼æ—¶é—´


class SchedulerStatusResponse(BaseModel):
    """å®šæ—¶ä»»åŠ¡çŠ¶æ€å“åº”"""

    is_running: bool
    last_run_at: Optional[str] = None
    last_run_tickers: List[str] = []
    last_run_articles_saved: int = 0
    last_run_duration_seconds: float = 0.0
    next_run_at: Optional[str] = None
    error_message: Optional[str] = None


class FetchAllKOLTickersResponse(BaseModel):
    """è·å–æ‰€æœ‰ KOL æ ‡çš„æ–°é—»å“åº”"""

    success: bool
    message: str
    tickers_processed: List[str]
    total_articles_saved: int
    start_time: str
    end_time: str
    duration_seconds: float


class FetchBulkNewsResponse(BaseModel):
    """æ‰¹é‡è·å–å…¨é‡æ–°é—»å“åº”"""

    success: bool
    message: str
    total_articles_fetched: int
    total_articles_saved: int
    time_periods: List[dict] = Field(
        default_factory=list, description="æ¯ä¸ªæ—¶é—´æ®µçš„è·å–è¯¦æƒ…"
    )
    start_time: str
    end_time: str
    duration_seconds: float


class BulkNewsSchedulerStatusResponse(BaseModel):
    """æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡çŠ¶æ€å“åº”"""

    is_enabled: bool = Field(..., description="å®šæ—¶ä»»åŠ¡æ˜¯å¦å·²å¯ç”¨")
    is_running: bool = Field(..., description="æ˜¯å¦æ­£åœ¨æ‰§è¡Œ")
    last_run_at: Optional[str] = Field(None, description="ä¸Šæ¬¡æ‰§è¡Œæ—¶é—´")
    last_run_fetched: int = Field(0, description="ä¸Šæ¬¡è·å–çš„æ–‡ç« æ•°")
    last_run_saved: int = Field(0, description="ä¸Šæ¬¡ä¿å­˜çš„æ–‡ç« æ•°")
    last_run_duration_seconds: float = Field(0.0, description="ä¸Šæ¬¡æ‰§è¡Œè€—æ—¶")
    next_run_at: Optional[str] = Field(None, description="ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´")
    interval_hours: int = Field(1, description="æ‰§è¡Œé—´éš”ï¼ˆå°æ—¶ï¼‰")
    error_message: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")


class SchedulerControlResponse(BaseModel):
    """å®šæ—¶ä»»åŠ¡æ§åˆ¶å“åº”"""

    success: bool
    message: str
    is_enabled: bool
    next_run_at: Optional[str] = None


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================


async def save_articles_to_db(articles: List[NewsArticle]) -> int:
    """
    å°†æ–°é—»æ–‡ç« ä¿å­˜åˆ°æ•°æ®åº“

    Args:
        articles: æ–°é—»æ–‡ç« åˆ—è¡¨

    Returns:
        int: æˆåŠŸä¿å­˜çš„æ–‡ç« æ•°é‡
    """
    if not articles:
        return 0

    supabase = get_supabase_service()
    saved_count = 0

    # æ‰¹é‡ upsertï¼šæ˜¾è‘—å‡å°‘ Supabase å¾€è¿”æ¬¡æ•°ï¼Œé¿å…é€æ¡å†™å…¥å¯¼è‡´æ…¢/å®¹æ˜“è§¦å‘é™æµ
    payload: List[dict] = []
    for article in articles:
        # URL æ˜¯å”¯ä¸€é”®ï¼›ç¼ºå¤± URL çš„æ•°æ®æ— æ³•å»é‡ï¼Œä¹Ÿæ— æ³•å†™å…¥ï¼ˆschema: url NOT NULL UNIQUEï¼‰
        if not article.url:
            continue
        payload.append(
            {
                "published_at": article.published_at,
                "title": article.title,
                "summary": article.summary,
                "url": article.url,
                "tags": article.tags,
                "tickers": [
                    t.upper()
                    for t in (article.tickers or [])
                    if isinstance(t, str) and t
                ],
                "source": "benzinga",
            }
        )

    if not payload:
        return 0

    # å•ä¸ª ticker æœ€å¤§ 50 ç¯‡ï¼›è¿™é‡Œä»åš chunk ä»¥é˜²æœªæ¥è°ƒå¤§ limit æˆ–å…¶ä»–è°ƒç”¨å¤ç”¨
    chunk_size = 200
    for i in range(0, len(payload), chunk_size):
        chunk = payload[i : i + chunk_size]
        try:
            result = (
                supabase.table("news_articles")
                .upsert(chunk, on_conflict="url")
                .execute()
            )

            # supabase-py é€šå¸¸ä¼šè¿”å›å†™å…¥åçš„è¡Œæ•°æ®ï¼›ä½†åœ¨æŸäº›é…ç½®ä¸‹å¯èƒ½ä¸ºç©ºæ•°ç»„
            if getattr(result, "data", None) is not None:
                saved_count += len(result.data or [])
            else:
                # æ— è¿”å›æ•°æ®æ—¶ï¼Œä¿å®ˆè®¤ä¸ºå†™å…¥æˆåŠŸï¼ˆè‹¥å¤±è´¥é€šå¸¸ä¼šæŠ›å¼‚å¸¸ï¼‰
                saved_count += len(chunk)
        except Exception as e:
            # è®°å½•æ›´å®Œæ•´çš„é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºå®šä½ RLS/key/schema é—®é¢˜
            logger.exception(f"æ‰¹é‡ä¿å­˜æ–°é—»å¤±è´¥ (chunk={i}-{i+len(chunk)-1}): {e}")

    return saved_count


def db_row_to_response(row: dict) -> NewsArticleResponse:
    """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸º API å“åº”æ¨¡å‹"""
    return NewsArticleResponse(
        id=row.get("id"),
        published_at=str(row.get("published_at") or ""),
        title=row.get("title", ""),
        summary=str(row.get("summary") or ""),
        url=row.get("url", ""),
        tags=row.get("tags") or [],
        tickers=row.get("tickers") or [],
        source=row.get("source", "benzinga"),
        created_at=row.get("created_at"),
    )


def _normalize_ticker(raw: str) -> Optional[str]:
    """
    æ¸…ç†å¹¶éªŒè¯ ticker æ ¼å¼
    - å»æ‰ $ ç¬¦å·
    - åªä¿ç•™ 1-5 ä½å¤§å†™å­—æ¯çš„æ ‡å‡† ticker
    """
    import re

    if not raw:
        return None
    # å»æ‰ $ ç¬¦å·å’Œç©ºæ ¼
    cleaned = raw.strip().lstrip("$").upper()
    # æ ‡å‡† ticker: 1-5 ä½å¤§å†™å­—æ¯
    if re.match(r"^[A-Z]{1,5}$", cleaned):
        return cleaned
    return None


async def get_all_kol_mentioned_tickers() -> List[str]:
    """
    è·å–æ‰€æœ‰è¢« KOL è®¨è®ºè¿‡çš„å”¯ä¸€è‚¡ç¥¨ä»£ç åˆ—è¡¨

    Returns:
        List[str]: å”¯ä¸€çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆæŒ‰å­—æ¯æ’åºï¼‰
    """
    supabase = get_supabase_service()

    result = (
        supabase.table("kol_tweets")
        .select("ai_tickers")
        .not_.is_("ai_tickers", "null")
        .execute()
    )

    all_tickers = set()
    for row in result.data or []:
        tickers = row.get("ai_tickers")
        if tickers:
            if isinstance(tickers, str):
                try:
                    import json

                    tickers = json.loads(tickers)
                except json.JSONDecodeError:
                    continue
            if isinstance(tickers, list):
                for ticker in tickers:
                    normalized = _normalize_ticker(ticker)
                    if normalized:
                        all_tickers.add(normalized)

    return sorted(list(all_tickers))


async def fetch_and_save_ticker_news(
    ticker: str,
    limit: int = 10,
    days: int = 7,
) -> tuple[int, int]:
    """
    è·å–å•ä¸ªè‚¡ç¥¨çš„æ–°é—»å¹¶ä¿å­˜

    Returns:
        (articles_fetched, articles_saved)
    """
    date_to = date.today()
    date_from = date_to - timedelta(days=days)

    async with BenzingaClient() as client:
        response = await client.get_news(
            tickers=ticker,
            limit=limit,
            date_from=date_from.isoformat(),
            date_to=date_to.isoformat(),
        )

    if not response.success or not response.articles:
        return 0, 0

    saved_count = await save_articles_to_db(response.articles)
    return len(response.articles), saved_count


# ============================================================
# API è·¯ç”±
# ============================================================


@router.get("/", response_model=NewsListResponse, summary="è·å–æ–°é—»åˆ—è¡¨")
async def get_news_list(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    ticker: Optional[str] = Query(None, description="æŒ‰è‚¡ç¥¨ä»£ç ç­›é€‰"),
    tag: Optional[str] = Query(None, description="æŒ‰æ ‡ç­¾ç­›é€‰"),
):
    """
    è·å–æ•°æ®åº“ä¸­çš„æ–°é—»åˆ—è¡¨

    - **page**: é¡µç ï¼Œä» 1 å¼€å§‹
    - **page_size**: æ¯é¡µæ•°é‡ï¼Œé»˜è®¤ 20
    - **ticker**: å¯é€‰ï¼ŒæŒ‰è‚¡ç¥¨ä»£ç ç­›é€‰
    - **tag**: å¯é€‰ï¼ŒæŒ‰æ ‡ç­¾ç­›é€‰
    """
    try:
        supabase = get_supabase_service()
        offset = (page - 1) * page_size

        query = supabase.table("news_articles").select("*", count="exact")

        if ticker:
            query = query.contains("tickers", [ticker.upper()])
        
        if tag:
            query = query.contains("tags", [tag])

        result = (
            query.order("published_at", desc=True)
            .range(offset, offset + page_size - 1)
            .execute()
        )

        articles = [db_row_to_response(row) for row in (result.data or [])]
        total = result.count or 0
        has_more = offset + len(articles) < total

        return NewsListResponse(
            articles=articles,
            total=total,
            page=page,
            page_size=page_size,
            has_more=has_more,
        )

    except Exception as e:
        logger.error(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get(
    "/fetch/{ticker}",
    response_model=FetchNewsResponse,
    summary="è·å–å•åªè‚¡ç¥¨çš„æ–°é—»å¹¶ä¿å­˜",
)
async def fetch_ticker_news(
    ticker: str,
    limit: int = Query(10, ge=1, le=50, description="è·å–æ•°é‡"),
    days: int = Query(7, ge=1, le=30, description="è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»"),
):
    """
    ä» Benzinga API è·å–æŒ‡å®šè‚¡ç¥¨çš„æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“

    - **ticker**: è‚¡ç¥¨ä»£ç  (å¦‚ NVDA, AAPL)
    - **limit**: è·å–çš„æ–°é—»æ•°é‡ (1-50)
    - **days**: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—» (1-30)
    """
    ticker = ticker.upper()
    fetch_time = datetime.now(timezone.utc)

    try:
        articles_fetched, articles_saved = await fetch_and_save_ticker_news(
            ticker=ticker,
            limit=limit,
            days=days,
        )

        return FetchNewsResponse(
            success=True,
            message=f"æˆåŠŸè·å– {ticker} çš„æ–°é—»",
            ticker=ticker,
            articles_fetched=articles_fetched,
            articles_saved=articles_saved,
            fetch_time=fetch_time.isoformat(),
        )

    except Exception as e:
        logger.error(f"è·å– {ticker} æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»å¤±è´¥: {str(e)}")


@router.get("/kol-tickers", summary="è·å–æ‰€æœ‰è¢« KOL è®¨è®ºè¿‡çš„è‚¡ç¥¨ä»£ç ")
async def get_kol_tickers():
    """
    è·å–æ‰€æœ‰è¢« KOL è®¨è®ºè¿‡çš„å”¯ä¸€è‚¡ç¥¨ä»£ç åˆ—è¡¨
    """
    tickers = await get_all_kol_mentioned_tickers()

    return {
        "tickers": tickers,
        "total": len(tickers),
    }


@router.post(
    "/fetch-kol-tickers",
    response_model=FetchAllKOLTickersResponse,
    summary="è·å–æ‰€æœ‰ KOL æ ‡çš„çš„æ–°é—»",
)
async def fetch_all_kol_tickers_news(
    limit_per_ticker: int = Query(
        10, ge=1, le=50, description="æ¯ä¸ªè‚¡ç¥¨è·å–çš„æ–°é—»æ•°é‡"
    ),
    days: int = Query(7, ge=1, le=30, description="è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»"),
    max_concurrent: int = Query(3, ge=1, le=10, description="æœ€å¤§å¹¶å‘æ•°"),
):
    """
    è·å–æ‰€æœ‰è¢« KOL è®¨è®ºè¿‡çš„è‚¡ç¥¨çš„æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“

    æ­¤ç«¯ç‚¹ä¼šï¼š
    1. ä» kol_tweets è¡¨ä¸­è·å–æ‰€æœ‰è¢« KOL è®¨è®ºè¿‡çš„è‚¡ç¥¨ä»£ç 
    2. å¯¹æ¯ä¸ªè‚¡ç¥¨ä» Benzinga è·å–æœ€æ–°æ–°é—»
    3. å°†æ–°é—»ä¿å­˜åˆ°æ•°æ®åº“

    - **limit_per_ticker**: æ¯ä¸ªè‚¡ç¥¨è·å–çš„æ–°é—»æ•°é‡
    - **days**: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»
    - **max_concurrent**: æœ€å¤§å¹¶å‘æ•°
    """
    import time

    start_time = datetime.now(timezone.utc)
    start_ts = time.time()

    # æ›´æ–°çŠ¶æ€
    scheduler_status.is_running = True
    scheduler_status.error_message = None

    try:
        tickers = await get_all_kol_mentioned_tickers()

        if not tickers:
            scheduler_status.is_running = False
            return FetchAllKOLTickersResponse(
                success=True,
                message="æ²¡æœ‰æ‰¾åˆ°è¢« KOL è®¨è®ºè¿‡çš„è‚¡ç¥¨",
                tickers_processed=[],
                total_articles_saved=0,
                start_time=start_time.isoformat(),
                end_time=datetime.now(timezone.utc).isoformat(),
                duration_seconds=0.0,
            )

        # å¹¶å‘è·å–æ–°é—»
        semaphore = asyncio.Semaphore(max_concurrent)
        total_saved = 0

        async def fetch_with_semaphore(t: str) -> int:
            async with semaphore:
                _, saved = await fetch_and_save_ticker_news(
                    ticker=t,
                    limit=limit_per_ticker,
                    days=days,
                )
                return saved

        tasks = [fetch_with_semaphore(t) for t in tickers]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for r in results:
            if isinstance(r, int):
                total_saved += r
            elif isinstance(r, Exception):
                logger.error(f"è·å–æ–°é—»å¤±è´¥: {r}")

        end_time = datetime.now(timezone.utc)
        duration = time.time() - start_ts

        # æ›´æ–°çŠ¶æ€
        scheduler_status.is_running = False
        scheduler_status.last_run_at = start_time
        scheduler_status.last_run_tickers = tickers
        scheduler_status.last_run_articles_saved = total_saved
        scheduler_status.last_run_duration_seconds = round(duration, 2)
        scheduler_status.next_run_at = start_time + timedelta(hours=1)

        logger.info(
            f"KOL æ ‡çš„æ–°é—»è·å–å®Œæˆ: {len(tickers)} ä¸ªè‚¡ç¥¨, "
            f"ä¿å­˜ {total_saved} ç¯‡, è€—æ—¶ {duration:.2f}s"
        )

        return FetchAllKOLTickersResponse(
            success=True,
            message=f"å®Œæˆ {len(tickers)} ä¸ª KOL æ ‡çš„çš„æ–°é—»è·å–",
            tickers_processed=tickers,
            total_articles_saved=total_saved,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            duration_seconds=round(duration, 2),
        )

    except Exception as e:
        scheduler_status.is_running = False
        scheduler_status.error_message = str(e)
        logger.error(f"è·å– KOL æ ‡çš„æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post(
    "/fetch-bulk",
    response_model=FetchBulkNewsResponse,
    summary="æ‰¹é‡è·å–å…¨é‡æ–°é—»",
)
async def fetch_bulk_news(
    days: int = Query(30, ge=1, le=90, description="è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»"),
    batch_size: int = Query(100, ge=10, le=500, description="æ¯æ‰¹è·å–çš„æ–°é—»æ•°é‡"),
    batch_days: int = Query(7, ge=1, le=14, description="æ¯æ‰¹è¦†ç›–çš„å¤©æ•°"),
):
    """
    æ‰¹é‡è·å– Benzinga å…¨é‡æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“

    æ­¤ç«¯ç‚¹ä¸æŒ‰ ticker è¿‡æ»¤ï¼Œç›´æ¥è·å–æ‰€æœ‰æ–°é—»ï¼Œèƒ½è·å–æ›´å¤šç‹¬ç‰¹çš„æ–‡ç« ã€‚

    å·¥ä½œæ–¹å¼ï¼š
    1. å°†æ—¶é—´èŒƒå›´åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼ˆæ¯æ‰¹ batch_days å¤©ï¼‰
    2. å¯¹æ¯ä¸ªæ—¶é—´æ®µè°ƒç”¨ Benzinga API
    3. å°†æ‰€æœ‰æ–°é—»ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆè‡ªåŠ¨å»é‡ï¼‰

    - **days**: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»ï¼ˆé»˜è®¤ 30 å¤©ï¼‰
    - **batch_size**: æ¯æ‰¹è·å–çš„æœ€å¤§æ–°é—»æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
    - **batch_days**: æ¯æ‰¹è¦†ç›–çš„å¤©æ•°ï¼ˆé»˜è®¤ 7 å¤©ï¼‰

    æ³¨æ„ï¼šBenzinga API æ¯æ¬¡è¯·æ±‚æœ€å¤šè¿”å›çº¦ 25 ç¯‡ï¼Œä¼šè‡ªåŠ¨åˆ†æ‰¹è¯·æ±‚ã€‚
    """
    import time

    start_time = datetime.now(timezone.utc)
    start_ts = time.time()

    try:
        date_to = date.today()
        total_fetched = 0
        total_saved = 0
        time_periods = []

        async with BenzingaClient() as client:
            # åˆ†æ‰¹è·å–ä¸åŒæ—¶é—´æ®µ
            for days_ago in range(0, days, batch_days):
                period_end = date_to - timedelta(days=days_ago)
                period_start = date_to - timedelta(
                    days=min(days_ago + batch_days, days)
                )

                response = await client.get_news(
                    tickers="",  # ç©ºå­—ç¬¦ä¸² = è·å–å…¨éƒ¨
                    limit=batch_size,
                    date_from=period_start.isoformat(),
                    date_to=period_end.isoformat(),
                )

                fetched_count = len(response.articles) if response.articles else 0
                saved_count = 0

                if response.articles:
                    saved_count = await save_articles_to_db(response.articles)

                total_fetched += fetched_count
                total_saved += saved_count

                period_info = {
                    "period_start": period_start.isoformat(),
                    "period_end": period_end.isoformat(),
                    "articles_fetched": fetched_count,
                    "articles_saved": saved_count,
                }
                time_periods.append(period_info)

                logger.info(
                    f"æ‰¹æ¬¡ {period_start} ~ {period_end}: "
                    f"è·å– {fetched_count}, ä¿å­˜ {saved_count}"
                )

        end_time = datetime.now(timezone.utc)
        duration = time.time() - start_ts

        logger.info(
            f"æ‰¹é‡æ–°é—»è·å–å®Œæˆ: å…±è·å– {total_fetched} ç¯‡, "
            f"ä¿å­˜ {total_saved} ç¯‡, è€—æ—¶ {duration:.2f}s"
        )

        return FetchBulkNewsResponse(
            success=True,
            message=f"å®Œæˆ {days} å¤©æ–°é—»çš„æ‰¹é‡è·å–",
            total_articles_fetched=total_fetched,
            total_articles_saved=total_saved,
            time_periods=time_periods,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            duration_seconds=round(duration, 2),
        )

    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get(
    "/scheduler-status",
    response_model=SchedulerStatusResponse,
    summary="è·å–å®šæ—¶ä»»åŠ¡çŠ¶æ€",
)
async def get_scheduler_status():
    """
    è·å–å®šæ—¶ä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€

    è¿”å›ï¼š
    - æ˜¯å¦æ­£åœ¨è¿è¡Œ
    - ä¸Šæ¬¡è¿è¡Œæ—¶é—´
    - ä¸Šæ¬¡å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨
    - ä¸Šæ¬¡ä¿å­˜çš„æ–‡ç« æ•°é‡
    - ä¸‹æ¬¡è¿è¡Œæ—¶é—´
    """
    return SchedulerStatusResponse(
        is_running=scheduler_status.is_running,
        last_run_at=(
            scheduler_status.last_run_at.isoformat()
            if scheduler_status.last_run_at
            else None
        ),
        last_run_tickers=scheduler_status.last_run_tickers,
        last_run_articles_saved=scheduler_status.last_run_articles_saved,
        last_run_duration_seconds=scheduler_status.last_run_duration_seconds,
        next_run_at=(
            scheduler_status.next_run_at.isoformat()
            if scheduler_status.next_run_at
            else None
        ),
        error_message=scheduler_status.error_message,
    )


# ============================================================
# å®šæ—¶ä»»åŠ¡å‡½æ•° (ä¾›å¤–éƒ¨è°ƒç”¨)
# ============================================================


async def scheduled_fetch_kol_news(
    limit_per_ticker: int = 10,
    days: int = 7,
    max_concurrent: int = 3,
):
    """
    å®šæ—¶ä»»åŠ¡ï¼šè·å–æ‰€æœ‰ KOL æ ‡çš„çš„æ–°é—»

    æ­¤å‡½æ•°ä¾›å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨è°ƒç”¨ï¼ˆå¦‚ APSchedulerï¼‰

    Args:
        limit_per_ticker: æ¯ä¸ªè‚¡ç¥¨è·å–çš„æ–°é—»æ•°é‡
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
    """
    import time

    start_time = datetime.now(timezone.utc)
    start_ts = time.time()

    scheduler_status.is_running = True
    scheduler_status.error_message = None

    try:
        tickers = await get_all_kol_mentioned_tickers()

        if not tickers:
            logger.info("å®šæ—¶ä»»åŠ¡: æ²¡æœ‰æ‰¾åˆ° KOL æ ‡çš„")
            scheduler_status.is_running = False
            return

        logger.info(f"å®šæ—¶ä»»åŠ¡å¼€å§‹: è·å– {len(tickers)} ä¸ª KOL æ ‡çš„çš„æ–°é—»")

        semaphore = asyncio.Semaphore(max_concurrent)
        total_saved = 0

        async def fetch_with_semaphore(t: str) -> int:
            async with semaphore:
                _, saved = await fetch_and_save_ticker_news(
                    ticker=t,
                    limit=limit_per_ticker,
                    days=days,
                )
                return saved

        tasks = [fetch_with_semaphore(t) for t in tickers]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for r in results:
            if isinstance(r, int):
                total_saved += r

        duration = time.time() - start_ts

        # æ›´æ–°çŠ¶æ€
        scheduler_status.is_running = False
        scheduler_status.last_run_at = start_time
        scheduler_status.last_run_tickers = tickers
        scheduler_status.last_run_articles_saved = total_saved
        scheduler_status.last_run_duration_seconds = round(duration, 2)
        scheduler_status.next_run_at = start_time + timedelta(hours=1)

        logger.info(
            f"å®šæ—¶ä»»åŠ¡å®Œæˆ: {len(tickers)} ä¸ªè‚¡ç¥¨, "
            f"ä¿å­˜ {total_saved} ç¯‡, è€—æ—¶ {duration:.2f}s"
        )

    except Exception as e:
        scheduler_status.is_running = False
        scheduler_status.error_message = str(e)
        logger.error(f"å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")


async def scheduled_fetch_bulk_news(
    days: int = 1,
    batch_size: int = 100,
):
    """
    å®šæ—¶ä»»åŠ¡ï¼šè·å–å…¨é‡æ–°é—»

    æ­¤å‡½æ•°ä¾›å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨è°ƒç”¨ï¼Œæ¯å°æ—¶è·å–æœ€è¿‘ 1 å¤©çš„æ–°é—»

    Args:
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»ï¼ˆé»˜è®¤ 1 å¤©ï¼Œåªè·å–æœ€æ–°çš„ï¼‰
        batch_size: æ¯æ‰¹è·å–æ•°é‡
    """
    import time

    start_time = datetime.now(timezone.utc)
    start_ts = time.time()

    bulk_news_scheduler_status.is_running = True
    bulk_news_scheduler_status.error_message = None

    try:
        date_to = date.today()
        date_from = date_to - timedelta(days=days)
        total_fetched = 0
        total_saved = 0

        logger.info(f"â° å®šæ—¶ä»»åŠ¡å¼€å§‹: è·å– {date_from} ~ {date_to} çš„å…¨é‡æ–°é—»")

        async with BenzingaClient() as client:
            response = await client.get_news(
                tickers="",  # è·å–å…¨éƒ¨
                limit=batch_size,
                date_from=date_from.isoformat(),
                date_to=date_to.isoformat(),
            )

            if response.articles:
                total_fetched = len(response.articles)
                total_saved = await save_articles_to_db(response.articles)

        duration = time.time() - start_ts

        # æ›´æ–°çŠ¶æ€
        bulk_news_scheduler_status.is_running = False
        bulk_news_scheduler_status.last_run_at = start_time
        bulk_news_scheduler_status.last_run_fetched = total_fetched
        bulk_news_scheduler_status.last_run_saved = total_saved
        bulk_news_scheduler_status.last_run_duration_seconds = round(duration, 2)
        bulk_news_scheduler_status.next_run_at = start_time + timedelta(
            hours=bulk_news_scheduler_status.interval_hours
        )

        logger.info(
            f"âœ… å®šæ—¶ä»»åŠ¡å®Œæˆ: è·å– {total_fetched} ç¯‡, "
            f"ä¿å­˜ {total_saved} ç¯‡, è€—æ—¶ {duration:.2f}s"
        )

    except Exception as e:
        bulk_news_scheduler_status.is_running = False
        bulk_news_scheduler_status.error_message = str(e)
        logger.error(f"âŒ æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")


# ============================================================
# å®šæ—¶ä»»åŠ¡æ§åˆ¶ API
# ============================================================


@router.get(
    "/bulk-scheduler-status",
    response_model=BulkNewsSchedulerStatusResponse,
    summary="è·å–æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡çŠ¶æ€",
)
async def get_bulk_scheduler_status():
    """
    è·å–æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€

    è¿”å›ï¼š
    - æ˜¯å¦å·²å¯ç”¨
    - æ˜¯å¦æ­£åœ¨è¿è¡Œ
    - ä¸Šæ¬¡è¿è¡Œæ—¶é—´å’Œç»“æœ
    - ä¸‹æ¬¡è¿è¡Œæ—¶é—´
    """
    return BulkNewsSchedulerStatusResponse(
        is_enabled=bulk_news_scheduler_status.is_enabled,
        is_running=bulk_news_scheduler_status.is_running,
        last_run_at=(
            bulk_news_scheduler_status.last_run_at.isoformat()
            if bulk_news_scheduler_status.last_run_at
            else None
        ),
        last_run_fetched=bulk_news_scheduler_status.last_run_fetched,
        last_run_saved=bulk_news_scheduler_status.last_run_saved,
        last_run_duration_seconds=bulk_news_scheduler_status.last_run_duration_seconds,
        next_run_at=(
            bulk_news_scheduler_status.next_run_at.isoformat()
            if bulk_news_scheduler_status.next_run_at
            else None
        ),
        interval_hours=bulk_news_scheduler_status.interval_hours,
        error_message=bulk_news_scheduler_status.error_message,
    )


@router.post(
    "/bulk-scheduler/start",
    response_model=SchedulerControlResponse,
    summary="å¯åŠ¨æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡",
)
async def start_bulk_scheduler(
    interval_hours: int = Query(1, ge=1, le=24, description="æ‰§è¡Œé—´éš”ï¼ˆå°æ—¶ï¼‰"),
):
    """
    å¯åŠ¨æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡

    - **interval_hours**: æ‰§è¡Œé—´éš”ï¼Œé»˜è®¤æ¯ 1 å°æ—¶æ‰§è¡Œä¸€æ¬¡
    """
    try:
        from main import scheduler

        if scheduler is None:
            raise HTTPException(status_code=500, detail="è°ƒåº¦å™¨æœªåˆå§‹åŒ–")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
        existing_job = scheduler.get_job("fetch_bulk_news")
        if existing_job:
            scheduler.remove_job("fetch_bulk_news")

        # æ·»åŠ æ–°ä»»åŠ¡
        from apscheduler.triggers.interval import IntervalTrigger

        scheduler.add_job(
            scheduled_fetch_bulk_news,
            IntervalTrigger(hours=interval_hours),
            id="fetch_bulk_news",
            name="è·å–å…¨é‡æ–°é—»",
            replace_existing=True,
        )

        # æ›´æ–°çŠ¶æ€
        bulk_news_scheduler_status.is_enabled = True
        bulk_news_scheduler_status.interval_hours = interval_hours

        job = scheduler.get_job("fetch_bulk_news")
        next_run = job.next_run_time if job else None
        bulk_news_scheduler_status.next_run_at = next_run

        logger.info(f"âœ… æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ (æ¯ {interval_hours} å°æ—¶)")

        return SchedulerControlResponse(
            success=True,
            message=f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡",
            is_enabled=True,
            next_run_at=next_run.isoformat() if next_run else None,
        )

    except ImportError:
        raise HTTPException(status_code=500, detail="APScheduler æœªå®‰è£…")
    except Exception as e:
        logger.error(f"å¯åŠ¨å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/bulk-scheduler/stop",
    response_model=SchedulerControlResponse,
    summary="åœæ­¢æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡",
)
async def stop_bulk_scheduler():
    """
    åœæ­¢æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡
    """
    try:
        from main import scheduler

        if scheduler is None:
            raise HTTPException(status_code=500, detail="è°ƒåº¦å™¨æœªåˆå§‹åŒ–")

        job = scheduler.get_job("fetch_bulk_news")
        if job:
            scheduler.remove_job("fetch_bulk_news")
            bulk_news_scheduler_status.is_enabled = False
            bulk_news_scheduler_status.next_run_at = None

            logger.info("ğŸ›‘ æ‰¹é‡æ–°é—»å®šæ—¶ä»»åŠ¡å·²åœæ­¢")

            return SchedulerControlResponse(
                success=True,
                message="å®šæ—¶ä»»åŠ¡å·²åœæ­¢",
                is_enabled=False,
            )
        else:
            return SchedulerControlResponse(
                success=True,
                message="å®šæ—¶ä»»åŠ¡æœªåœ¨è¿è¡Œ",
                is_enabled=False,
            )

    except Exception as e:
        logger.error(f"åœæ­¢å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/bulk-scheduler/run-now",
    response_model=FetchBulkNewsResponse,
    summary="ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰¹é‡æ–°é—»è·å–",
)
async def run_bulk_scheduler_now():
    """
    ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰¹é‡æ–°é—»è·å–ï¼ˆä¸å½±å“å®šæ—¶ä»»åŠ¡ï¼‰
    """
    # å¤ç”¨ fetch_bulk_news çš„é€»è¾‘
    return await fetch_bulk_news(days=1, batch_size=100, batch_days=1)
