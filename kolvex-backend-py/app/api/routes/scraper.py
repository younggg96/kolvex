"""
KOL æ‰¹é‡çˆ¬è™« API è·¯ç”±
æä¾› REST API æ¥å£ç”¨äºæ‰¹é‡çˆ¬å– KOL æ¨æ–‡å’Œ Profile ä¿¡æ¯
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
from enum import Enum
import asyncio
from datetime import datetime, timezone

# å¯¼å…¥çˆ¬è™«æ¨¡å—
from app.services.batch_kol_scraper import (
    BatchKOLScraper,
    get_supabase_client,
    get_stats,
    load_cookies,
    KOL_LIST,
    get_all_kols,
    COOKIES_FILE,
)

router = APIRouter(prefix="/scraper", tags=["KOL Scraper"])


# ============================================================
# è¯·æ±‚/å“åº”æ¨¡å‹
# ============================================================


class KOLItem(BaseModel):
    """å•ä¸ª KOL ä¿¡æ¯"""

    username: str = Field(..., description="KOL ç”¨æˆ·åï¼Œå¦‚ elonmusk")
    description: Optional[str] = Field(None, description="KOL æè¿°")
    category: Optional[str] = Field(None, description="KOL ç±»åˆ«")


class BatchScrapeRequest(BaseModel):
    """æ‰¹é‡çˆ¬å–è¯·æ±‚"""

    usernames: List[str] = Field(
        ..., description="è¦çˆ¬å–çš„ç”¨æˆ·ååˆ—è¡¨", min_length=1, max_length=50
    )
    max_posts_per_user: int = Field(
        10, ge=1, le=50, description="æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•°é‡"
    )
    category: Optional[str] = Field(None, description="ç»Ÿä¸€åˆ†ç±»ï¼ˆå¯é€‰ï¼‰")


class BatchScrapeWithDetailsRequest(BaseModel):
    """å¸¦è¯¦ç»†ä¿¡æ¯çš„æ‰¹é‡çˆ¬å–è¯·æ±‚"""

    kols: List[KOLItem] = Field(
        ..., description="KOL åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·åå’Œæè¿°", min_length=1, max_length=50
    )
    max_posts_per_user: int = Field(
        10, ge=1, le=50, description="æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•°é‡"
    )


class ScrapeResponse(BaseModel):
    """çˆ¬å–å“åº”"""

    success: bool
    message: str
    task_id: Optional[str] = None
    stats: Optional[Dict] = None


class ScraperStats(BaseModel):
    """çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""

    total_tweets: int
    total_profiles: int
    by_user: Dict[str, int]
    by_category: Dict[str, int]


class TaskStatus(str, Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


# ============================================================
# ä»»åŠ¡ç®¡ç†ï¼ˆç®€å•å†…å­˜å­˜å‚¨ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ Redisï¼‰
# ============================================================

_scrape_tasks: Dict[str, Dict] = {}


def generate_task_id() -> str:
    """ç”Ÿæˆä»»åŠ¡ ID"""
    return f"scrape_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}_{id(datetime.now(timezone.utc)) % 10000}"


# ============================================================
# API ç«¯ç‚¹
# ============================================================


@router.get("/status", response_model=Dict)
def get_scraper_status():
    """
    è·å–çˆ¬è™«çŠ¶æ€

    æ£€æŸ¥ï¼š
    - Cookies æ˜¯å¦å­˜åœ¨
    - Supabase è¿æ¥çŠ¶æ€
    - é¢„å®šä¹‰ KOL åˆ—è¡¨
    """
    cookies = load_cookies()
    supabase = get_supabase_client()

    return {
        "cookies_available": cookies is not None,
        "cookies_file": str(COOKIES_FILE),
        "supabase_connected": supabase is not None,
        "predefined_kol_count": len(get_all_kols()),
        "predefined_categories": list(KOL_LIST.keys()),
    }


@router.get("/stats", response_model=Dict)
def get_database_stats():
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    è¿”å›ï¼š
    - æ€»æ¨æ–‡æ•°
    - æŒ‰ç”¨æˆ·ç»Ÿè®¡
    - æŒ‰ç±»åˆ«ç»Ÿè®¡
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    stats = get_stats(supabase)

    # è·å– KOL profiles ç»Ÿè®¡
    try:
        profiles_result = (
            supabase.table("kol_profiles")
            .select("username, verification_type", count="exact")
            .execute()
        )
        total_profiles = profiles_result.count or 0

        # æŒ‰è®¤è¯ç±»å‹ç»Ÿè®¡
        verified_counts = {}
        for profile in profiles_result.data:
            v_type = profile.get("verification_type") or "None"
            verified_counts[v_type] = verified_counts.get(v_type, 0) + 1

        stats["total_profiles"] = total_profiles
        stats["by_verification"] = verified_counts
    except Exception:
        stats["total_profiles"] = 0
        stats["by_verification"] = {}

    return stats


@router.get("/kols", response_model=Dict)
def get_predefined_kols():
    """
    è·å–é¢„å®šä¹‰çš„ KOL åˆ—è¡¨

    è¿”å›æŒ‰ç±»åˆ«ç»„ç»‡çš„ KOL åˆ—è¡¨
    """
    return {
        "categories": KOL_LIST,
        "total": len(get_all_kols()),
    }


@router.delete("/tweets/old", response_model=Dict)
def delete_old_tweets(days: int = 7, include_null_dates: bool = True):
    """
    ğŸ—‘ï¸ åˆ é™¤æŒ‡å®šå¤©æ•°ä¹‹å‰çš„æ—§æ¨æ–‡

    å‚æ•°ï¼š
    - days: ä¿ç•™æœ€è¿‘ N å¤©çš„æ¨æ–‡ï¼Œåˆ é™¤æ›´æ—©çš„æ•°æ®ï¼ˆé»˜è®¤: 7 å¤©ï¼‰
    - include_null_dates: æ˜¯å¦ä¹Ÿåˆ é™¤æ²¡æœ‰æ—¥æœŸçš„æ¨æ–‡ï¼ˆé»˜è®¤: Trueï¼‰

    ç¤ºä¾‹ï¼š
    - DELETE /api/scraper/tweets/old?days=7  â†’ åˆ é™¤ 7 å¤©å‰çš„æ¨æ–‡
    - DELETE /api/scraper/tweets/old?days=30 â†’ åˆ é™¤ 30 å¤©å‰çš„æ¨æ–‡
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    from datetime import timedelta

    # è®¡ç®—æˆªæ­¢æ—¥æœŸï¼ˆä½¿ç”¨ç®€å•çš„æ—¥æœŸæ ¼å¼ï¼‰
    cutoff_date = (datetime.now(timezone.utc) - timedelta(days=days)).strftime(
        "%Y-%m-%d"
    )

    try:
        deleted_old = 0
        deleted_null = 0

        # 1. åˆ é™¤æ—¥æœŸæ—©äºæˆªæ­¢æ—¥æœŸçš„æ¨æ–‡
        count_old = (
            supabase.table("kol_tweets")
            .select("id", count="exact")
            .lt("created_at", cutoff_date)
            .execute()
        )
        deleted_old = count_old.count or 0

        if deleted_old > 0:
            supabase.table("kol_tweets").delete().lt(
                "created_at", cutoff_date
            ).execute()

        # 2. åˆ é™¤ created_at ä¸º NULL çš„æ¨æ–‡
        if include_null_dates:
            count_null = (
                supabase.table("kol_tweets")
                .select("id", count="exact")
                .is_("created_at", "null")
                .execute()
            )
            deleted_null = count_null.count or 0

            if deleted_null > 0:
                supabase.table("kol_tweets").delete().is_(
                    "created_at", "null"
                ).execute()

        total_deleted = deleted_old + deleted_null

        if total_deleted == 0:
            return {
                "success": True,
                "message": f"æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ é™¤çš„æ¨æ–‡",
                "deleted_count": 0,
                "cutoff_date": cutoff_date,
            }

        return {
            "success": True,
            "message": f"âœ… å·²åˆ é™¤ {total_deleted} æ¡æ—§æ¨æ–‡ï¼ˆ{deleted_old} æ¡æ—©äº {cutoff_date}ï¼Œ{deleted_null} æ¡æ— æ—¥æœŸï¼‰",
            "deleted_count": total_deleted,
            "deleted_before_cutoff": deleted_old,
            "deleted_null_dates": deleted_null,
            "cutoff_date": cutoff_date,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.get("/tweets/date-stats", response_model=Dict)
def get_tweets_date_stats():
    """
    ğŸ“Š æŸ¥çœ‹æ¨æ–‡æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡

    ç”¨äºè¯Šæ–­æ•°æ®åº“ä¸­æ¨æ–‡çš„æ—¶é—´åˆ†å¸ƒ
    """
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    try:
        # è·å–æ‰€æœ‰æ¨æ–‡çš„ created_at
        result = supabase.table("kol_tweets").select("created_at").execute()
        tweets = result.data or []

        # ç»Ÿè®¡
        total = len(tweets)
        null_dates = 0
        by_year = {}
        recent_7_days = 0
        recent_30_days = 0

        from datetime import timedelta

        now = datetime.now(timezone.utc)
        cutoff_7 = now - timedelta(days=7)
        cutoff_30 = now - timedelta(days=30)

        for tweet in tweets:
            created_at = tweet.get("created_at")
            if not created_at:
                null_dates += 1
                continue

            try:
                # è§£ææ—¶é—´
                if isinstance(created_at, str):
                    tweet_time = datetime.fromisoformat(
                        created_at.replace("Z", "+00:00")
                    )
                else:
                    tweet_time = created_at

                year = tweet_time.year
                by_year[year] = by_year.get(year, 0) + 1

                if tweet_time.tzinfo is None:
                    tweet_time = tweet_time.replace(tzinfo=timezone.utc)

                if tweet_time >= cutoff_7:
                    recent_7_days += 1
                if tweet_time >= cutoff_30:
                    recent_30_days += 1
            except:
                null_dates += 1

        # æŒ‰å¹´ä»½æ’åº
        by_year_sorted = dict(sorted(by_year.items(), reverse=True))

        return {
            "total_tweets": total,
            "null_dates": null_dates,
            "recent_7_days": recent_7_days,
            "recent_30_days": recent_30_days,
            "older_than_7_days": total - recent_7_days - null_dates,
            "by_year": by_year_sorted,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.delete("/tweets/all", response_model=Dict)
def delete_all_tweets(confirm: bool = False):
    """
    âš ï¸ åˆ é™¤æ‰€æœ‰æ¨æ–‡æ•°æ®ï¼ˆå±é™©æ“ä½œï¼ï¼‰

    å‚æ•°ï¼š
    - confirm: å¿…é¡»è®¾ä¸º true æ‰èƒ½æ‰§è¡Œåˆ é™¤
    """
    if not confirm:
        raise HTTPException(
            status_code=400, detail="è¯·æ·»åŠ  ?confirm=true å‚æ•°ç¡®è®¤åˆ é™¤æ‰€æœ‰æ¨æ–‡"
        )

    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    try:
        # å…ˆç»Ÿè®¡æ€»æ•°
        count_result = (
            supabase.table("kol_tweets").select("id", count="exact").execute()
        )
        total_count = count_result.count or 0

        if total_count == 0:
            return {
                "success": True,
                "message": "è¡¨ä¸­æ²¡æœ‰æ•°æ®",
                "deleted_count": 0,
            }

        # åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆä½¿ç”¨ neq æ¡ä»¶åˆ é™¤æ‰€æœ‰è®°å½•ï¼‰
        supabase.table("kol_tweets").delete().neq("id", -1).execute()

        return {
            "success": True,
            "message": f"âš ï¸ å·²åˆ é™¤æ‰€æœ‰ {total_count} æ¡æ¨æ–‡",
            "deleted_count": total_count,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.post("/scrape", response_model=ScrapeResponse)
def scrape_kols_sync(
    request: BatchScrapeRequest,
    background_tasks: BackgroundTasks,
):
    """
    åŒæ­¥çˆ¬å– KOL åˆ—è¡¨ï¼ˆç®€å•ç”¨æˆ·ååˆ—è¡¨ï¼‰

    è¯·æ±‚ç¤ºä¾‹:
    ```json
    {
        "usernames": ["elonmusk", "unusual_whales", "zerohedge"],
        "max_posts_per_user": 10,
        "category": "custom"
    }
    ```

    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹ä¼šåœ¨åå°æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡ ID
    """
    # æ£€æŸ¥ cookies
    cookies = load_cookies()
    if not cookies:
        raise HTTPException(
            status_code=400, detail="æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ setup æ¨¡å¼ç™»å½•"
        )

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = generate_task_id()

    # å‡†å¤‡ KOL åˆ—è¡¨
    kol_list = [
        (username, request.category or "api_request") for username in request.usernames
    ]

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    _scrape_tasks[task_id] = {
        "status": TaskStatus.PENDING,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "usernames": request.usernames,
        "stats": None,
        "error": None,
    }

    # åœ¨åå°æ‰§è¡Œçˆ¬å–
    background_tasks.add_task(
        _run_scrape_task,
        task_id,
        kol_list,
        request.max_posts_per_user,
    )

    return ScrapeResponse(
        success=True,
        message=f"çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…± {len(request.usernames)} ä¸ªç”¨æˆ·",
        task_id=task_id,
    )


@router.post("/scrape-detailed", response_model=ScrapeResponse)
def scrape_kols_detailed(
    request: BatchScrapeWithDetailsRequest,
    background_tasks: BackgroundTasks,
):
    """
    çˆ¬å– KOL åˆ—è¡¨ï¼ˆå¸¦è¯¦ç»†ä¿¡æ¯ï¼‰

    è¯·æ±‚ç¤ºä¾‹:
    ```json
    {
        "kols": [
            {"username": "elonmusk", "description": "Tesla CEO", "category": "tech"},
            {"username": "unusual_whales", "description": "Options flow data", "category": "news_flow"}
        ],
        "max_posts_per_user": 10
    }
    ```
    """
    # æ£€æŸ¥ cookies
    cookies = load_cookies()
    if not cookies:
        raise HTTPException(
            status_code=400, detail="æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ setup æ¨¡å¼ç™»å½•"
        )

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = generate_task_id()

    # å‡†å¤‡ KOL åˆ—è¡¨
    kol_list = [
        (kol.username, kol.category or "api_request", kol.description)
        for kol in request.kols
    ]

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    _scrape_tasks[task_id] = {
        "status": TaskStatus.PENDING,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "usernames": [kol.username for kol in request.kols],
        "stats": None,
        "error": None,
    }

    # åœ¨åå°æ‰§è¡Œçˆ¬å–
    background_tasks.add_task(
        _run_scrape_task_detailed,
        task_id,
        kol_list,
        request.max_posts_per_user,
    )

    return ScrapeResponse(
        success=True,
        message=f"çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…± {len(request.kols)} ä¸ªç”¨æˆ·",
        task_id=task_id,
    )


@router.post("/scrape-all-profiles", response_model=ScrapeResponse)
def scrape_all_kol_profiles(
    max_posts_per_user: int = 10,
    background_tasks: BackgroundTasks = None,
):
    """
    ğŸ”„ çˆ¬å–æ•°æ®åº“ä¸­æ‰€æœ‰ KOL çš„æœ€æ–°æ¨æ–‡

    ä» Supabase çš„ kol_profiles è¡¨è·å–æ‰€æœ‰ KOLï¼Œ
    ç„¶åçˆ¬å–æ¯ä¸ª KOL çš„æœ€æ–°æ¨æ–‡ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰ã€‚

    - å·²å­˜åœ¨çš„æ¨æ–‡ä¼šè‡ªåŠ¨è·³è¿‡
    - æ–°æ¨æ–‡ä¼šæ·»åŠ åˆ° kol_tweets è¡¨

    å‚æ•°ï¼š
    - max_posts_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•°é‡ (é»˜è®¤: 10)
    """
    # æ£€æŸ¥ cookies
    cookies = load_cookies()
    if not cookies:
        raise HTTPException(
            status_code=400, detail="æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ setup æ¨¡å¼ç™»å½•"
        )

    # è·å– Supabase å®¢æˆ·ç«¯
    supabase = get_supabase_client()
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase æœªè¿æ¥")

    # ä» kol_profiles è¡¨è·å–æ‰€æœ‰ KOL
    try:
        profiles_result = (
            supabase.table("kol_profiles")
            .select("username, category, description")
            .eq("is_active", True)
            .execute()
        )
        kol_profiles = profiles_result.data or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å– KOL åˆ—è¡¨å¤±è´¥: {str(e)}")

    if not kol_profiles:
        raise HTTPException(status_code=404, detail="kol_profiles è¡¨ä¸­æ²¡æœ‰æ´»è·ƒçš„ KOL")

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = generate_task_id()

    # å‡†å¤‡ KOL åˆ—è¡¨
    kol_list = [
        (
            profile["username"],
            profile.get("category") or "from_profiles",
            profile.get("description") or f"KOL from profiles table",
        )
        for profile in kol_profiles
    ]

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    _scrape_tasks[task_id] = {
        "status": TaskStatus.PENDING,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "source": "kol_profiles",
        "total_kols": len(kol_list),
        "usernames": [kol[0] for kol in kol_list],
        "stats": None,
        "error": None,
    }

    # åœ¨åå°æ‰§è¡Œçˆ¬å–
    background_tasks.add_task(
        _run_all_profiles_scrape_task,
        task_id,
        kol_list,
        max_posts_per_user,
    )

    return ScrapeResponse(
        success=True,
        message=f"ğŸš€ å¼€å§‹çˆ¬å– kol_profiles è¡¨ä¸­çš„ {len(kol_list)} ä¸ª KOL çš„æœ€æ–°æ¨æ–‡",
        task_id=task_id,
    )


@router.post("/scrape-categories", response_model=ScrapeResponse)
def scrape_predefined_categories(
    categories: List[str] = None,
    max_posts_per_user: int = 10,
    background_tasks: BackgroundTasks = None,
):
    """
    çˆ¬å–é¢„å®šä¹‰ç±»åˆ«çš„ KOL

    å¯é€‰ç±»åˆ«ï¼š
    - news_flow: æ–°é—»ä¸æ•°æ®æµ
    - short_macro: ç©ºå¤´ä¸å®è§‚
    - charts_data: å›¾è¡¨ä¸æ•°æ®
    - institutional: æœºæ„ä¸ä¸»æµ
    - retail_meme: æ•£æˆ·ä¸ Meme

    å¦‚æœä¸æŒ‡å®šç±»åˆ«ï¼Œå°†çˆ¬å–æ‰€æœ‰é¢„å®šä¹‰ KOL
    """
    # æ£€æŸ¥ cookies
    cookies = load_cookies()
    if not cookies:
        raise HTTPException(
            status_code=400, detail="æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ setup æ¨¡å¼ç™»å½•"
        )

    # éªŒè¯ç±»åˆ«
    if categories:
        invalid = [c for c in categories if c not in KOL_LIST]
        if invalid:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ•ˆçš„ç±»åˆ«: {invalid}ã€‚å¯ç”¨ç±»åˆ«: {list(KOL_LIST.keys())}",
            )

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = generate_task_id()

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    _scrape_tasks[task_id] = {
        "status": TaskStatus.PENDING,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "categories": categories or list(KOL_LIST.keys()),
        "stats": None,
        "error": None,
    }

    # åœ¨åå°æ‰§è¡Œçˆ¬å–
    background_tasks.add_task(
        _run_category_scrape_task,
        task_id,
        categories,
        max_posts_per_user,
    )

    total_kols = sum(
        len(kols)
        for cat, kols in KOL_LIST.items()
        if categories is None or cat in categories
    )

    return ScrapeResponse(
        success=True,
        message=f"çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…± {total_kols} ä¸ªé¢„å®šä¹‰ KOL",
        task_id=task_id,
    )


@router.get("/task/{task_id}", response_model=Dict)
def get_task_status(task_id: str):
    """
    è·å–çˆ¬å–ä»»åŠ¡çŠ¶æ€

    è¿”å›ä»»åŠ¡çš„å½“å‰çŠ¶æ€ã€ç»Ÿè®¡ä¿¡æ¯å’Œé”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    if task_id not in _scrape_tasks:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

    return _scrape_tasks[task_id]


@router.get("/tasks", response_model=List[Dict])
def list_tasks(limit: int = 10):
    """
    åˆ—å‡ºæœ€è¿‘çš„çˆ¬å–ä»»åŠ¡
    """
    tasks = list(_scrape_tasks.items())
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åº
    tasks.sort(key=lambda x: x[1].get("created_at", ""), reverse=True)

    return [{"task_id": task_id, **task_data} for task_id, task_data in tasks[:limit]]


@router.post("/scrape-single/{username}", response_model=Dict)
def scrape_single_user(
    username: str,
    max_posts: int = 10,
    category: Optional[str] = None,
    description: Optional[str] = None,
):
    """
    åŒæ­¥çˆ¬å–å•ä¸ªç”¨æˆ·ï¼ˆé˜»å¡å¼ï¼Œç­‰å¾…å®Œæˆï¼‰

    é€‚ç”¨äºæµ‹è¯•æˆ–éœ€è¦ç«‹å³è·å–ç»“æœçš„åœºæ™¯

    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹ä¼šé˜»å¡ç›´åˆ°çˆ¬å–å®Œæˆï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    """
    # æ£€æŸ¥ cookies
    cookies = load_cookies()
    if not cookies:
        raise HTTPException(
            status_code=400, detail="æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ setup æ¨¡å¼ç™»å½•"
        )

    try:
        scraper = BatchKOLScraper(
            headless=True,
            max_posts_per_user=max_posts,
        )

        kol_list = [(username, description or f"API request for @{username}")]

        # è¿™é‡Œä½¿ç”¨åŒæ­¥æ–¹å¼æ‰§è¡Œ
        stats = scraper.batch_scrape(kol_list=kol_list)

        return {
            "success": True,
            "username": username,
            "stats": stats,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"çˆ¬å–å¤±è´¥: {str(e)}")


# ============================================================
# åå°ä»»åŠ¡å‡½æ•°
# ============================================================


def _run_scrape_task(
    task_id: str,
    kol_list: List[tuple],
    max_posts_per_user: int,
):
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ˆç®€å•åˆ—è¡¨ï¼‰"""
    try:
        _scrape_tasks[task_id]["status"] = TaskStatus.RUNNING
        _scrape_tasks[task_id]["started_at"] = datetime.now(timezone.utc).isoformat()

        scraper = BatchKOLScraper(
            headless=True,
            max_posts_per_user=max_posts_per_user,
        )

        stats = scraper.batch_scrape(kol_list=kol_list)

        _scrape_tasks[task_id]["status"] = TaskStatus.COMPLETED
        _scrape_tasks[task_id]["completed_at"] = datetime.now(timezone.utc).isoformat()
        _scrape_tasks[task_id]["stats"] = stats

    except Exception as e:
        _scrape_tasks[task_id]["status"] = TaskStatus.FAILED
        _scrape_tasks[task_id]["error"] = str(e)
        _scrape_tasks[task_id]["failed_at"] = datetime.now(timezone.utc).isoformat()


def _run_scrape_task_detailed(
    task_id: str,
    kol_list: List[tuple],
    max_posts_per_user: int,
):
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ˆå¸¦è¯¦ç»†ä¿¡æ¯ï¼‰"""
    try:
        _scrape_tasks[task_id]["status"] = TaskStatus.RUNNING
        _scrape_tasks[task_id]["started_at"] = datetime.now(timezone.utc).isoformat()

        scraper = BatchKOLScraper(
            headless=True,
            max_posts_per_user=max_posts_per_user,
        )

        # è½¬æ¢ä¸º batch_scrape éœ€è¦çš„æ ¼å¼
        # kol_list æ ¼å¼: [(username, category, description), ...]
        # batch_scrape éœ€è¦: [(username, description), ...]
        simple_list = [(username, desc) for username, cat, desc in kol_list]

        stats = scraper.batch_scrape(kol_list=simple_list)

        _scrape_tasks[task_id]["status"] = TaskStatus.COMPLETED
        _scrape_tasks[task_id]["completed_at"] = datetime.now(timezone.utc).isoformat()
        _scrape_tasks[task_id]["stats"] = stats

    except Exception as e:
        _scrape_tasks[task_id]["status"] = TaskStatus.FAILED
        _scrape_tasks[task_id]["error"] = str(e)
        _scrape_tasks[task_id]["failed_at"] = datetime.now(timezone.utc).isoformat()


def _run_category_scrape_task(
    task_id: str,
    categories: Optional[List[str]],
    max_posts_per_user: int,
):
    """æ‰§è¡Œé¢„å®šä¹‰ç±»åˆ«çˆ¬å–ä»»åŠ¡"""
    try:
        _scrape_tasks[task_id]["status"] = TaskStatus.RUNNING
        _scrape_tasks[task_id]["started_at"] = datetime.now(timezone.utc).isoformat()

        scraper = BatchKOLScraper(
            headless=True,
            max_posts_per_user=max_posts_per_user,
        )

        stats = scraper.batch_scrape(categories=categories)

        _scrape_tasks[task_id]["status"] = TaskStatus.COMPLETED
        _scrape_tasks[task_id]["completed_at"] = datetime.now(timezone.utc).isoformat()
        _scrape_tasks[task_id]["stats"] = stats

    except Exception as e:
        _scrape_tasks[task_id]["status"] = TaskStatus.FAILED
        _scrape_tasks[task_id]["error"] = str(e)
        _scrape_tasks[task_id]["failed_at"] = datetime.now(timezone.utc).isoformat()


def _run_all_profiles_scrape_task(
    task_id: str,
    kol_list: List[tuple],
    max_posts_per_user: int,
):
    """
    æ‰§è¡Œ kol_profiles è¡¨ä¸­æ‰€æœ‰ KOL çš„çˆ¬å–ä»»åŠ¡

    kol_list æ ¼å¼: [(username, category, description), ...]
    """
    try:
        _scrape_tasks[task_id]["status"] = TaskStatus.RUNNING
        _scrape_tasks[task_id]["started_at"] = datetime.now(timezone.utc).isoformat()

        scraper = BatchKOLScraper(
            headless=True,
            max_posts_per_user=max_posts_per_user,
        )

        # è½¬æ¢ä¸º batch_scrape éœ€è¦çš„æ ¼å¼
        # batch_scrape æœŸæœ›: [(username, description), ...]
        # ä½†æˆ‘ä»¬è¦ä¼ é€’ categoryï¼Œæ‰€ä»¥ä½¿ç”¨å®Œæ•´çš„ä¸‰å…ƒç»„æ ¼å¼
        # è®© batch_scrape å†…éƒ¨å¤„ç†

        # ä½¿ç”¨è‡ªå®šä¹‰ kol_list è°ƒç”¨ batch_scrape
        # batch_scrape æ¥å— [(username, description), ...] æ ¼å¼
        simple_list = [(username, desc) for username, cat, desc in kol_list]

        stats = scraper.batch_scrape(kol_list=simple_list)

        _scrape_tasks[task_id]["status"] = TaskStatus.COMPLETED
        _scrape_tasks[task_id]["completed_at"] = datetime.now(timezone.utc).isoformat()
        _scrape_tasks[task_id]["stats"] = stats

        # è®¡ç®—è€—æ—¶
        started = datetime.fromisoformat(_scrape_tasks[task_id]["started_at"])
        completed = datetime.fromisoformat(_scrape_tasks[task_id]["completed_at"])
        duration = (completed - started).total_seconds()
        _scrape_tasks[task_id]["duration_seconds"] = duration
        _scrape_tasks[task_id][
            "duration_human"
        ] = f"{int(duration // 60)}åˆ†{int(duration % 60)}ç§’"

    except Exception as e:
        _scrape_tasks[task_id]["status"] = TaskStatus.FAILED
        _scrape_tasks[task_id]["error"] = str(e)
        _scrape_tasks[task_id]["failed_at"] = datetime.now(timezone.utc).isoformat()
