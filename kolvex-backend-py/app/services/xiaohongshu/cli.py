"""
CLI å‘½ä»¤è¡Œå…¥å£
"""

import argparse

from .scraper import XiaohongshuScraper
from .database import get_supabase_client, get_stats, get_recent_posts
from .config import DEFAULT_KEYWORDS


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦ç¾è‚¡çƒ­å¸–çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç™»å½•æ¨¡å¼ - é¦–æ¬¡è¿è¡Œï¼Œæ‰‹åŠ¨æ‰«ç ç™»å½•ä¿å­˜ cookies
  python -m app.services.xiaohongshu --login
  # æˆ–è€…
  python -m app.services.xiaohongshu --setup
  
  # çˆ¬å–é»˜è®¤å…³é”®è¯ï¼ˆç¾è‚¡ã€NVDA ç­‰ï¼‰
  python -m app.services.xiaohongshu
  
  # çˆ¬å–è‡ªå®šä¹‰å…³é”®è¯
  python -m app.services.xiaohongshu ç¾è‚¡ è‹±ä¼Ÿè¾¾ ç‰¹æ–¯æ‹‰
  
  # ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆå¯è§æµè§ˆå™¨ï¼‰
  python -m app.services.xiaohongshu --no-headless ç¾è‚¡
  
  # ä¸è·å–è¯¦æƒ…é¡µï¼ˆæ›´å¿«ï¼‰
  python -m app.services.xiaohongshu --no-details ç¾è‚¡
  
  # æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
  python -m app.services.xiaohongshu --stats
  
  # æŸ¥çœ‹æœ€è¿‘å¸–å­
  python -m app.services.xiaohongshu --recent 10

æ“ä½œæµç¨‹:
  1. é¦–æ¬¡ä½¿ç”¨ï¼Œå…ˆè¿è¡Œ --login è¿›è¡Œç™»å½•
  2. åœ¨æµè§ˆå™¨ä¸­æ‰«ç ç™»å½•
  3. ç™»å½•æˆåŠŸåç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œå¦‚æœæ²¡ååº”å°±æŒ‰å›è½¦é”®
  4. ç™»å½•å®Œæˆåå°±å¯ä»¥è¿è¡Œçˆ¬å–å‘½ä»¤äº†
        """,
    )

    parser.add_argument(
        "--setup", "--login",
        action="store_true",
        dest="setup",
        help="ç™»å½•æ¨¡å¼: æ‰“å¼€æµè§ˆå™¨æ‰‹åŠ¨æ‰«ç ç™»å½•å¹¶ä¿å­˜ cookies",
    )

    parser.add_argument(
        "--stats",
        action="store_true",
        help="æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯",
    )

    parser.add_argument(
        "--recent",
        type=int,
        metavar="N",
        help="æ˜¾ç¤ºæœ€è¿‘ N æ¡å¸–å­",
    )

    parser.add_argument(
        "keywords",
        nargs="*",
        help="è¦æœç´¢çš„å…³é”®è¯åˆ—è¡¨ï¼ˆä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯ï¼‰",
    )

    parser.add_argument(
        "--max-posts",
        type=int,
        default=20,
        help="æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬å–çš„å¸–å­æ•°é‡ (é»˜è®¤: 20)",
    )

    parser.add_argument(
        "--no-headless",
        action="store_true",
        help="ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰",
    )

    parser.add_argument(
        "--no-details",
        action="store_true",
        help="ä¸è·å–è¯¦æƒ…é¡µï¼ˆæ›´å¿«ä½†æ•°æ®è¾ƒå°‘ï¼‰",
    )

    parser.add_argument(
        "--cookies",
        type=str,
        default=None,
        help="Cookies æ–‡ä»¶è·¯å¾„",
    )

    parser.add_argument(
        "--stock-only",
        action="store_true",
        help="ä»…æ˜¾ç¤ºè‚¡ç¥¨ç›¸å…³å¸–å­ï¼ˆç”¨äº --recentï¼‰",
    )

    args = parser.parse_args()

    # æ˜¾ç¤ºç»Ÿè®¡
    if args.stats:
        supabase = get_supabase_client()
        if not supabase:
            print("âŒ æ— æ³•è¿æ¥ Supabase")
            return

        stats = get_stats(supabase)
        print("\nğŸ“Š å°çº¢ä¹¦å¸–å­æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  ğŸ“ æ€»å¸–å­æ•°: {stats['total']}")
        print(f"  ğŸ“ˆ è‚¡ç¥¨ç›¸å…³: {stats.get('stock_related', 0)}")

        if stats.get("by_keyword"):
            print("\nğŸ“‹ æŒ‰å…³é”®è¯ç»Ÿè®¡:")
            for kw, count in list(stats["by_keyword"].items())[:15]:
                print(f"  '{kw}': {count}")
        return

    # æ˜¾ç¤ºæœ€è¿‘å¸–å­
    if args.recent:
        supabase = get_supabase_client()
        if not supabase:
            print("âŒ æ— æ³•è¿æ¥ Supabase")
            return

        posts = get_recent_posts(
            supabase,
            limit=args.recent,
            stock_related_only=args.stock_only,
        )
        print(f"\nğŸ“‹ æœ€è¿‘ {len(posts)} æ¡å¸–å­:")
        print("=" * 60)

        for i, post in enumerate(posts, 1):
            title = post.get("title", "æ— æ ‡é¢˜")[:50]
            author = post.get("author_name", "æœªçŸ¥")
            likes = post.get("like_count", 0)
            keyword = post.get("search_keyword", "")
            sentiment = post.get("ai_sentiment", "")
            tickers = post.get("ai_tickers", [])

            print(f"\n{i}. {title}")
            print(f"   ğŸ‘¤ {author} | â¤ï¸ {likes} | ğŸ” {keyword}")

            if sentiment or tickers:
                ticker_str = ", ".join(tickers) if tickers else "æ— "
                print(f"   ğŸ¤– æƒ…ç»ª: {sentiment or 'æœªåˆ†æ'} | è‚¡ç¥¨: {ticker_str}")

            if post.get("permalink"):
                print(f"   ğŸ”— {post['permalink']}")

        return

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = XiaohongshuScraper(
        cookies_file=args.cookies,
        headless=not args.no_headless,
        max_posts=args.max_posts,
        fetch_details=not args.no_details,
    )

    try:
        if args.setup:
            # Setup æ¨¡å¼
            scraper.setup_mode()
        else:
            # çˆ¬å–æ¨¡å¼
            keywords = args.keywords if args.keywords else DEFAULT_KEYWORDS
            scraper.scrape(keywords=keywords)

    finally:
        scraper.close()


if __name__ == "__main__":
    main()

