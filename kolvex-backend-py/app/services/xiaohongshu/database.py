"""
å°çº¢ä¹¦å¸–å­æ•°æ®åº“æ“ä½œ
"""

import os
import json
import hashlib
from typing import Dict, Optional, Tuple, List
from datetime import datetime, timedelta, timezone

# Supabase ç›¸å…³å¯¼å…¥
try:
    from supabase import create_client, Client

    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    Client = None

from .config import DEFAULT_POST_MAX_AGE_DAYS


def get_supabase_client() -> Optional[Client]:
    """
    è·å– Supabase å®¢æˆ·ç«¯

    Returns:
        Optional[Client]: Supabase å®¢æˆ·ç«¯ï¼Œå¦‚æœæœªé…ç½®è¿”å› None
    """
    if not SUPABASE_AVAILABLE:
        print("âš ï¸ Supabase æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install supabase")
        return None

    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    from dotenv import load_dotenv

    load_dotenv()

    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_SERVICE_KEY") or os.getenv("SUPABASE_KEY")

    if not supabase_url or not supabase_key:
        print(
            "âš ï¸ Supabase é…ç½®æœªæ‰¾åˆ°ï¼Œè¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_SERVICE_KEY ç¯å¢ƒå˜é‡"
        )
        return None

    return create_client(supabase_url, supabase_key)


def compute_post_hash(note_id: str, content: str) -> str:
    """
    è®¡ç®—å¸–å­çš„å”¯ä¸€å“ˆå¸Œå€¼

    Args:
        note_id: å°çº¢ä¹¦ç¬”è®° ID
        content: å¸–å­å†…å®¹

    Returns:
        str: SHA256 å“ˆå¸Œå€¼çš„å‰ 16 ä½
    """
    unique_content = f"xhs:{note_id}:{content[:200]}"
    return hashlib.sha256(unique_content.encode()).hexdigest()[:16]


def post_exists(client: Client, post_hash: str) -> bool:
    """
    æ£€æŸ¥å¸–å­æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­

    Args:
        client: Supabase å®¢æˆ·ç«¯
        post_hash: å¸–å­å“ˆå¸Œå€¼

    Returns:
        bool: å¦‚æœå­˜åœ¨è¿”å› True
    """
    try:
        result = (
            client.table("xhs_posts")
            .select("id")
            .eq("post_hash", post_hash)
            .limit(1)
            .execute()
        )
        return len(result.data) > 0
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥å¸–å­æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
        return False


def note_id_exists(client: Client, note_id: str) -> bool:
    """
    æ£€æŸ¥ç¬”è®° ID æ˜¯å¦å·²å­˜åœ¨

    Args:
        client: Supabase å®¢æˆ·ç«¯
        note_id: å°çº¢ä¹¦ç¬”è®° ID

    Returns:
        bool: å¦‚æœå­˜åœ¨è¿”å› True
    """
    try:
        result = (
            client.table("xhs_posts")
            .select("id")
            .eq("note_id", note_id)
            .limit(1)
            .execute()
        )
        return len(result.data) > 0
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥ç¬”è®° ID æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
        return False


def insert_post(
    client: Client,
    post_data: Dict,
    max_age_days: int = DEFAULT_POST_MAX_AGE_DAYS,
    enable_ai_analysis: bool = True,
) -> Tuple[bool, Optional[int]]:
    """
    æ’å…¥å¸–å­åˆ° Supabase æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ä¸”ä¸å¤ªæ—§ï¼‰ï¼Œå¹¶è¿›è¡Œ AI åˆ†æ

    Args:
        client: Supabase å®¢æˆ·ç«¯
        post_data: å¸–å­æ•°æ®å­—å…¸ï¼ŒåŒ…å«:
            - note_id: å°çº¢ä¹¦ç¬”è®° ID
            - title: æ ‡é¢˜
            - content: å¸–å­å†…å®¹
            - author_name: ä½œè€…åç§°
            - author_id: ä½œè€… ID
            - author_avatar: ä½œè€…å¤´åƒ URL
            - cover_url: å°é¢å›¾ URL
            - image_urls: å›¾ç‰‡ URL åˆ—è¡¨
            - video_url: è§†é¢‘ URLï¼ˆå¦‚æœæ˜¯è§†é¢‘ç¬”è®°ï¼‰
            - like_count: ç‚¹èµæ•°
            - collect_count: æ”¶è—æ•°
            - comment_count: è¯„è®ºæ•°
            - share_count: åˆ†äº«æ•°
            - tags: æ ‡ç­¾åˆ—è¡¨
            - note_type: ç¬”è®°ç±»å‹ï¼ˆnormal/videoï¼‰
            - permalink: å¸–å­é“¾æ¥
            - created_at: åˆ›å»ºæ—¶é—´
        max_age_days: æœ€å¤§å¸–å­å¹´é¾„ï¼ˆå¤©ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„å¸–å­ä¸ä¼šè¢«æ’å…¥
        enable_ai_analysis: æ˜¯å¦å¯ç”¨ AI åˆ†æï¼ˆé»˜è®¤ Trueï¼‰

    Returns:
        Tuple[bool, Optional[int]]: (æ’å…¥æˆåŠŸè¿”å› Trueï¼Œå¸–å­ ID æˆ– None)
    """
    note_id = post_data.get("note_id")
    
    # æ£€æŸ¥ç¬”è®° ID æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿«é€Ÿå»é‡ï¼‰
    if note_id and note_id_exists(client, note_id):
        return False, None
    
    # æ£€æŸ¥å¸–å­æ—¶é—´ï¼Œå¦‚æœå¤ªæ—§å°±è·³è¿‡
    created_at_str = post_data.get("created_at")
    if created_at_str:
        try:
            # è§£ææ—¶é—´
            if isinstance(created_at_str, str):
                if created_at_str.endswith("Z"):
                    created_at_str = created_at_str[:-1] + "+00:00"
                post_time = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
            else:
                post_time = created_at_str

            # å¦‚æœæ˜¯ naive datetimeï¼Œå‡è®¾ä¸º UTC+8
            if post_time.tzinfo is None:
                post_time = post_time.replace(tzinfo=timezone(timedelta(hours=8)))

            cutoff_time = datetime.now(timezone.utc) - timedelta(days=max_age_days)

            if post_time < cutoff_time:
                print(
                    f"   â­ï¸ è·³è¿‡æ—§å¸–å­ ({str(created_at_str)[:10]}): {post_data.get('title', '')[:30]}..."
                )
                return False, None
        except Exception:
            # è§£æå¤±è´¥å°±ç»§ç»­æ’å…¥
            pass

    content = post_data.get("content", "") or post_data.get("title", "")
    post_hash = compute_post_hash(note_id or "", content)

    if post_exists(client, post_hash):
        return False, None

    # è¿›è¡Œ AI åˆ†æï¼ˆåœ¨æ’å…¥å‰ï¼‰
    ai_analysis = None
    if enable_ai_analysis:
        ai_analysis = _perform_ai_analysis(content, post_data.get("title", ""))

    try:
        # å¤„ç†åˆ—è¡¨å­—æ®µ - è½¬æ¢ä¸º JSON
        image_urls = post_data.get("image_urls", [])
        image_urls_json = json.dumps(image_urls) if image_urls else None
        
        tags = post_data.get("tags", [])
        tags_json = json.dumps(tags) if tags else None

        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨æˆªæ–­å­—ç¬¦ä¸²
        def safe_str(value, max_len: int) -> Optional[str]:
            if value is None:
                return None
            return str(value)[:max_len] if value else None

        data = {
            # åŸºç¡€ä¿¡æ¯ï¼ˆæŒ‰æ•°æ®åº“å­—æ®µé•¿åº¦æˆªæ–­ï¼‰
            "note_id": safe_str(note_id, 64),
            "post_hash": post_hash,
            "title": post_data.get("title"),
            "content": content,
            "author_name": safe_str(post_data.get("author_name"), 255),
            "author_id": safe_str(post_data.get("author_id"), 64),
            "author_avatar": post_data.get("author_avatar"),
            "cover_url": post_data.get("cover_url"),
            "image_urls": image_urls_json,
            "video_url": post_data.get("video_url"),
            "note_type": safe_str(post_data.get("note_type", "normal"), 20),
            "permalink": post_data.get("permalink"),
            # äº’åŠ¨æ•°æ®
            "like_count": post_data.get("like_count", 0),
            "collect_count": post_data.get("collect_count", 0),
            "comment_count": post_data.get("comment_count", 0),
            "share_count": post_data.get("share_count", 0),
            # æ ‡ç­¾
            "tags": tags_json,
            # æœç´¢å…³é”®è¯
            "search_keyword": safe_str(post_data.get("search_keyword"), 100),
            # æ—¶é—´
            "created_at": post_data.get("created_at"),
            "scraped_at": datetime.now(timezone.utc).isoformat(),
        }

        # æ·»åŠ  AI åˆ†æç»“æœ
        if ai_analysis:
            stock_related_data = ai_analysis.get("is_stock_related", {})
            trading_signal = ai_analysis.get("trading_signal")
            # å¤„ç† trading_signal å¯èƒ½æ˜¯ dict çš„æƒ…å†µ
            if isinstance(trading_signal, dict):
                trading_signal = trading_signal.get("action")
            
            data.update(
                {
                    # æƒ…æ„Ÿåˆ†æï¼ˆVARCHAR(20)ï¼‰
                    "ai_sentiment": safe_str(
                        ai_analysis.get("sentiment", {}).get("sentiment"), 20
                    ),
                    "ai_sentiment_confidence": ai_analysis.get("sentiment", {}).get(
                        "confidence"
                    ),
                    "ai_sentiment_reasoning": ai_analysis.get("sentiment", {}).get(
                        "reasoning"
                    ),
                    # è‚¡ç¥¨ä»£ç å’Œæ ‡ç­¾ (JSONB)
                    "ai_tickers": ai_analysis.get("tickers", []),
                    "ai_tags": ai_analysis.get("tags", []),
                    # æ‘˜è¦å’ŒæŠ•èµ„ä¿¡å·ï¼ˆVARCHAR(20)ï¼‰
                    "ai_summary": ai_analysis.get("summary"),
                    "ai_trading_signal": safe_str(trading_signal, 20),
                    # è‚¡å¸‚ç›¸å…³æ€§
                    "ai_is_stock_related": stock_related_data.get(
                        "is_stock_related", False
                    ),
                    "ai_stock_related_confidence": stock_related_data.get("confidence"),
                    "ai_stock_related_reason": stock_related_data.get("reason"),
                    # å…ƒæ•°æ®ï¼ˆVARCHAR(50)ï¼‰
                    "ai_analyzed_at": ai_analysis.get("analyzed_at"),
                    "ai_model": safe_str(ai_analysis.get("model"), 50),
                }
            )

        result = client.table("xhs_posts").insert(data).execute()
        # è·å–æ’å…¥çš„å¸–å­ ID
        post_id = result.data[0]["id"] if result.data else None
        return True, post_id
    except Exception as e:
        # å¯èƒ½æ˜¯å”¯ä¸€çº¦æŸå†²çªï¼ˆå¹¶å‘æƒ…å†µï¼‰
        if "duplicate" in str(e).lower() or "unique" in str(e).lower():
            return False, None
        print(f"âš ï¸ æ’å…¥å¸–å­å¤±è´¥: {e}")
        return False, None


# AI åˆ†æå™¨å•ä¾‹ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
_ai_analyzer = None


def _get_ai_analyzer():
    """è·å– AI åˆ†æå™¨å•ä¾‹"""
    global _ai_analyzer
    if _ai_analyzer is None:
        try:
            from app.services.ai import TweetAnalyzerSync, OllamaClientSync

            client = OllamaClientSync()
            # å…ˆæ£€æŸ¥ AI æœåŠ¡æ˜¯å¦å¯ç”¨
            if client.health_check():
                _ai_analyzer = TweetAnalyzerSync(client)
                print("ğŸ¤– AI åˆ†æå™¨å·²åˆå§‹åŒ–")
            else:
                print("âš ï¸ AI æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ AI åˆ†æ")
                _ai_analyzer = False  # æ ‡è®°ä¸ºä¸å¯ç”¨
        except Exception as e:
            print(f"âš ï¸ AI åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            _ai_analyzer = False
    return _ai_analyzer if _ai_analyzer else None


def _perform_ai_analysis(content: str, title: str = "") -> Optional[Dict]:
    """
    æ‰§è¡Œ AI åˆ†æï¼ˆåŒæ­¥ï¼‰

    Args:
        content: å¸–å­å†…å®¹
        title: å¸–å­æ ‡é¢˜

    Returns:
        Optional[Dict]: åˆ†æç»“æœï¼Œå¤±è´¥è¿”å› None
    """
    analyzer = _get_ai_analyzer()
    if not analyzer:
        return None

    try:
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹è¿›è¡Œåˆ†æ
        full_text = f"{title}\n{content}" if title else content
        analysis = analyzer.basic_analysis(full_text)
        sentiment = analysis.get("sentiment", {}).get("sentiment", "neutral")
        tickers = analysis.get("tickers", [])
        print(f"   ğŸ¤– AI: {sentiment} | è‚¡ç¥¨: {tickers if tickers else 'æ— '}")
        return analysis
    except Exception as e:
        print(f"   âš ï¸ AI åˆ†æå¤±è´¥: {e}")
        return None


def get_stats(client: Client) -> Dict:
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    Returns:
        Dict: åŒ…å«æ€»æ•°ã€å„å…³é”®è¯æ•°é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # æ€»å¸–å­æ•°
        total_result = client.table("xhs_posts").select("id", count="exact").execute()
        total = total_result.count or 0

        # æŒ‰æœç´¢å…³é”®è¯ç»Ÿè®¡
        by_keyword = {}
        try:
            result = client.table("xhs_posts").select("search_keyword").execute()
            for row in result.data:
                keyword = row.get("search_keyword") or "æœªçŸ¥"
                by_keyword[keyword] = by_keyword.get(keyword, 0) + 1
        except Exception:
            pass

        # æŒ‰è‚¡ç¥¨ç›¸å…³æ€§ç»Ÿè®¡
        stock_related_count = 0
        try:
            result = (
                client.table("xhs_posts")
                .select("id", count="exact")
                .eq("ai_is_stock_related", True)
                .execute()
            )
            stock_related_count = result.count or 0
        except Exception:
            pass

        return {
            "total": total,
            "by_keyword": dict(
                sorted(by_keyword.items(), key=lambda x: x[1], reverse=True)
            ),
            "stock_related": stock_related_count,
        }
    except Exception as e:
        print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {"total": 0, "by_keyword": {}, "stock_related": 0}


def get_recent_posts(
    client: Client,
    limit: int = 50,
    keyword: str = None,
    stock_related_only: bool = False,
) -> List[Dict]:
    """
    è·å–æœ€è¿‘çš„å¸–å­

    Args:
        client: Supabase å®¢æˆ·ç«¯
        limit: è¿”å›æ•°é‡é™åˆ¶
        keyword: ç­›é€‰å…³é”®è¯
        stock_related_only: æ˜¯å¦åªè¿”å›è‚¡ç¥¨ç›¸å…³å¸–å­

    Returns:
        List[Dict]: å¸–å­åˆ—è¡¨
    """
    try:
        query = client.table("xhs_posts").select("*").order("scraped_at", desc=True)

        if keyword:
            query = query.eq("search_keyword", keyword)

        if stock_related_only:
            query = query.eq("ai_is_stock_related", True)

        result = query.limit(limit).execute()
        return result.data or []
    except Exception as e:
        print(f"âš ï¸ è·å–å¸–å­å¤±è´¥: {e}")
        return []

