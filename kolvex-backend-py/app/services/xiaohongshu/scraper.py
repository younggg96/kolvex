"""
æ ¸å¿ƒçˆ¬è™«ç±» - XiaohongshuScraper
"""

import random
import time
import re
from typing import List, Dict, Set, Tuple, Optional
from urllib.parse import quote, urljoin

# Playwright ç›¸å…³å¯¼å…¥
try:
    from playwright.sync_api import sync_playwright, Page

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    Page = None

from .config import (
    COOKIES_FILE,
    USER_AGENTS,
    BASE_URL,
    SEARCH_URL,
    DEFAULT_MAX_POSTS,
    DEFAULT_DELAY_BETWEEN_POSTS,
    DEFAULT_DELAY_DURING_SCROLL,
    DEFAULT_MAX_SCROLLS,
    PAGE_LOAD_TIMEOUT,
    ELEMENT_WAIT_TIMEOUT,
    NETWORK_IDLE_TIMEOUT,
    SETUP_LOGIN_TIMEOUT,
    BROWSER_ARGS,
    BROWSER_VIEWPORT,
    BROWSER_LOCALE,
    BROWSER_TIMEZONE,
    SELECTORS,
)
from .database import (
    get_supabase_client,
    insert_post,
    get_stats,
    note_id_exists,
)
from .extractors import (
    extract_note_card,
    extract_note_detail,
    extract_all_note_cards,
    merge_note_data,
    extract_note_id_from_url,
)


def random_sleep(min_sec: float, max_sec: float, message: str = None) -> None:
    """
    éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º

    Args:
        min_sec: æœ€å°å»¶è¿Ÿç§’æ•°
        max_sec: æœ€å¤§å»¶è¿Ÿç§’æ•°
        message: å¯é€‰çš„æç¤ºä¿¡æ¯
    """
    delay = random.uniform(min_sec, max_sec)
    if message:
        print(f"â³ {message} (ç­‰å¾… {delay:.1f}s)")
    time.sleep(delay)


def load_cookies(cookies_file: str = None) -> Optional[List[Dict]]:
    """åŠ è½½ä¿å­˜çš„ cookies"""
    import os
    import json

    if cookies_file is None:
        cookies_file = str(COOKIES_FILE)

    if os.path.exists(cookies_file):
        try:
            with open(cookies_file, "r") as f:
                cookies = json.load(f)
                print(f"ğŸª å·²åŠ è½½ cookies: {cookies_file}")
                return cookies
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ cookies å¤±è´¥: {e}")
    return None


def save_cookies(cookies: List[Dict], cookies_file: str = None) -> bool:
    """ä¿å­˜ cookies åˆ°æ–‡ä»¶"""
    import json

    if cookies_file is None:
        cookies_file = str(COOKIES_FILE)

    try:
        with open(cookies_file, "w") as f:
            json.dump(cookies, f, indent=2)
        print(f"ğŸª Cookies å·²ä¿å­˜åˆ°: {cookies_file}")
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ cookies å¤±è´¥: {e}")
        return False


class XiaohongshuScraper:
    """
    å°çº¢ä¹¦ç¾è‚¡å¸–å­çˆ¬è™«ç±»

    æ”¯æŒä¸¤ç§æ¨¡å¼:
    1. Setup Mode: headless=Falseï¼Œç”¨äºæ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ cookies
    2. Scrape Mode: åˆ©ç”¨å·²ä¿å­˜çš„ cookies è¿›è¡Œæ‰¹é‡çˆ¬å–
    """

    def __init__(
        self,
        cookies_file: str = None,
        headless: bool = False,
        max_posts: int = DEFAULT_MAX_POSTS,
        delay_between_posts: Tuple[float, float] = DEFAULT_DELAY_BETWEEN_POSTS,
        delay_during_scroll: Tuple[float, float] = DEFAULT_DELAY_DURING_SCROLL,
        fetch_details: bool = True,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            cookies_file: cookies æ–‡ä»¶è·¯å¾„
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            max_posts: æ¯æ¬¡æœç´¢æœ€å¤šçˆ¬å–çš„å¸–å­æ•°é‡
            delay_between_posts: å¸–å­é—´å»¶è¿ŸèŒƒå›´ (min, max) ç§’
            delay_during_scroll: æ»šåŠ¨æ—¶å»¶è¿ŸèŒƒå›´ (min, max) ç§’
            fetch_details: æ˜¯å¦æŠ“å–è¯¦æƒ…é¡µï¼ˆä¼šæ›´æ…¢ä½†æ•°æ®æ›´å®Œæ•´ï¼‰
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise RuntimeError(
                "âŒ Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ:\n"
                "   pip install playwright\n"
                "   playwright install chromium"
            )

        self.cookies_file = str(cookies_file or COOKIES_FILE)
        self.headless = headless
        self.max_posts = max_posts
        self.delay_between_posts = delay_between_posts
        self.delay_during_scroll = delay_during_scroll
        self.fetch_details = fetch_details

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "keywords_processed": 0,
            "posts_scraped": 0,
            "posts_new": 0,
            "posts_duplicate": 0,
            "posts_failed": 0,
        }

        # åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
        self.supabase = get_supabase_client()
        if self.supabase:
            print("âœ… Supabase è¿æ¥æˆåŠŸ")
        else:
            print("âš ï¸ Supabase æœªè¿æ¥ï¼Œå°†åªæ‰“å°å¸–å­è€Œä¸ä¿å­˜")

    def setup_mode(self, timeout: int = SETUP_LOGIN_TIMEOUT) -> bool:
        """
        Setup æ¨¡å¼: æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•

        å¢å¼ºç‰ˆæœ¬ï¼šæ”¯æŒè‡ªåŠ¨æ£€æµ‹ + æ‰‹åŠ¨ç¡®è®¤ä¸¤ç§æ–¹å¼

        Args:
            timeout: ç­‰å¾…ç™»å½•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: ç™»å½•æˆåŠŸè¿”å› True
        """
        print("\n" + "=" * 60)
        print("ğŸ”§ SETUP MODE - è¯·æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦")
        print("=" * 60)

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=False,  # å¿…é¡»æœ‰å¤´æ¨¡å¼æ‰èƒ½çœ‹åˆ°æµè§ˆå™¨
                args=BROWSER_ARGS,
            )

            context = browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                viewport=BROWSER_VIEWPORT,
                locale=BROWSER_LOCALE,
                timezone_id=BROWSER_TIMEZONE,
            )

            page = context.new_page()
            self._add_stealth_scripts(page)

            try:
                print("ğŸ“± æ­£åœ¨æ‰“å¼€å°çº¢ä¹¦...")
                page.goto(BASE_URL, wait_until="domcontentloaded", timeout=60000)

                print("\n" + "âš ï¸ " * 20)
                print("ã€é‡è¦æç¤ºã€‘")
                print("1. è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­ï¼Œä½¿ç”¨æ‰‹æœºå°çº¢ä¹¦ App æ‰«ç ç™»å½•")
                print("2. ç™»å½•æˆåŠŸåï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹ã€‚")
                print(
                    "3. å¦‚æœç¨‹åºæ²¡æœ‰ååº”ï¼Œè¯·åœ¨ä¸‹æ–¹æ§åˆ¶å°æŒ‰ã€å›è½¦é”®ã€‘å¼ºåˆ¶ä¿å­˜ Cookieï¼"
                )
                print("âš ï¸ " * 20 + "\n")

                # æ–¹æ³• A: è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€
                try:
                    page.wait_for_selector(
                        '.user-avatar, .user-info, [class*="user-menu"], .side-bar .user',
                        timeout=timeout * 1000,
                        state="visible",
                    )
                    print("âœ… è‡ªåŠ¨æ£€æµ‹åˆ°å·²ç™»å½•ï¼")
                except Exception:
                    # æ–¹æ³• B: æ‰‹åŠ¨ç¡®è®¤ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
                    print("â³ è‡ªåŠ¨æ£€æµ‹è¶…æ—¶ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤...")
                    input("ğŸ‘‰ ç™»å½•å®Œæˆåï¼Œè¯·åœ¨æ­¤å¤„æŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")

                # ä¿å­˜ cookies
                cookies = context.cookies()
                if save_cookies(cookies, self.cookies_file):
                    print(f"âœ… Cookies å·²ä¿å­˜æˆåŠŸï¼æ–‡ä»¶è·¯å¾„: {self.cookies_file}")
                    return True
                else:
                    print("âŒ Cookies ä¿å­˜å¤±è´¥")
                    return False

            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                return False

            finally:
                print("\næµè§ˆå™¨å°†åœ¨ 3 ç§’åå…³é—­...")
                time.sleep(3)
                browser.close()

        return False

    def _wait_for_manual_login(self, context, page: "Page", timeout: int = 300) -> bool:
        """
        ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ‰«ç ç™»å½•

        å½“æ£€æµ‹åˆ°ç™»å½•å¼¹çª—æ—¶ï¼Œæš‚åœçˆ¬å–ï¼Œç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•åç»§ç»­

        Args:
            context: æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿å­˜ cookiesï¼‰
            page: Playwright é¡µé¢å¯¹è±¡
            timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: ç™»å½•æˆåŠŸè¿”å› True
        """
        print("\n" + "=" * 60)
        print("ğŸ”‘ æ£€æµ‹åˆ°éœ€è¦ç™»å½•ï¼")
        print("=" * 60)
        print("\nã€è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œã€‘")
        print("1. åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨æ‰‹æœºå°çº¢ä¹¦ App æ‰«æäºŒç»´ç ç™»å½•")
        print("2. ç™»å½•æˆåŠŸåï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹")
        print("3. å¦‚æœç¨‹åºæ²¡æœ‰ååº”ï¼Œè¯·åœ¨ç»ˆç«¯æŒ‰ã€å›è½¦é”®ã€‘å¼ºåˆ¶ç»§ç»­")
        print("\n" + "=" * 60 + "\n")

        try:
            # æ–¹æ³• A: è‡ªåŠ¨æ£€æµ‹ç™»å½•æˆåŠŸ
            try:
                page.wait_for_selector(
                    '.user-avatar, .user-info, [class*="user-menu"], .side-bar .user',
                    timeout=timeout * 1000,
                    state="visible",
                )
                print("âœ… è‡ªåŠ¨æ£€æµ‹åˆ°å·²ç™»å½•ï¼")
            except Exception:
                # æ–¹æ³• B: æ‰‹åŠ¨ç¡®è®¤ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
                print("â³ è‡ªåŠ¨æ£€æµ‹è¶…æ—¶ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤...")
                input("ğŸ‘‰ ç™»å½•å®Œæˆåï¼Œè¯·åœ¨æ­¤å¤„æŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")

            # ä¿å­˜ cookies
            cookies = context.cookies()
            if save_cookies(cookies, self.cookies_file):
                print(f"âœ… Cookies å·²ä¿å­˜ï¼æ–‡ä»¶è·¯å¾„: {self.cookies_file}")
                print("ğŸš€ ç»§ç»­çˆ¬å–...\n")
                return True
            else:
                print("âš ï¸ Cookies ä¿å­˜å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•çˆ¬å–")
                return True

        except Exception as e:
            print(f"âŒ ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _check_login_required(self, page: "Page") -> bool:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦éœ€è¦ç™»å½•

        Args:
            page: Playwright é¡µé¢å¯¹è±¡

        Returns:
            bool: éœ€è¦ç™»å½•è¿”å› True
        """
        # æ£€æµ‹ç™»å½•å¼¹çª—é€‰æ‹©å™¨
        login_popup_selectors = [
            '[class*="login-modal"]',
            '[class*="login-container"]',
            '[class*="login-dialog"]',
            '[class*="login-popup"]',
        ]

        for selector in login_popup_selectors:
            try:
                popup = page.query_selector(selector)
                if popup and popup.is_visible():
                    return True
            except Exception:
                continue

        # æ£€æŸ¥é¡µé¢å†…å®¹
        try:
            page_html = page.content()
            if (
                "ç™»å½•åæŸ¥çœ‹" in page_html
                or "è¯·ç™»å½•" in page_html
                or "ç™»å½•åæŸ¥çœ‹æœç´¢ç»“æœ" in page_html
            ):
                return True
        except Exception:
            pass

        return False

    def _handle_login_if_needed(self, context, page: "Page") -> bool:
        """
        å¦‚æœéœ€è¦ç™»å½•ï¼Œåˆ™ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•

        Args:
            context: æµè§ˆå™¨ä¸Šä¸‹æ–‡
            page: Playwright é¡µé¢å¯¹è±¡

        Returns:
            bool: å¤„ç†æˆåŠŸè¿”å› True
        """
        if self._check_login_required(page):
            return self._wait_for_manual_login(context, page)
        return True

    def _add_stealth_scripts(self, page: "Page") -> None:
        """æ·»åŠ åæ£€æµ‹è„šæœ¬"""
        page.add_init_script(
            """
            // éšè— webdriver å±æ€§
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            // æ¨¡æ‹ŸçœŸå®çš„ plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            // æ¨¡æ‹ŸçœŸå®çš„ languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-CN', 'zh', 'en-US', 'en']
            });
            
            // éšè—è‡ªåŠ¨åŒ–ç—•è¿¹
            window.chrome = { runtime: {} };
            
            // è¦†ç›– permissions
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
        """
        )

    def _build_search_url(self, keyword: str) -> str:
        """
        æ„å»ºæœç´¢ URL

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            str: å®Œæ•´çš„æœç´¢ URL
        """
        encoded_keyword = quote(keyword)
        return f"{SEARCH_URL}?keyword={encoded_keyword}&source=unknown"

    def _scrape_search_results(self, context, page: "Page", keyword: str) -> List[Dict]:
        """
        çˆ¬å–æœç´¢ç»“æœé¡µé¢

        Args:
            context: æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿å­˜ cookiesï¼‰
            page: Playwright é¡µé¢å¯¹è±¡
            keyword: æœç´¢å…³é”®è¯

        Returns:
            List[Dict]: çˆ¬å–åˆ°çš„å¸–å­åˆ—è¡¨
        """
        search_url = self._build_search_url(keyword)
        collected_posts = []
        seen_note_ids: Set[str] = set()

        print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
        print(f"   URL: {search_url}")

        try:
            page.goto(
                search_url, wait_until="domcontentloaded", timeout=PAGE_LOAD_TIMEOUT
            )
            random_sleep(3, 5)

            # ğŸ”‘ æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•ï¼Œå¦‚éœ€è¦åˆ™ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ‰«ç 
            if self._check_login_required(page):
                self._wait_for_manual_login(context, page)
                # ç™»å½•ååˆ·æ–°é¡µé¢
                page.goto(
                    search_url, wait_until="domcontentloaded", timeout=PAGE_LOAD_TIMEOUT
                )
                random_sleep(2, 3)

            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                selectors_to_try = [
                    "section.note-item",
                    ".note-item",
                    '[class*="note-item"]',
                    ".feeds-page section",
                    'a[href*="/explore/"]',
                ]

                found = False
                for selector in selectors_to_try:
                    try:
                        page.wait_for_selector(
                            selector, timeout=ELEMENT_WAIT_TIMEOUT, state="visible"
                        )
                        found = True
                        print(f"   âœ… æ‰¾åˆ°å†…å®¹é€‰æ‹©å™¨: {selector}")
                        break
                    except Exception:
                        continue

                if not found:
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                    if self._check_login_required(page):
                        print("   ğŸ”‘ ä»éœ€è¦ç™»å½•ï¼Œç­‰å¾…ç”¨æˆ·æ‰«ç ...")
                        self._wait_for_manual_login(context, page)
                        # ç™»å½•ååˆ·æ–°é¡µé¢
                        page.goto(
                            search_url,
                            wait_until="domcontentloaded",
                            timeout=PAGE_LOAD_TIMEOUT,
                        )
                        random_sleep(2, 3)

                        # å†æ¬¡å°è¯•æŸ¥æ‰¾å†…å®¹
                        for selector in selectors_to_try:
                            try:
                                page.wait_for_selector(
                                    selector, timeout=5000, state="visible"
                                )
                                found = True
                                print(f"   âœ… ç™»å½•åæ‰¾åˆ°å†…å®¹: {selector}")
                                break
                            except Exception:
                                continue

                    if not found:
                        # æˆªå›¾è°ƒè¯•
                        debug_path = f"debug_search_{keyword[:10]}.png"
                        page.screenshot(path=debug_path)
                        print(f"   âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœï¼Œæˆªå›¾å·²ä¿å­˜: {debug_path}")
                        print(f"   ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦ç™»å½•ï¼Œè¯·è¿è¡Œ --login è¿›è¡Œç™»å½•")
                        return []

            except Exception as e:
                print(f"   âš ï¸ åŠ è½½æœç´¢ç»“æœè¶…æ—¶: {e}")
                return []

            # æ»šåŠ¨å’Œçˆ¬å–
            scroll_count = 0
            no_new_count = 0

            while (
                len(collected_posts) < self.max_posts
                and scroll_count < DEFAULT_MAX_SCROLLS
            ):
                scroll_count += 1

                # ä½¿ç”¨ JS æ‰¹é‡æå–æ‰€æœ‰ç¬”è®°å¡ç‰‡ï¼ˆé¿å…å…ƒç´ å¤±æ•ˆé—®é¢˜ï¼‰
                cards_data = extract_all_note_cards(page)

                new_in_batch = 0

                for card_data in cards_data:
                    if len(collected_posts) >= self.max_posts:
                        break

                    try:
                        note_id = card_data.get("note_id")
                        if not note_id or note_id in seen_note_ids:
                            continue

                        seen_note_ids.add(note_id)
                        card_data["search_keyword"] = keyword

                        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
                        if self.supabase and note_id_exists(self.supabase, note_id):
                            self.stats["posts_duplicate"] += 1
                            continue

                        # æ˜¯å¦è·å–è¯¦æƒ…
                        if self.fetch_details and card_data.get("permalink"):
                            print(
                                f"   ğŸ“– [{len(collected_posts)+1}/{self.max_posts}] è·å–: {card_data.get('title', '')[:30]}..."
                            )
                            detail_data = self._fetch_note_detail(
                                context, page, card_data["permalink"]
                            )
                            if detail_data:
                                card_data = merge_note_data(card_data, detail_data)
                            random_sleep(*self.delay_between_posts)

                        collected_posts.append(card_data)
                        new_in_batch += 1

                        # ä¿å­˜åˆ° Supabaseï¼ˆå« AI åˆ†æï¼‰
                        if self.supabase:
                            inserted, post_id = insert_post(self.supabase, card_data)
                            if inserted:
                                self.stats["posts_new"] += 1
                                print(
                                    f"   âœ… [{len(collected_posts)}/{self.max_posts}] {card_data.get('title', '')[:40]}..."
                                )
                            else:
                                self.stats["posts_duplicate"] += 1
                        else:
                            print(
                                f"   ğŸ“ [{len(collected_posts)}/{self.max_posts}] {card_data.get('title', '')[:40]}..."
                            )

                    except Exception as e:
                        print(f"   âš ï¸ å¤„ç†å¡ç‰‡å¤±è´¥: {e}")
                        self.stats["posts_failed"] += 1
                        continue

                if new_in_batch == 0:
                    no_new_count += 1
                    if no_new_count >= 3:
                        print(f"   â„¹ï¸ è¿ç»­ {no_new_count} æ¬¡æ— æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨")
                        break
                else:
                    no_new_count = 0

                if len(collected_posts) >= self.max_posts:
                    break

                # æ»šåŠ¨é¡µé¢
                page.evaluate(
                    """
                    window.scrollBy({
                        top: window.innerHeight * 0.8,
                        behavior: 'smooth'
                    });
                """
                )

                random_sleep(*self.delay_during_scroll)

                try:
                    page.wait_for_load_state(
                        "networkidle", timeout=NETWORK_IDLE_TIMEOUT
                    )
                except Exception:
                    pass

            self.stats["posts_scraped"] += len(collected_posts)
            print(f"\n   ğŸ“Š å…³é”®è¯ '{keyword}': çˆ¬å– {len(collected_posts)} æ¡å¸–å­")

        except Exception as e:
            print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
            # æˆªå›¾ä¿å­˜é”™è¯¯ç°åœº
            try:
                page.screenshot(path=f"error_search_{keyword[:10]}.png")
            except Exception:
                pass

        return collected_posts

    def _fetch_note_detail(self, context, page: "Page", url: str) -> Optional[Dict]:
        """
        è·å–ç¬”è®°è¯¦æƒ…é¡µå†…å®¹

        Args:
            context: æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿å­˜ cookiesï¼‰
            page: Playwright é¡µé¢å¯¹è±¡
            url: ç¬”è®°è¯¦æƒ…é¡µ URL

        Returns:
            Optional[Dict]: è¯¦æƒ…æ•°æ®
        """
        # ä¿å­˜å½“å‰ URL ä»¥ä¾¿è¿”å›
        original_url = page.url

        try:
            page.goto(url, wait_until="domcontentloaded", timeout=PAGE_LOAD_TIMEOUT)
            random_sleep(2, 4)

            # æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•
            if self._check_login_required(page):
                self._wait_for_manual_login(context, page)
                # é‡æ–°åŠ è½½è¯¦æƒ…é¡µ
                page.goto(url, wait_until="domcontentloaded", timeout=PAGE_LOAD_TIMEOUT)
                random_sleep(1, 2)

            detail_data = extract_note_detail(page)
            return detail_data

        except Exception as e:
            print(f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: {e}")
            return None

        finally:
            # è¿”å›æœç´¢ç»“æœé¡µé¢
            try:
                page.goto(
                    original_url,
                    wait_until="domcontentloaded",
                    timeout=PAGE_LOAD_TIMEOUT,
                )
                random_sleep(1, 2)
            except Exception:
                pass

    def scrape(self, keywords: List[str]) -> Dict:
        """
        çˆ¬å–å¤šä¸ªå…³é”®è¯çš„æœç´¢ç»“æœ

        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not keywords:
            print("âŒ æ²¡æœ‰è¦æœç´¢çš„å…³é”®è¯")
            return self.stats

        # æ£€æŸ¥ cookies
        cookies = load_cookies(self.cookies_file)
        if cookies is None:
            print("\nâš ï¸ æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œå°†ä»¥æ¸¸å®¢æ¨¡å¼è¿è¡Œï¼ˆå¯èƒ½åŠŸèƒ½å—é™ï¼‰")
            print("å»ºè®®å…ˆè¿è¡Œ Setup Mode è¿›è¡Œç™»å½•:")
            print("   python -m app.services.xiaohongshu --setup")

        print("\n" + "=" * 60)
        print(f"ğŸš€ å¼€å§‹çˆ¬å–å°çº¢ä¹¦ç¾è‚¡å¸–å­")
        print(f"ğŸ“‹ å…³é”®è¯: {', '.join(keywords)}")
        print(f"ğŸ“ æ¯å…³é”®è¯æœ€å¤š: {self.max_posts} æ¡å¸–å­")
        print(f"ğŸ“– è·å–è¯¦æƒ…: {'æ˜¯' if self.fetch_details else 'å¦'}")
        print(f"ğŸ’¾ å­˜å‚¨: {'Supabase' if self.supabase else 'ä»…æ‰“å°'}")
        print("=" * 60)

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=self.headless,
                args=BROWSER_ARGS,
            )

            context = browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                viewport=BROWSER_VIEWPORT,
                locale=BROWSER_LOCALE,
                timezone_id=BROWSER_TIMEZONE,
            )

            # åŠ è½½ cookiesï¼ˆå¦‚æœæœ‰ï¼‰
            if cookies:
                context.add_cookies(cookies)

            page = context.new_page()
            self._add_stealth_scripts(page)

            # ä¿å­˜ context å¼•ç”¨ä¾›å†…éƒ¨æ–¹æ³•ä½¿ç”¨
            self._current_context = context

            try:
                for i, keyword in enumerate(keywords, 1):
                    print(f"\n[{i}/{len(keywords)}] ğŸ” å…³é”®è¯: {keyword}")

                    self._scrape_search_results(context, page, keyword)
                    self.stats["keywords_processed"] += 1

                    # å…³é”®è¯é—´å»¶è¿Ÿ
                    if i < len(keywords):
                        random_sleep(5, 10, "åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå…³é”®è¯å‰ç­‰å¾…")

            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")

            except Exception as e:
                print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")

            finally:
                # æ›´æ–° cookies
                try:
                    new_cookies = context.cookies()
                    save_cookies(new_cookies, self.cookies_file)
                except Exception:
                    pass

                browser.close()

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()

        return self.stats

    def _print_final_stats(self) -> None:
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 60)
        print(f"  ğŸ” å¤„ç†å…³é”®è¯: {self.stats['keywords_processed']}")
        print(f"  ğŸ“ çˆ¬å–å¸–å­: {self.stats['posts_scraped']}")
        print(f"  ğŸ†• æ–°å¢å¸–å­: {self.stats['posts_new']}")
        print(f"  ğŸ“‹ é‡å¤å¸–å­: {self.stats['posts_duplicate']}")
        print(f"  âŒ å¤±è´¥å¸–å­: {self.stats['posts_failed']}")
        print("=" * 60)

        # æ•°æ®åº“ç»Ÿè®¡
        if self.supabase:
            db_stats = get_stats(self.supabase)
            print(f"\nğŸ“¦ Supabase æ•°æ®åº“æ€»è®¡:")
            print(f"  ğŸ“ æ€»å¸–å­æ•°: {db_stats['total']}")
            print(f"  ğŸ“ˆ è‚¡ç¥¨ç›¸å…³: {db_stats.get('stock_related', 0)}")

            if db_stats.get("by_keyword"):
                print(f"\nğŸ“‹ æŒ‰å…³é”®è¯ç»Ÿè®¡:")
                for kw, count in list(db_stats["by_keyword"].items())[:10]:
                    print(f"  '{kw}': {count}")

    def close(self) -> None:
        """å…³é—­èµ„æºï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰"""
        pass


# ============================================================
# ç›´æ¥è¿è¡Œå…¥å£
# ============================================================
if __name__ == "__main__":
    import argparse
    import sys

    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦é‡‡é›†å·¥å…·")

    # æ·»åŠ  --login å‚æ•°
    parser.add_argument(
        "--login",
        "--setup",
        action="store_true",
        dest="login",
        help="å¯åŠ¨ç™»å½•æ¨¡å¼ (æ‰‹åŠ¨æ‰«ç ä¿å­˜ Cookie)",
    )

    # æ·»åŠ  --keywords å‚æ•°
    parser.add_argument(
        "--keywords", nargs="+", help="å¼€å§‹çˆ¬å–æŒ‡å®šçš„å…³é”®è¯ï¼Œä¾‹å¦‚: --keywords ç¾è‚¡ æŠ•èµ„"
    )

    # æ·»åŠ  --max-posts å‚æ•°
    parser.add_argument(
        "--max-posts",
        type=int,
        default=20,
        help="æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬å–çš„å¸–å­æ•°é‡ (é»˜è®¤: 20)",
    )

    # è§£æå‚æ•°
    args = parser.parse_args()

    # åˆå§‹åŒ–çˆ¬è™«
    scraper = XiaohongshuScraper(
        headless=True,
        max_posts=args.max_posts,
    )

    if args.login:
        # æ¨¡å¼ 1: è¿è¡Œç™»å½•
        scraper.setup_mode()

    elif args.keywords:
        # æ¨¡å¼ 2: è¿è¡Œçˆ¬è™«
        scraper.scrape(args.keywords)

    else:
        # å¦‚æœæ²¡ä¼ å‚æ•°ï¼Œæ‰“å°å¸®åŠ©ä¿¡æ¯
        print("âŒ æœªæŒ‡å®šæ“ä½œæ¨¡å¼ã€‚")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  1. ç™»å½•: python scraper.py --login")
        print("  2. çˆ¬å–: python scraper.py --keywords å…³é”®è¯1 å…³é”®è¯2")
        print("\nç¤ºä¾‹:")
        print("  python scraper.py --login")
        print("  python scraper.py --keywords ç¾è‚¡ è‹±ä¼Ÿè¾¾ ç‰¹æ–¯æ‹‰")
        print("\næˆ–ä½¿ç”¨æ¨¡å—æ–¹å¼è¿è¡Œ:")
        print("  python -m app.services.xiaohongshu --login")
        print("  python -m app.services.xiaohongshu ç¾è‚¡ è‹±ä¼Ÿè¾¾")
