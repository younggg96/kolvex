"""
ç¾è‚¡ KOL æ‰¹é‡çˆ¬è™«ç³»ç»Ÿ
ä½¿ç”¨ Playwright (sync_api) æŠ“å– X.com ä¸Šçš„ç¾è‚¡ KOL æ¨æ–‡
å¹¶ä¿å­˜åˆ° Supabase æ•°æ®åº“
"""

import time
import random
import hashlib
import json
import os
from typing import List, Dict, Optional, Set, Tuple
from datetime import datetime, timedelta, timezone
from pathlib import Path

# Playwright ç›¸å…³å¯¼å…¥
try:
    from playwright.sync_api import sync_playwright, Page, Browser, BrowserContext

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    # å®šä¹‰å ä½ç±»å‹ï¼Œé¿å…ç±»å‹æ³¨è§£é”™è¯¯
    Page = None
    Browser = None
    BrowserContext = None

# Supabase ç›¸å…³å¯¼å…¥
try:
    from supabase import create_client, Client

    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False

# ============================================================
# é…ç½®å¸¸é‡
# ============================================================

# Cookies æ–‡ä»¶è·¯å¾„
COOKIES_FILE = Path(__file__).parent.parent.parent / "x_cookies.json"

# çœŸå®çš„ User-Agent åˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
]

# ============================================================
# ğŸ¯ ç¾è‚¡ KOL åˆ—è¡¨ (æŒ‰ç±»åˆ«ç»„ç»‡)
# ============================================================

KOL_LIST = {
    # --- ğŸš¨ Must-Have News & Flow (é€Ÿåº¦æœ€å¿«çš„æ•°æ®æº) ---
    "news_flow": [
        ("unusual_whales", "Options flow & dark pool data"),
        ("DeItaone", "Walter Bloomberg - The fastest breaking news terminal"),
        ("FinancialJuice", "Real-time audio/text news"),
        ("zerohedge", "Macro/Geopolitics/Contrarian views"),
        ("FirstSquawk", "Real-time trading audio news squawk"),
    ],
    # --- ğŸ“‰ Famous Big Short & Macro (å®è§‚ä¸ç©ºå¤´) ---
    "short_macro": [
        ("BurryTracker", "Tracking Michael Burry's portfolio/deleted tweets"),
        ("HindenburgRes", "Most influential short seller - Market mover"),
        ("MuddyWatersRE", "Carson Block's short selling firm"),
        ("CitronResearch", "Andrew Left - Short seller"),
        ("MacroAlf", "Alfonso Peccatiello - Ex-PIMCO Macro Economist"),
        ("elerianm", "Mohamed A. El-Erian - Allianz Chief Economic Advisor"),
    ],
    # --- ğŸ“Š Charts & Data (ç¡¬æ ¸æ•°æ®æ´¾) ---
    "charts_data": [
        ("charliebilello", "Top tier market charts & stats"),
        ("SJosephBurns", "Technical analysis education"),
        ("TrendSpider", "Automated technical analysis"),
        ("SpotGamma", "Options gamma exposure & volatility"),
        ("KobeissiLetter", "Global capital markets commentary"),
    ],
    # --- ğŸ‚ Institutional & Mainstream (ä¸»æµå£°éŸ³) ---
    "institutional": [
        ("JimCramer", "CNBC Host - often used as inverse indicator"),
        ("WSJmarkets", "Wall Street Journal Markets"),
        ("BloombergTV", "Global Finance News"),
        ("BillAckman", "Pershing Square - Activist Investor"),
    ],
    # --- ğŸ¦ Retail Sentiment & Meme (æ•£æˆ·é£å‘æ ‡) ---
    "retail_meme": [
        ("StockTwits", "Community sentiment aggregation"),
        ("TheRoaringKitty", "Keith Gill - GME/Meme stock leader"),
        ("wallstreetbets", "Reddit WSB official handle"),
        ("QuiverQuant", "Tracking congressional trading & alternative data"),
    ],
}


def get_all_kols() -> List[Tuple[str, str]]:
    """è·å–æ‰€æœ‰ KOL çš„ (username, description) åˆ—è¡¨"""
    all_kols = []
    for category in KOL_LIST.values():
        all_kols.extend(category)
    return all_kols


# ============================================================
# Supabase æ•°æ®åº“æ“ä½œ
# ============================================================


def get_supabase_client() -> Optional[Client]:
    """
    è·å– Supabase å®¢æˆ·ç«¯

    Returns:
        Optional[Client]: Supabase å®¢æˆ·ç«¯ï¼Œå¦‚æœæœªé…ç½®è¿”å› None
    """
    if not SUPABASE_AVAILABLE:
        print("âš ï¸ Supabase æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install supabase")
        return None

    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    from dotenv import load_dotenv

    load_dotenv()

    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_SERVICE_KEY") or os.getenv("SUPABASE_KEY")

    if not supabase_url or not supabase_key:
        print(
            "âš ï¸ Supabase é…ç½®æœªæ‰¾åˆ°ï¼Œè¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_SERVICE_KEY ç¯å¢ƒå˜é‡"
        )
        return None

    return create_client(supabase_url, supabase_key)


def compute_tweet_hash(text: str, username: str) -> str:
    """
    è®¡ç®—æ¨æ–‡çš„å”¯ä¸€å“ˆå¸Œå€¼

    Args:
        text: æ¨æ–‡æ–‡æœ¬
        username: ç”¨æˆ·å

    Returns:
        str: SHA256 å“ˆå¸Œå€¼çš„å‰ 16 ä½
    """
    content = f"{username}:{text}"
    return hashlib.sha256(content.encode()).hexdigest()[:16]


def tweet_exists(client: Client, tweet_hash: str) -> bool:
    """
    æ£€æŸ¥æ¨æ–‡æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­

    Args:
        client: Supabase å®¢æˆ·ç«¯
        tweet_hash: æ¨æ–‡å“ˆå¸Œå€¼

    Returns:
        bool: å¦‚æœå­˜åœ¨è¿”å› True
    """
    try:
        result = (
            client.table("kol_tweets")
            .select("id")
            .eq("tweet_hash", tweet_hash)
            .limit(1)
            .execute()
        )
        return len(result.data) > 0
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æ¨æ–‡æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
        return False


def insert_tweet(
    client: Client, tweet_data: Dict, category: str = None, max_age_days: int = 7
) -> bool:
    """
    æ’å…¥æ¨æ–‡åˆ° Supabase æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ä¸”ä¸å¤ªæ—§ï¼‰

    Args:
        client: Supabase å®¢æˆ·ç«¯
        tweet_data: æ¨æ–‡æ•°æ®å­—å…¸ï¼ŒåŒ…å«:
            - username: ç”¨æˆ·å
            - text: æ¨æ–‡æ–‡æœ¬
            - created_at: åˆ›å»ºæ—¶é—´
            - permalink: æ¨æ–‡é“¾æ¥
            - avatar_url: KOL å¤´åƒ URL
            - media_urls: åª’ä½“ URL åˆ—è¡¨
            - is_repost: æ˜¯å¦æ˜¯è½¬å‘
            - original_author: åŸä½œè€…
            - reply_count, repost_count, like_count, bookmark_count, views_count
        category: KOL ç±»åˆ«
        max_age_days: æœ€å¤§æ¨æ–‡å¹´é¾„ï¼ˆå¤©ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ¨æ–‡ä¸ä¼šè¢«æ’å…¥

    Returns:
        bool: æ’å…¥æˆåŠŸè¿”å› Trueï¼Œå·²å­˜åœ¨/å¤ªæ—§/å¤±è´¥è¿”å› False
    """

    # æ£€æŸ¥æ¨æ–‡æ—¶é—´ï¼Œå¦‚æœå¤ªæ—§å°±è·³è¿‡
    created_at_str = tweet_data.get("created_at")
    if created_at_str:
        try:
            # è§£æ ISO æ ¼å¼æ—¶é—´
            if created_at_str.endswith("Z"):
                created_at_str = created_at_str[:-1] + "+00:00"
            tweet_time = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))

            # å¦‚æœæ˜¯ naive datetimeï¼Œå‡è®¾ä¸º UTC
            if tweet_time.tzinfo is None:
                tweet_time = tweet_time.replace(tzinfo=timezone.utc)

            cutoff_time = datetime.now(timezone.utc) - timedelta(days=max_age_days)

            if tweet_time < cutoff_time:
                print(
                    f"   â­ï¸ è·³è¿‡æ—§æ¨æ–‡ ({created_at_str[:10]}): {tweet_data['text'][:30]}..."
                )
                return False
        except Exception as e:
            # è§£æå¤±è´¥å°±ç»§ç»­æ’å…¥
            pass

    tweet_hash = compute_tweet_hash(tweet_data["text"], tweet_data["username"])

    if tweet_exists(client, tweet_hash):
        return False

    try:
        # å¤„ç† media_urls - è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²å­˜å‚¨
        media_urls = tweet_data.get("media_urls", [])
        media_urls_json = json.dumps(media_urls) if media_urls else None

        data = {
            "username": tweet_data["username"],
            "tweet_text": tweet_data["text"],
            "tweet_hash": tweet_hash,
            "created_at": tweet_data.get("created_at"),
            "permalink": tweet_data.get("permalink"),
            # æ–°å¢å­—æ®µ
            "avatar_url": tweet_data.get("avatar_url"),
            "media_urls": media_urls_json,
            "is_repost": tweet_data.get("is_repost", False),
            "original_author": tweet_data.get("original_author"),
            # äº’åŠ¨æ•°æ®
            "like_count": tweet_data.get("like_count", 0),
            "retweet_count": tweet_data.get("repost_count", 0),  # å…¼å®¹æ—§å­—æ®µå
            "reply_count": tweet_data.get("reply_count", 0),
            "bookmark_count": tweet_data.get("bookmark_count", 0),
            "views_count": tweet_data.get("views_count", 0),
            # å…ƒæ•°æ®
            "scraped_at": datetime.now(timezone.utc).isoformat(),
            "category": category,
        }
        client.table("kol_tweets").insert(data).execute()
        return True
    except Exception as e:
        # å¯èƒ½æ˜¯å”¯ä¸€çº¦æŸå†²çªï¼ˆå¹¶å‘æƒ…å†µï¼‰
        if "duplicate" in str(e).lower() or "unique" in str(e).lower():
            return False
        print(f"âš ï¸ æ’å…¥æ¨æ–‡å¤±è´¥: {e}")
        return False


def upsert_kol_profile(
    client: Client, profile_data: Dict, category: str = None, description: str = None
) -> bool:
    """
    æ’å…¥æˆ–æ›´æ–° KOL profile åˆ° Supabase çš„ kol_profiles è¡¨

    Args:
        client: Supabase å®¢æˆ·ç«¯
        profile_data: å®Œæ•´çš„ profile æ•°æ®å­—å…¸
        category: KOL ç±»åˆ«
        description: KOL æè¿°ï¼ˆæ¥è‡ªé¢„å®šä¹‰åˆ—è¡¨ï¼‰

    Returns:
        bool: æ“ä½œæˆåŠŸè¿”å› True
    """
    try:
        data = {
            # æ ¸å¿ƒèº«ä»½ä¿¡æ¯
            "username": profile_data["username"],
            "rest_id": profile_data.get("rest_id"),
            "display_name": profile_data.get("display_name"),
            # è®¤è¯çŠ¶æ€
            "is_verified": profile_data.get("is_verified", False),
            "verification_type": profile_data.get("verification_type", "None"),
            # å½±å“åŠ›æŒ‡æ ‡
            "followers_count": profile_data.get("followers_count", 0),
            "following_count": profile_data.get("following_count", 0),
            "posts_count": profile_data.get("posts_count", 0),
            # æ—¶é—´ä¿¡æ¯
            "join_date": profile_data.get("join_date"),
            # å¤–éƒ¨é“¾æ¥ä¸ä½ç½®
            "location": profile_data.get("location"),
            "website": profile_data.get("website"),
            "bio": profile_data.get("bio"),
            # è§†è§‰ç´ æ
            "avatar_url": profile_data.get("avatar_url"),
            "banner_url": profile_data.get("banner_url"),
            # KOL åˆ†ç±»
            "category": category,
            "description": description,
            # å…ƒæ•°æ®
            "is_active": True,
            "updated_at": datetime.utcnow().isoformat(),
        }
        # ä½¿ç”¨ upsertï¼Œå¦‚æœ username å·²å­˜åœ¨åˆ™æ›´æ–°
        client.table("kol_profiles").upsert(data, on_conflict="username").execute()
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ KOL profile å¤±è´¥: {e}")
        return False


# ä¿ç•™æ—§å‡½æ•°åä½œä¸ºåˆ«åï¼Œä¿æŒå…¼å®¹æ€§
upsert_user_profile = upsert_kol_profile


def get_stats(client: Client) -> Dict:
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    Returns:
        Dict: åŒ…å«æ€»æ•°ã€å„ç”¨æˆ·æ•°é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # æ€»æ¨æ–‡æ•°
        total_result = client.table("kol_tweets").select("id", count="exact").execute()
        total = total_result.count or 0

        # å„ç”¨æˆ·æ¨æ–‡æ•°
        user_result = (
            client.rpc("get_kol_tweet_counts_by_user", {}).execute() if False else None
        )  # RPC å¯èƒ½ä¸å­˜åœ¨ï¼Œfallback

        # ç®€å•æŸ¥è¯¢å„ç”¨æˆ·æ¨æ–‡æ•°
        by_user = {}
        try:
            users_result = client.table("kol_tweets").select("username").execute()
            for row in users_result.data:
                username = row["username"]
                by_user[username] = by_user.get(username, 0) + 1
        except:
            pass

        # å„ç±»åˆ«æ¨æ–‡æ•°
        by_category = {}
        try:
            cat_result = (
                client.table("kol_tweets")
                .select("category")
                .not_.is_("category", "null")
                .execute()
            )
            for row in cat_result.data:
                cat = row["category"]
                by_category[cat] = by_category.get(cat, 0) + 1
        except:
            pass

        return {
            "total": total,
            "by_user": dict(sorted(by_user.items(), key=lambda x: x[1], reverse=True)),
            "by_category": dict(
                sorted(by_category.items(), key=lambda x: x[1], reverse=True)
            ),
        }
    except Exception as e:
        print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {"total": 0, "by_user": {}, "by_category": {}}


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================


def random_sleep(min_sec: float, max_sec: float, message: str = None) -> None:
    """
    éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º

    Args:
        min_sec: æœ€å°å»¶è¿Ÿç§’æ•°
        max_sec: æœ€å¤§å»¶è¿Ÿç§’æ•°
        message: å¯é€‰çš„æç¤ºä¿¡æ¯
    """
    delay = random.uniform(min_sec, max_sec)
    if message:
        print(f"â³ {message} (ç­‰å¾… {delay:.1f}s)")
    time.sleep(delay)


def load_cookies(cookies_file: str = None) -> Optional[List[Dict]]:
    """
    åŠ è½½ä¿å­˜çš„ cookies

    Args:
        cookies_file: cookies æ–‡ä»¶è·¯å¾„

    Returns:
        Optional[List[Dict]]: cookies åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å› None
    """
    if cookies_file is None:
        cookies_file = str(COOKIES_FILE)

    if os.path.exists(cookies_file):
        try:
            with open(cookies_file, "r") as f:
                cookies = json.load(f)
                print(f"ğŸª å·²åŠ è½½ cookies: {cookies_file}")
                return cookies
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ cookies å¤±è´¥: {e}")
    return None


def save_cookies(cookies: List[Dict], cookies_file: str = None) -> bool:
    """
    ä¿å­˜ cookies åˆ°æ–‡ä»¶

    Args:
        cookies: cookies åˆ—è¡¨
        cookies_file: ä¿å­˜è·¯å¾„

    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å› True
    """
    if cookies_file is None:
        cookies_file = str(COOKIES_FILE)

    try:
        with open(cookies_file, "w") as f:
            json.dump(cookies, f, indent=2)
        print(f"ğŸª Cookies å·²ä¿å­˜åˆ°: {cookies_file}")
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ cookies å¤±è´¥: {e}")
        return False


# ============================================================
# æ¨æ–‡æå–å‡½æ•°
# ============================================================


def parse_metric(text: str) -> int:
    """
    è§£ææ•°é‡æ–‡æœ¬ï¼Œå°† "1.5M", "10K", "5,302" è½¬æ¢ä¸ºçº¯æ•´æ•°

    Args:
        text: åŒ…å«æ•°é‡çš„æ–‡æœ¬ï¼Œå¦‚ "1.2M", "12.5K", "5,302"

    Returns:
        int: è§£æå‡ºçš„æ•°é‡
    """
    if not text:
        return 0
    try:
        import re

        # æ¸…ç†æ–‡æœ¬
        text = text.strip().replace(",", "")

        # åŒ¹é…æ•°å­—å’Œåç¼€
        match = re.search(r"([\d.]+)\s*([KMB])?", text, re.IGNORECASE)
        if match:
            num_str = match.group(1)
            num = float(num_str)
            suffix = match.group(2)

            if suffix:
                suffix = suffix.upper()
                multipliers = {"K": 1_000, "M": 1_000_000, "B": 1_000_000_000}
                num *= multipliers.get(suffix, 1)

            return int(num)
    except Exception:
        pass
    return 0


# ä¿ç•™æ—§å‡½æ•°åä½œä¸ºåˆ«å
_parse_count_text = parse_metric


def extract_user_profile(page) -> Dict:
    """
    ä»ç”¨æˆ·ä¸»é¡µæå–å®Œæ•´çš„ profile ä¿¡æ¯

    æå–å­—æ®µï¼š
    - æ ¸å¿ƒèº«ä»½: username, rest_id, display_name
    - è®¤è¯çŠ¶æ€: is_verified, verification_type (Blue/Gold/Grey/None)
    - å½±å“åŠ›æŒ‡æ ‡: followers_count, following_count, posts_count
    - æ—¶é—´ä¿¡æ¯: join_date
    - å¤–éƒ¨é“¾æ¥: location, website, bio
    - è§†è§‰ç´ æ: avatar_url, banner_url

    Args:
        page: Playwright é¡µé¢å¯¹è±¡

    Returns:
        Dict: åŒ…å«å®Œæ•´ç”¨æˆ· profile ä¿¡æ¯çš„å­—å…¸
    """
    profile = {
        # æ ¸å¿ƒèº«ä»½ä¿¡æ¯
        "username": None,
        "rest_id": None,
        "display_name": None,
        # è®¤è¯çŠ¶æ€
        "is_verified": False,
        "verification_type": "None",  # 'Blue', 'Gold', 'Grey', 'None'
        # å½±å“åŠ›æŒ‡æ ‡
        "followers_count": 0,
        "following_count": 0,
        "posts_count": 0,
        # æ—¶é—´ä¿¡æ¯
        "join_date": None,
        # å¤–éƒ¨é“¾æ¥ä¸ä½ç½®
        "location": None,
        "website": None,
        "bio": None,
        # è§†è§‰ç´ æ
        "avatar_url": None,
        "banner_url": None,
    }

    try:
        # ========== 1. æå–ç”¨æˆ·å (ä» URL) ==========
        url = page.url
        if "//" in url:
            parts = url.split("/")
            for part in parts:
                if part and part not in [
                    "https:",
                    "http:",
                    "",
                    "x.com",
                    "twitter.com",
                ]:
                    profile["username"] = part.split("?")[0]
                    break

        # ========== 2. æå– Rest ID (ä» HTML) ==========
        try:
            # Rest ID é€šå¸¸åœ¨æŸäº›å…ƒç´ çš„å±æ€§ä¸­
            # å°è¯•ä»é¡µé¢å†…å®¹ä¸­æå–
            page_content = page.content()
            import re

            rest_id_match = re.search(r'"rest_id":"(\d+)"', page_content)
            if rest_id_match:
                profile["rest_id"] = rest_id_match.group(1)
        except Exception:
            pass

        # ========== 3. æå–å¤´åƒ URL ==========
        try:
            avatar_selectors = [
                'img[src*="profile_images"]',
                '[data-testid="UserAvatar-Container-unknown"] img',
                'a[href*="photo"] img',
            ]
            for selector in avatar_selectors:
                avatar = page.query_selector(selector)
                if avatar:
                    src = avatar.get_attribute("src")
                    if src and "profile_images" in src:
                        # è·å–æ›´é«˜åˆ†è¾¨ç‡çš„å¤´åƒ (400x400)
                        profile["avatar_url"] = src.replace(
                            "_normal", "_400x400"
                        ).replace("_bigger", "_400x400")
                        break
        except Exception:
            pass

        # ========== 4. æå–èƒŒæ™¯å›¾ URL ==========
        try:
            banner_selectors = [
                'img[src*="profile_banners"]',
                '[data-testid="UserProfileHeader_Items"] img[src*="banner"]',
                'a[href*="header_photo"] img',
            ]
            for selector in banner_selectors:
                banner = page.query_selector(selector)
                if banner:
                    src = banner.get_attribute("src")
                    if src and "profile_banners" in src:
                        profile["banner_url"] = src
                        break
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä» CSS èƒŒæ™¯å›¾æå–
            if not profile["banner_url"]:
                header = page.query_selector('[data-testid="UserProfileHeader_Items"]')
                if header:
                    style = header.evaluate(
                        "el => getComputedStyle(el).backgroundImage"
                    )
                    if style and "url(" in style:
                        import re

                        match = re.search(r'url\(["\']?(.*?)["\']?\)', style)
                        if match:
                            profile["banner_url"] = match.group(1)
        except Exception:
            pass

        # ========== 5. æå–æ˜¾ç¤ºåç§° ==========
        try:
            name_selectors = [
                '[data-testid="UserName"] span span',
                '[data-testid="UserName"] > div > div > span',
                'h2[role="heading"] span',
            ]
            for selector in name_selectors:
                name_element = page.query_selector(selector)
                if name_element:
                    text = name_element.inner_text().strip()
                    if text and not text.startswith("@"):
                        profile["display_name"] = text
                        break
        except Exception:
            pass

        # ========== 6. æå–è®¤è¯çŠ¶æ€ ==========
        try:
            # æŸ¥æ‰¾è®¤è¯å›¾æ ‡
            verified_selectors = [
                'svg[data-testid="icon-verified"]',
                '[data-testid="UserName"] svg[aria-label*="Verified"]',
                '[data-testid="UserName"] svg[aria-label*="verified"]',
            ]
            for selector in verified_selectors:
                verified_icon = page.query_selector(selector)
                if verified_icon:
                    profile["is_verified"] = True
                    # è·å–è®¤è¯ç±»å‹ï¼ˆé€šè¿‡é¢œè‰²åˆ¤æ–­ï¼‰
                    try:
                        # è·å– SVG çš„é¢œè‰²
                        color = verified_icon.evaluate(
                            "el => getComputedStyle(el).color"
                        )
                        aria_label = verified_icon.get_attribute("aria-label") or ""

                        # åˆ¤æ–­è®¤è¯ç±»å‹
                        if (
                            "gold" in color.lower()
                            or "rgb(255, 212, 0)" in color
                            or "affiliates" in aria_label.lower()
                        ):
                            profile["verification_type"] = "Gold"  # ä¼ä¸š/æœºæ„
                        elif (
                            "grey" in color.lower()
                            or "gray" in color.lower()
                            or "government" in aria_label.lower()
                        ):
                            profile["verification_type"] = "Grey"  # æ”¿åºœ
                        else:
                            profile["verification_type"] = "Blue"  # ä¸ªäºº/ä»˜è´¹
                    except Exception:
                        profile["verification_type"] = "Blue"
                    break
        except Exception:
            pass

        # ========== 7. æå– Bio ==========
        try:
            bio_element = page.query_selector('[data-testid="UserDescription"]')
            if bio_element:
                bio_text = bio_element.inner_text().strip()
                if bio_text:
                    profile["bio"] = bio_text[:1000]  # é™åˆ¶é•¿åº¦
        except Exception:
            pass

        # ========== 8. æå–ç²‰ä¸æ•° ==========
        try:
            followers_link = page.query_selector('a[href*="/verified_followers"]')
            if not followers_link:
                followers_link = page.query_selector('a[href*="/followers"]')
            if followers_link:
                text = followers_link.inner_text()
                profile["followers_count"] = parse_metric(text)
        except Exception:
            pass

        # ========== 9. æå–å…³æ³¨æ•° ==========
        try:
            following_link = page.query_selector('a[href*="/following"]')
            if following_link:
                text = following_link.inner_text()
                profile["following_count"] = parse_metric(text)
        except Exception:
            pass

        # ========== 10. æå–æ¨æ–‡æ•° ==========
        try:
            # æ¨æ–‡æ•°é€šå¸¸åœ¨ header ä¸­æ˜¾ç¤ºï¼Œå¦‚ "156.9K posts"
            header_items = page.query_selector('[data-testid="UserName"]')
            if header_items:
                parent = header_items.evaluate(
                    "el => el.parentElement?.parentElement?.textContent"
                )
                if parent:
                    import re

                    posts_match = re.search(
                        r"([\d,.]+[KMB]?)\s*(?:posts?|tweets?)", parent, re.IGNORECASE
                    )
                    if posts_match:
                        profile["posts_count"] = parse_metric(posts_match.group(1))
            # å¤‡ç”¨æ–¹æ¡ˆ
            if profile["posts_count"] == 0:
                nav_items = page.query_selector_all("nav a span")
                for item in nav_items:
                    text = item.inner_text()
                    if "post" in text.lower() or "tweet" in text.lower():
                        profile["posts_count"] = parse_metric(text)
                        break
        except Exception:
            pass

        # ========== 11. æå–åŠ å…¥æ—¥æœŸ ==========
        try:
            join_selectors = [
                '[data-testid="UserJoinDate"]',
                'span[data-testid="UserJoinDate"]',
            ]
            for selector in join_selectors:
                join_element = page.query_selector(selector)
                if join_element:
                    text = join_element.inner_text().strip()
                    # æå– "Joined June 2014" ä¸­çš„æ—¥æœŸéƒ¨åˆ†
                    if "Joined" in text:
                        profile["join_date"] = text.replace("Joined", "").strip()
                    else:
                        profile["join_date"] = text
                    break
        except Exception:
            pass

        # ========== 12. æå–ä½ç½® ==========
        try:
            location_selectors = [
                '[data-testid="UserLocation"]',
                '[data-testid="UserProfileHeader_Items"] span[data-testid="UserLocation"]',
            ]
            for selector in location_selectors:
                location_element = page.query_selector(selector)
                if location_element:
                    text = location_element.inner_text().strip()
                    if text:
                        profile["location"] = text
                        break
        except Exception:
            pass

        # ========== 13. æå–ç½‘ç«™é“¾æ¥ ==========
        try:
            url_selectors = [
                '[data-testid="UserUrl"] a',
                '[data-testid="UserProfileHeader_Items"] a[href*="t.co"]',
                'a[data-testid="UserUrl"]',
            ]
            for selector in url_selectors:
                url_element = page.query_selector(selector)
                if url_element:
                    href = url_element.get_attribute("href")
                    text = url_element.inner_text().strip()
                    # ä¼˜å…ˆä½¿ç”¨æ˜¾ç¤ºæ–‡æœ¬ï¼Œå› ä¸º href é€šå¸¸æ˜¯ t.co çŸ­é“¾æ¥
                    profile["website"] = text if text else href
                    break
        except Exception:
            pass

    except Exception as e:
        print(f"   âš ï¸ æå– profile ä¿¡æ¯æ—¶å‡ºé”™: {e}")

    return profile


def extract_tweet_text(article) -> Optional[str]:
    """ä» article å…ƒç´ ä¸­æå–æ¨æ–‡æ–‡æœ¬ï¼ˆæ›´å®½å®¹çš„ç‰ˆæœ¬ï¼‰"""
    try:
        # æ–¹æ³•1: ä½¿ç”¨ data-testid="tweetText"
        tweet_text_element = article.query_selector('[data-testid="tweetText"]')
        if tweet_text_element:
            text = tweet_text_element.inner_text().strip()
            if text:
                return text

        # æ–¹æ³•2: æŸ¥æ‰¾å¸¦ lang å±æ€§çš„ div
        lang_div = article.query_selector("div[lang]")
        if lang_div:
            text = lang_div.inner_text().strip()
            if text:
                return text

        # æ–¹æ³•3: å¦‚æœæ²¡æœ‰æ­£æ–‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åª’ä½“å†…å®¹ (å›¾ç‰‡/è§†é¢‘)
        media_photo = article.query_selector('[data-testid="tweetPhoto"]')
        media_video = article.query_selector('[data-testid="videoPlayer"]')
        media_card = article.query_selector('[data-testid="card.wrapper"]')

        if media_photo or media_video or media_card:
            # å°è¯•è·å– alt æ–‡æœ¬æˆ–æè¿°
            img = article.query_selector("img[alt]")
            if img:
                alt = img.get_attribute("alt")
                if alt and alt != "Image":
                    return f"[åª’ä½“] {alt}"
            return "[åª’ä½“æ¨æ–‡]"

        return None
    except Exception:
        return None


def extract_tweet_metadata(article) -> Dict:
    """
    ä» article å…ƒç´ ä¸­æå–æ¨æ–‡å…ƒæ•°æ®

    æå–å­—æ®µï¼š
    - created_at: æ¨æ–‡åˆ›å»ºæ—¶é—´
    - permalink: æ¨æ–‡é“¾æ¥
    - avatar_url: KOL å¤´åƒ URL
    - media_urls: å›¾ç‰‡/è§†é¢‘ URL åˆ—è¡¨
    - is_repost: æ˜¯å¦æ˜¯è½¬å‘
    - original_author: åŸä½œè€…ï¼ˆå¦‚æœæ˜¯è½¬å‘ï¼‰
    - reply_count, repost_count, like_count, bookmark_count, views_count
    """
    metadata = {
        "created_at": None,
        "permalink": None,
        "avatar_url": None,
        "media_urls": [],
        "is_repost": False,
        "original_author": None,
        "reply_count": 0,
        "repost_count": 0,
        "like_count": 0,
        "bookmark_count": 0,
        "views_count": 0,
    }

    try:
        # ========== 1. æå–æ—¶é—´ ==========
        time_element = article.query_selector("time")
        if time_element:
            metadata["created_at"] = time_element.get_attribute("datetime")

        # ========== 2. æå–é“¾æ¥ ==========
        link = article.query_selector('a[href*="/status/"]')
        if link:
            href = link.get_attribute("href")
            if href:
                metadata["permalink"] = (
                    f"https://x.com{href}" if href.startswith("/") else href
                )

        # ========== 3. æå– KOL å¤´åƒ URL ==========
        # æ¨æ–‡å†…çš„å¤´åƒåœ¨ article å†…éƒ¨çš„ img å…ƒç´ ï¼Œå¸¦æœ‰ profile_images
        try:
            # æ–¹æ³•1: ä»æ¨æ–‡å¤´éƒ¨çš„ç”¨æˆ·ä¿¡æ¯åŒºåŸŸæå–å¤´åƒ
            avatar_selectors = [
                '[data-testid="Tweet-User-Avatar"] img[src*="profile_images"]',
                'div[data-testid="Tweet-User-Avatar"] img',
                'a[role="link"] img[src*="profile_images"]',
            ]
            for selector in avatar_selectors:
                avatar_img = article.query_selector(selector)
                if avatar_img:
                    src = avatar_img.get_attribute("src")
                    if src and "profile_images" in src:
                        # è·å–æ›´é«˜åˆ†è¾¨ç‡çš„å¤´åƒ
                        metadata["avatar_url"] = (
                            src.replace("_normal", "_400x400")
                            .replace("_bigger", "_400x400")
                            .replace("_mini", "_400x400")
                        )
                        break
        except Exception:
            pass

        # ========== 4. æ£€æµ‹æ˜¯å¦æ˜¯è½¬å‘ (Repost) ==========
        try:
            # è½¬å‘ä¼šæœ‰ "reposted" æ–‡å­—æˆ–ç‰¹å®šçš„æ ‡è¯†
            repost_indicators = [
                'span[data-testid="socialContext"]',  # åŒ…å« "XXX reposted" çš„åŒºåŸŸ
                'div[data-testid="socialContext"]',
            ]
            for selector in repost_indicators:
                social_context = article.query_selector(selector)
                if social_context:
                    text = social_context.inner_text().lower()
                    if "repost" in text or "retweeted" in text:
                        metadata["is_repost"] = True
                        # å°è¯•æå–åŸä½œè€…
                        # è½¬å‘æ—¶ï¼Œæ¨æ–‡ä½œè€…é“¾æ¥ä¼šæŒ‡å‘åŸä½œè€…
                        author_link = article.query_selector(
                            'div[data-testid="User-Name"] a[href^="/"]'
                        )
                        if author_link:
                            href = author_link.get_attribute("href")
                            if href:
                                metadata["original_author"] = (
                                    href.lstrip("/").split("/")[0].split("?")[0]
                                )
                        break
        except Exception:
            pass

        # ========== 5. æå–åª’ä½“ URLs (å›¾ç‰‡å’Œè§†é¢‘) ==========
        try:
            media_urls = []

            # 5a. æå–å›¾ç‰‡ URLs
            photo_elements = article.query_selector_all(
                '[data-testid="tweetPhoto"] img'
            )
            for photo in photo_elements:
                src = photo.get_attribute("src")
                if src and "profile_images" not in src and "emoji" not in src:
                    # è·å–åŸå›¾å°ºå¯¸
                    # Twitter å›¾ç‰‡ URL æ ¼å¼: https://pbs.twimg.com/media/xxx?format=jpg&name=small
                    # æ”¹ä¸º name=large æˆ– name=orig è·å–é«˜æ¸…å›¾
                    if "twimg.com/media" in src:
                        if "name=" in src:
                            src = src.split("name=")[0] + "name=large"
                        elif "?" not in src:
                            src = src + "?name=large"
                    media_urls.append({"type": "photo", "url": src})

            # 5b. æå–è§†é¢‘ URLs
            video_elements = article.query_selector_all(
                '[data-testid="videoPlayer"] video'
            )
            for video in video_elements:
                src = video.get_attribute("src")
                poster = video.get_attribute("poster")
                if src:
                    media_urls.append({"type": "video", "url": src, "poster": poster})
                elif poster:
                    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ video srcï¼Œè‡³å°‘ä¿å­˜å°é¢å›¾
                    media_urls.append({"type": "video", "url": None, "poster": poster})

            # 5c. å¦‚æœæ²¡æ‰¾åˆ°ç›´æ¥çš„è§†é¢‘æºï¼Œå°è¯•æ‰¾è§†é¢‘å°é¢
            if not any(m["type"] == "video" for m in media_urls):
                video_container = article.query_selector('[data-testid="videoPlayer"]')
                if video_container:
                    # è§†é¢‘å°é¢å›¾
                    poster_img = video_container.query_selector(
                        'img[src*="ext_tw_video"]'
                    )
                    if poster_img:
                        poster_src = poster_img.get_attribute("src")
                        if poster_src:
                            media_urls.append(
                                {"type": "video", "url": None, "poster": poster_src}
                            )

            # 5d. æå– GIF
            gif_elements = article.query_selector_all(
                '[data-testid="tweetPhoto"] video[poster*="tweet_video_thumb"]'
            )
            for gif in gif_elements:
                src = gif.get_attribute("src")
                poster = gif.get_attribute("poster")
                if src or poster:
                    media_urls.append({"type": "gif", "url": src, "poster": poster})

            # 5e. æå–å¡ç‰‡ä¸­çš„å›¾ç‰‡ (é“¾æ¥é¢„è§ˆç­‰)
            card_img = article.query_selector(
                '[data-testid="card.wrapper"] img[src*="twimg.com"]'
            )
            if card_img:
                src = card_img.get_attribute("src")
                if src and "profile_images" not in src:
                    media_urls.append({"type": "card", "url": src})

            metadata["media_urls"] = media_urls

        except Exception:
            pass

        # ========== 6. æå–äº’åŠ¨æ•°æ® ==========
        def parse_aria_count(element) -> int:
            """ä»å…ƒç´ çš„ aria-label è§£ææ•°é‡"""
            try:
                if element:
                    aria_label = element.get_attribute("aria-label")
                    if aria_label:
                        import re

                        # åŒ¹é…å„ç§æ ¼å¼: "123 replies", "1,234 Likes", "12.5K views"
                        match = re.search(r"([\d,.]+[KMB]?)", aria_label)
                        if match:
                            return parse_metric(match.group(1))
            except:
                pass
            return 0

        # 6a. Reply count
        reply_btn = article.query_selector('[data-testid="reply"]')
        metadata["reply_count"] = parse_aria_count(reply_btn)

        # 6b. Repost/Retweet count
        retweet_btn = article.query_selector('[data-testid="retweet"]')
        metadata["repost_count"] = parse_aria_count(retweet_btn)

        # 6c. Like count
        like_btn = article.query_selector('[data-testid="like"]')
        metadata["like_count"] = parse_aria_count(like_btn)

        # 6d. Bookmark count (å¯èƒ½æ²¡æœ‰æ˜¾ç¤º)
        bookmark_btn = article.query_selector('[data-testid="bookmark"]')
        metadata["bookmark_count"] = parse_aria_count(bookmark_btn)

        # 6e. Views count - é€šå¸¸åœ¨ analytics é“¾æ¥æˆ–å•ç‹¬çš„åŒºåŸŸ
        try:
            # æ–¹æ³•1: ä» analytics åŒºåŸŸè·å–
            views_element = article.query_selector('a[href*="/analytics"] span')
            if views_element:
                views_text = views_element.inner_text()
                metadata["views_count"] = parse_metric(views_text)

            # æ–¹æ³•2: ä» aria-label åŒ…å« "views" çš„å…ƒç´ è·å–
            if metadata["views_count"] == 0:
                analytics_link = article.query_selector('a[href*="/analytics"]')
                if analytics_link:
                    aria = analytics_link.get_attribute("aria-label")
                    if aria and "view" in aria.lower():
                        import re

                        match = re.search(
                            r"([\d,.]+[KMB]?)\s*view", aria, re.IGNORECASE
                        )
                        if match:
                            metadata["views_count"] = parse_metric(match.group(1))
        except Exception:
            pass

    except Exception:
        pass

    return metadata


# ============================================================
# æ ¸å¿ƒçˆ¬è™«ç±»
# ============================================================


class BatchKOLScraper:
    """
    æ‰¹é‡ KOL çˆ¬è™«ç±»

    æ”¯æŒä¸¤ç§æ¨¡å¼:
    1. Setup Mode: headless=Falseï¼Œç”¨äºæ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ cookies
    2. Batch Mode: åˆ©ç”¨å·²ä¿å­˜çš„ cookies è¿›è¡Œæ‰¹é‡çˆ¬å–
    """

    def __init__(
        self,
        cookies_file: str = None,
        headless: bool = False,
        max_posts_per_user: int = 10,
        delay_between_users: Tuple[float, float] = (5.0, 15.0),
        delay_during_scroll: Tuple[float, float] = (1.0, 3.0),
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            cookies_file: cookies æ–‡ä»¶è·¯å¾„
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            max_posts_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•°é‡
            delay_between_users: ç”¨æˆ·é—´å»¶è¿ŸèŒƒå›´ (min, max) ç§’
            delay_during_scroll: æ»šåŠ¨æ—¶å»¶è¿ŸèŒƒå›´ (min, max) ç§’
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise RuntimeError(
                "âŒ Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ:\n"
                "   pip install playwright\n"
                "   playwright install chromium"
            )

        self.cookies_file = str(cookies_file or COOKIES_FILE)
        self.headless = headless
        self.max_posts_per_user = max_posts_per_user
        self.delay_between_users = delay_between_users
        self.delay_during_scroll = delay_during_scroll

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "users_processed": 0,
            "users_failed": 0,
            "tweets_scraped": 0,
            "tweets_new": 0,
            "tweets_duplicate": 0,
        }

        # åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
        self.supabase = get_supabase_client()
        if self.supabase:
            print("âœ… Supabase è¿æ¥æˆåŠŸ")
        else:
            print("âš ï¸ Supabase æœªè¿æ¥ï¼Œå°†åªæ‰“å°æ¨æ–‡è€Œä¸ä¿å­˜")

    def setup_mode(self, timeout: int = 300) -> bool:
        """
        Setup æ¨¡å¼: æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•

        Args:
            timeout: ç­‰å¾…ç™»å½•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: ç™»å½•æˆåŠŸè¿”å› True
        """
        print("\n" + "=" * 60)
        print("ğŸ”§ SETUP MODE - è¯·æ‰‹åŠ¨ç™»å½• X.com")
        print("=" * 60)

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=False,  # Setup æ¨¡å¼å¿…é¡»æœ‰å¤´
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--disable-dev-shm-usage",
                    "--no-sandbox",
                ],
            )

            context = browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                viewport={"width": 1280, "height": 900},
                locale="en-US",
                timezone_id="America/New_York",
            )

            page = context.new_page()
            self._add_stealth_scripts(page)

            try:
                print("ğŸ“± æ­£åœ¨æ‰“å¼€ X.com...")
                page.goto(
                    "https://x.com/login", wait_until="domcontentloaded", timeout=60000
                )

                print("\n" + "âš ï¸ " * 20)
                print("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•ï¼")
                print(f"è¶…æ—¶æ—¶é—´: {timeout} ç§’")
                print("âš ï¸ " * 20 + "\n")

                # ç­‰å¾…ç”¨æˆ·å®Œæˆç™»å½•ï¼ˆæ£€æµ‹ä¸»é¡µå…ƒç´ ï¼‰
                try:
                    page.wait_for_selector(
                        '[data-testid="primaryColumn"]',
                        timeout=timeout * 1000,
                        state="visible",
                    )
                    print("âœ… æ£€æµ‹åˆ°å·²ç™»å½•ï¼")

                    # ä¿å­˜ cookies
                    cookies = context.cookies()
                    if save_cookies(cookies, self.cookies_file):
                        print("âœ… Setup å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œ Batch Mode äº†ã€‚")
                        return True

                except Exception as e:
                    print(f"âŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥: {e}")
                    return False

            finally:
                print("\næµè§ˆå™¨å°†åœ¨ 3 ç§’åå…³é—­...")
                time.sleep(3)
                browser.close()

        return False

    def _add_stealth_scripts(self, page: "Page") -> None:
        """æ·»åŠ åæ£€æµ‹è„šæœ¬"""
        page.add_init_script(
            """
            // éšè— webdriver å±æ€§
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            // æ¨¡æ‹ŸçœŸå®çš„ plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            // æ¨¡æ‹ŸçœŸå®çš„ languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en']
            });
            
            // éšè—è‡ªåŠ¨åŒ–ç—•è¿¹
            window.chrome = { runtime: {} };
        """
        )

    def _scrape_single_user(
        self,
        page: "Page",
        username: str,
        category: str = None,
        description: str = None,
    ) -> List[Dict]:
        """
        çˆ¬å–å•ä¸ªç”¨æˆ·çš„æ¨æ–‡å’Œ profile ä¿¡æ¯

        Args:
            page: Playwright é¡µé¢å¯¹è±¡
            username: ç”¨æˆ·å
            category: KOL ç±»åˆ«
            description: KOL æè¿°ï¼ˆæ¥è‡ªé¢„å®šä¹‰åˆ—è¡¨ï¼‰

        Returns:
            List[Dict]: çˆ¬å–åˆ°çš„æ¨æ–‡åˆ—è¡¨
        """
        clean_username = username.lstrip("@").strip()
        profile_url = f"https://x.com/{clean_username}"

        # ä½¿ç”¨æœç´¢ URL å¹¶æŒ‰æ—¶é—´æ’åºï¼ˆf=live è¡¨ç¤ºæœ€æ–°ï¼‰
        # ä¸åœ¨æœç´¢ä¸­é™åˆ¶æ—¶é—´ï¼Œè€Œæ˜¯åœ¨ insert_tweet æ—¶è¿‡æ»¤æ—§æ¨æ–‡
        # è¿™æ ·å¯ä»¥ç¡®ä¿æœç´¢æœ‰ç»“æœï¼ŒåŒæ—¶åªä¿å­˜æœ€æ–°çš„æ¨æ–‡
        search_url = (
            f"https://x.com/search?q=from%3A{clean_username}&src=typed_query&f=live"
        )

        collected_tweets = []
        seen_texts: Set[str] = set()

        print(f"\nğŸ“ æ­£åœ¨è®¿é—® @{clean_username}...")

        try:
            # ========== ç¬¬ä¸€æ­¥ï¼šè®¿é—®ç”¨æˆ·ä¸»é¡µè·å– Profile ä¿¡æ¯ ==========
            page.goto(profile_url, wait_until="domcontentloaded", timeout=30000)
            random_sleep(2, 4)

            # æ£€æµ‹æ˜¯å¦æˆåŠŸåŠ è½½ç”¨æˆ·é¡µé¢
            try:
                page.wait_for_selector("article", timeout=15000, state="visible")
            except Exception:
                print(f"   âš ï¸ æ— æ³•åŠ è½½ @{clean_username} çš„é¡µé¢ï¼ˆå¯èƒ½ä¸å­˜åœ¨æˆ–è¢«å°ç¦ï¼‰")
                return []

            # ========== æå–å¹¶ä¿å­˜ Profile ä¿¡æ¯ ==========
            profile_data = extract_user_profile(page)
            profile_data["username"] = clean_username  # ç¡®ä¿ç”¨æˆ·åæ­£ç¡®

            if self.supabase:
                if upsert_user_profile(
                    self.supabase, profile_data, category, description
                ):
                    self.stats["profiles_updated"] = (
                        self.stats.get("profiles_updated", 0) + 1
                    )
                    # æ‰“å° profile ä¿¡æ¯
                    display_name = profile_data.get("display_name", clean_username)
                    followers = profile_data.get("followers_count", 0)
                    following = profile_data.get("following_count", 0)
                    posts = profile_data.get("posts_count", 0)
                    verified = profile_data.get("verification_type", "None")

                    # è®¤è¯å¾½ç« 
                    badge = ""
                    if verified == "Gold":
                        badge = "ğŸ¢"
                    elif verified == "Blue":
                        badge = "âœ“"
                    elif verified == "Grey":
                        badge = "ğŸ›ï¸"

                    print(f"   ğŸ‘¤ {display_name} {badge}")
                    print(
                        f"      ğŸ“Š ç²‰ä¸: {followers:,} | å…³æ³¨: {following:,} | æ¨æ–‡: {posts:,}"
                    )

                    # é¢å¤–ä¿¡æ¯
                    extras = []
                    if profile_data.get("avatar_url"):
                        extras.append("å¤´åƒâœ“")
                    if profile_data.get("banner_url"):
                        extras.append("èƒŒæ™¯âœ“")
                    if profile_data.get("location"):
                        extras.append(f"ğŸ“{profile_data['location']}")
                    if profile_data.get("join_date"):
                        extras.append(f"ğŸ“…{profile_data['join_date']}")
                    if profile_data.get("website"):
                        extras.append(f"ğŸ”—")

                    if extras:
                        print(f"      {' | '.join(extras)}")

            # ========== ç¬¬äºŒæ­¥ï¼šè·³è½¬åˆ°æœç´¢é¡µé¢è·å–æœ€æ–°æ¨æ–‡ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰==========
            print(f"   ğŸ” åˆ‡æ¢åˆ°æœ€æ–°æ¨æ–‡è§†å›¾ (æœç´¢: from:{clean_username})...")
            page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
            random_sleep(2, 4)

            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            try:
                page.wait_for_selector("article", timeout=15000, state="visible")
            except Exception:
                # æˆªå›¾ä¿å­˜ï¼Œæ–¹ä¾¿è°ƒè¯•
                debug_path = f"debug_{clean_username}.png"
                try:
                    page.screenshot(path=debug_path)
                    print(f"   âš ï¸ æœç´¢ç»“æœä¸ºç©ºæˆ–åŠ è½½å¤±è´¥ï¼Œæˆªå›¾å·²ä¿å­˜: {debug_path}")
                except:
                    print(f"   âš ï¸ æœç´¢ç»“æœä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")

                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æç¤ºæˆ–éœ€è¦éªŒè¯
                page_content = page.content().lower()
                if "verify" in page_content or "captcha" in page_content:
                    print(f"   ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç /äººæœºéªŒè¯ï¼Œè¯·é‡æ–°è¿è¡Œ --setup ç™»å½•")
                elif "something went wrong" in page_content:
                    print(f"   âŒ é¡µé¢æ˜¾ç¤º 'Something went wrong'ï¼Œå¯èƒ½æ˜¯è´¦å·é—®é¢˜")
                elif "log in" in page_content or "sign in" in page_content:
                    print(f"   ğŸ”‘ éœ€è¦ç™»å½•ï¼Œè¯·åˆ é™¤ cookies æ–‡ä»¶å¹¶é‡æ–°è¿è¡Œ --setup")

                return []

            # æ»šåŠ¨å’Œçˆ¬å–æœ€æ–°æ¨æ–‡
            scroll_count = 0
            max_scrolls = 10
            no_new_count = 0

            while (
                len(collected_tweets) < self.max_posts_per_user
                and scroll_count < max_scrolls
            ):
                scroll_count += 1

                articles = page.query_selector_all("article")
                new_in_batch = 0

                for article in articles:
                    if len(collected_tweets) >= self.max_posts_per_user:
                        break

                    text = extract_tweet_text(article)

                    if text and text not in seen_texts:
                        seen_texts.add(text)
                        metadata = extract_tweet_metadata(article)

                        tweet_data = {
                            "username": clean_username,
                            "text": text,
                            **metadata,
                        }

                        collected_tweets.append(tweet_data)
                        new_in_batch += 1

                        # ä¿å­˜åˆ° Supabase
                        if self.supabase:
                            if insert_tweet(self.supabase, tweet_data, category):
                                self.stats["tweets_new"] += 1
                                # æ˜¾ç¤ºæ¨æ–‡æ—¶é—´ï¼Œæ–¹ä¾¿ç¡®è®¤æ˜¯å¦æ˜¯æœ€æ–°æ¨æ–‡
                                created_at = metadata.get("created_at", "")
                                time_str = created_at[:16] if created_at else "æœªçŸ¥æ—¶é—´"
                                print(
                                    f"   âœ… [{len(collected_tweets)}/{self.max_posts_per_user}] ğŸ•{time_str} | {text[:40]}..."
                                )
                            else:
                                self.stats["tweets_duplicate"] += 1
                                print(
                                    f"   ğŸ“‹ [{len(collected_tweets)}/{self.max_posts_per_user}] å·²å­˜åœ¨: {text[:40]}..."
                                )
                        else:
                            created_at = metadata.get("created_at", "")
                            time_str = created_at[:16] if created_at else "æœªçŸ¥æ—¶é—´"
                            print(
                                f"   ğŸ“ [{len(collected_tweets)}/{self.max_posts_per_user}] ğŸ•{time_str} | {text[:40]}..."
                            )

                if new_in_batch == 0:
                    no_new_count += 1
                    if no_new_count >= 2:
                        break
                else:
                    no_new_count = 0

                if len(collected_tweets) >= self.max_posts_per_user:
                    break

                # æ»šåŠ¨é¡µé¢
                page.evaluate(
                    """
                    window.scrollBy({
                        top: window.innerHeight * 0.8,
                        behavior: 'smooth'
                    });
                """
                )

                random_sleep(*self.delay_during_scroll)

                try:
                    page.wait_for_load_state("networkidle", timeout=5000)
                except:
                    pass

            self.stats["tweets_scraped"] += len(collected_tweets)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨æ–‡ï¼Œæˆªå›¾è°ƒè¯•
            if len(collected_tweets) == 0:
                debug_path = f"debug_empty_{clean_username}.png"
                try:
                    page.screenshot(path=debug_path)
                    print(f"   âš ï¸ æœªæ‰¾åˆ°æ¨æ–‡ï¼Œæˆªå›¾å·²ä¿å­˜: {debug_path}")
                except:
                    pass
            else:
                print(
                    f"   ğŸ“Š @{clean_username}: çˆ¬å– {len(collected_tweets)} æ¡æœ€æ–°æ¨æ–‡"
                )

        except Exception as e:
            print(f"   âŒ çˆ¬å– @{clean_username} å¤±è´¥: {e}")
            # æˆªå›¾ä¿å­˜é”™è¯¯ç°åœº
            try:
                page.screenshot(path=f"error_{clean_username}.png")
                print(f"   ğŸ“¸ é”™è¯¯æˆªå›¾å·²ä¿å­˜: error_{clean_username}.png")
            except:
                pass
            self.stats["users_failed"] += 1

        return collected_tweets

    def batch_scrape(
        self,
        kol_list: List[Tuple[str, str]] = None,
        categories: List[str] = None,
    ) -> Dict:
        """
        æ‰¹é‡çˆ¬å– KOL æ¨æ–‡

        Args:
            kol_list: è‡ªå®šä¹‰ KOL åˆ—è¡¨ [(username, description), ...]
                      å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤çš„ KOL_LIST
            categories: è¦çˆ¬å–çš„ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ ["news_flow", "short_macro"]
                        å¦‚æœä¸º Noneï¼Œçˆ¬å–æ‰€æœ‰ç±»åˆ«

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        # å‡†å¤‡ KOL åˆ—è¡¨
        if kol_list is not None:
            targets = [(username, None, desc) for username, desc in kol_list]
        else:
            targets = []
            for cat_name, cat_kols in KOL_LIST.items():
                if categories is None or cat_name in categories:
                    for username, desc in cat_kols:
                        targets.append((username, cat_name, desc))

        if not targets:
            print("âŒ æ²¡æœ‰è¦çˆ¬å–çš„ KOL")
            return self.stats

        # æ£€æŸ¥ cookies
        cookies = load_cookies(self.cookies_file)
        if cookies is None:
            print("\nâŒ æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼")
            print("è¯·å…ˆè¿è¡Œ Setup Mode è¿›è¡Œç™»å½•:")
            print("   python -m app.services.batch_kol_scraper --setup")
            return self.stats

        print("\n" + "=" * 60)
        print(f"ğŸš€ BATCH MODE - å¼€å§‹æ‰¹é‡çˆ¬å–")
        print(f"ğŸ“‹ ç›®æ ‡: {len(targets)} ä¸ª KOL")
        print(f"ğŸ“ æ¯ç”¨æˆ·æœ€å¤š: {self.max_posts_per_user} æ¡æ¨æ–‡")
        print(f"ğŸ’¾ å­˜å‚¨: {'Supabase' if self.supabase else 'ä»…æ‰“å°'}")
        print("=" * 60)

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=self.headless,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--disable-dev-shm-usage",
                    "--no-sandbox",
                ],
            )

            context = browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                viewport={"width": 1280, "height": 900},
                locale="en-US",
                timezone_id="America/New_York",
            )

            # åŠ è½½ cookies
            context.add_cookies(cookies)

            page = context.new_page()
            self._add_stealth_scripts(page)

            try:
                for i, (username, category, description) in enumerate(targets, 1):
                    print(f"\n[{i}/{len(targets)}] ğŸ¯ @{username}")
                    if description:
                        print(f"   ğŸ“ {description}")

                    self._scrape_single_user(page, username, category, description)
                    self.stats["users_processed"] += 1

                    # ç”¨æˆ·é—´å»¶è¿Ÿï¼ˆæœ€åä¸€ä¸ªç”¨æˆ·ä¸éœ€è¦ï¼‰
                    if i < len(targets):
                        random_sleep(
                            *self.delay_between_users, f"åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç”¨æˆ·å‰ç­‰å¾…"
                        )

            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")

            except Exception as e:
                print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")

            finally:
                # æ›´æ–° cookiesï¼ˆå¯èƒ½å·²åˆ·æ–°ï¼‰
                try:
                    new_cookies = context.cookies()
                    save_cookies(new_cookies, self.cookies_file)
                except:
                    pass

                browser.close()

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()

        return self.stats

    def _print_final_stats(self) -> None:
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 60)
        print(f"  âœ… å¤„ç†ç”¨æˆ·: {self.stats['users_processed']}")
        print(f"  âŒ å¤±è´¥ç”¨æˆ·: {self.stats['users_failed']}")
        print(f"  ğŸ‘¤ æ›´æ–° Profile: {self.stats.get('profiles_updated', 0)}")
        print(f"  ğŸ“ çˆ¬å–æ¨æ–‡: {self.stats['tweets_scraped']}")
        print(f"  ğŸ†• æ–°å¢æ¨æ–‡: {self.stats['tweets_new']}")
        print(f"  ğŸ“‹ é‡å¤æ¨æ–‡: {self.stats['tweets_duplicate']}")
        print("=" * 60)

        # æ•°æ®åº“ç»Ÿè®¡
        if self.supabase:
            db_stats = get_stats(self.supabase)
            print(f"\nğŸ“¦ Supabase æ•°æ®åº“æ€»è®¡: {db_stats['total']} æ¡æ¨æ–‡")

            # ç»Ÿè®¡ kol_profiles è¡¨æ•°é‡
            try:
                profiles_result = (
                    self.supabase.table("kol_profiles")
                    .select("username, verification_type", count="exact")
                    .execute()
                )
                total_profiles = profiles_result.count or 0
                print(f"ğŸ‘¤ KOL Profiles: {total_profiles} ä¸ª")

                # æŒ‰è®¤è¯ç±»å‹ç»Ÿè®¡
                if profiles_result.data:
                    verified_counts = {}
                    for profile in profiles_result.data:
                        v_type = profile.get("verification_type") or "None"
                        verified_counts[v_type] = verified_counts.get(v_type, 0) + 1
                    if verified_counts and any(k != "None" for k in verified_counts):
                        badges = {"Gold": "ğŸ¢", "Blue": "âœ“", "Grey": "ğŸ›ï¸", "None": "â—‹"}
                        parts = [
                            f"{badges.get(k, '')} {k}: {v}"
                            for k, v in verified_counts.items()
                        ]
                        print(f"   è®¤è¯: {' | '.join(parts)}")
            except Exception:
                pass

            if db_stats["by_category"]:
                print("\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
                for cat, count in db_stats["by_category"].items():
                    print(f"  - {cat}: {count} æ¡")

    def close(self) -> None:
        """å…³é—­èµ„æºï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰"""
        pass


# ============================================================
# æ•°æ®è¿ç§»å·¥å…·
# ============================================================


def migrate_sqlite_to_supabase(sqlite_path: str) -> int:
    """
    å°† SQLite æ•°æ®è¿ç§»åˆ° Supabase

    Args:
        sqlite_path: SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„

    Returns:
        int: è¿ç§»çš„è®°å½•æ•°
    """
    import sqlite3

    supabase = get_supabase_client()
    if not supabase:
        print("âŒ æ— æ³•è¿æ¥ Supabase")
        return 0

    if not os.path.exists(sqlite_path):
        print(f"âŒ SQLite æ–‡ä»¶ä¸å­˜åœ¨: {sqlite_path}")
        return 0

    conn = sqlite3.connect(sqlite_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    cursor.execute("SELECT * FROM tweets")
    rows = cursor.fetchall()

    migrated = 0
    for row in rows:
        try:
            data = {
                "username": row["username"],
                "tweet_text": row["tweet_text"],
                "tweet_hash": row["tweet_hash"],
                "created_at": row["created_at"],
                "permalink": row["permalink"],
                "like_count": row["like_count"] or 0,
                "retweet_count": row["retweet_count"] or 0,
                "reply_count": row["reply_count"] or 0,
                "scraped_at": row["scraped_at"],
                "category": row["category"],
            }
            supabase.table("kol_tweets").upsert(
                data, on_conflict="tweet_hash"
            ).execute()
            migrated += 1
            print(f"  âœ… è¿ç§»: @{row['username']}: {row['tweet_text'][:30]}...")
        except Exception as e:
            print(f"  âš ï¸ è·³è¿‡: {e}")

    conn.close()
    print(f"\nâœ… è¿ç§»å®Œæˆ: {migrated}/{len(rows)} æ¡è®°å½•")
    return migrated


# ============================================================
# CLI å…¥å£ç‚¹
# ============================================================


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ç¾è‚¡ KOL æ‰¹é‡çˆ¬è™« (Supabase ç‰ˆ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # Setup æ¨¡å¼ - é¦–æ¬¡è¿è¡Œï¼Œæ‰‹åŠ¨ç™»å½•ä¿å­˜ cookies
  python -m app.services.batch_kol_scraper --setup
  
  # Batch æ¨¡å¼ - æ‰¹é‡çˆ¬å–æ‰€æœ‰ KOL
  python -m app.services.batch_kol_scraper
  
  # åªçˆ¬å–ç‰¹å®šç±»åˆ«
  python -m app.services.batch_kol_scraper --categories news_flow short_macro
  
  # ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆå¯è§æµè§ˆå™¨ï¼‰
  python -m app.services.batch_kol_scraper --no-headless
  
  # æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
  python -m app.services.batch_kol_scraper --stats
  
  # ä» SQLite è¿ç§»æ•°æ®åˆ° Supabase
  python -m app.services.batch_kol_scraper --migrate kol_tweets.db
        """,
    )

    parser.add_argument(
        "--setup",
        action="store_true",
        help="Setup æ¨¡å¼: æ‰“å¼€æµè§ˆå™¨æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ cookies",
    )

    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯")

    parser.add_argument(
        "--migrate",
        type=str,
        metavar="SQLITE_FILE",
        help="ä» SQLite æ–‡ä»¶è¿ç§»æ•°æ®åˆ° Supabase",
    )

    parser.add_argument(
        "--categories",
        nargs="+",
        choices=list(KOL_LIST.keys()),
        help="æŒ‡å®šè¦çˆ¬å–çš„ KOL ç±»åˆ«",
    )

    parser.add_argument(
        "--max-posts",
        type=int,
        default=10,
        help="æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•°é‡ (é»˜è®¤: 10)",
    )

    parser.add_argument(
        "--no-headless", action="store_true", help="ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰"
    )

    parser.add_argument("--cookies", type=str, default=None, help="Cookies æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # è¿ç§»æ•°æ®
    if args.migrate:
        print(f"\nğŸ“¦ å¼€å§‹è¿ç§» SQLite æ•°æ®åˆ° Supabase...")
        migrate_sqlite_to_supabase(args.migrate)
        return

    # æ˜¾ç¤ºç»Ÿè®¡
    if args.stats:
        supabase = get_supabase_client()
        if not supabase:
            print("âŒ æ— æ³•è¿æ¥ Supabase")
            return

        stats = get_stats(supabase)
        print("\nğŸ“Š Supabase æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ€»æ¨æ–‡æ•°: {stats['total']}")
        print("\nğŸ“‹ æŒ‰ç”¨æˆ·ç»Ÿè®¡:")
        for user, count in list(stats["by_user"].items())[:20]:
            print(f"  @{user}: {count}")
        if stats["by_category"]:
            print("\nğŸ“ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for cat, count in stats["by_category"].items():
                print(f"  {cat}: {count}")
        return

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = BatchKOLScraper(
        cookies_file=args.cookies,
        headless=not args.no_headless,
        max_posts_per_user=args.max_posts,
    )

    try:
        if args.setup:
            # Setup æ¨¡å¼
            scraper.setup_mode()
        else:
            # Batch æ¨¡å¼
            scraper.batch_scrape(categories=args.categories)

    finally:
        scraper.close()


if __name__ == "__main__":
    main()
