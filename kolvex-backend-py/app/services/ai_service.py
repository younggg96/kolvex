"""
AI æœåŠ¡æ¨¡å—
ä½¿ç”¨ Ollama API è¿›è¡Œæ¨æ–‡åˆ†æã€æƒ…æ„Ÿåˆ†æç­‰

API æ–‡æ¡£: https://github.com/ollama/ollama/blob/main/docs/api.md
"""

import os
import re
import json
import httpx
from typing import Optional, Dict, List, Any
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# é…ç½®
# ============================================================

# Ollama API Base URL (å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–)
OLLAMA_BASE_URL = os.getenv(
    "OLLAMA_BASE_URL", "https://zaksw0tikh2qca-11434.proxy.runpod.net"
)

# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = os.getenv("OLLAMA_MODEL", "deepseek-r1:70b")

# è¯·æ±‚è¶…æ—¶ (ç§’) - DeepSeek R1 éœ€è¦æ›´é•¿çš„æ€è€ƒæ—¶é—´
REQUEST_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "300.0"))


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================


def strip_think_tags(text: str) -> str:
    """
    ç§»é™¤ DeepSeek R1 æ¨¡å‹è¾“å‡ºçš„ <think>...</think> æ€è€ƒè¿‡ç¨‹æ ‡ç­¾

    Args:
        text: æ¨¡å‹åŸå§‹è¾“å‡º

    Returns:
        str: æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text:
        return text
    # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
    cleaned = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    return cleaned.strip()


def extract_json_object(text: str) -> Optional[Dict]:
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ JSON å¯¹è±¡

    Args:
        text: æ¸…ç†åçš„æ–‡æœ¬

    Returns:
        Dict æˆ– None
    """
    if not text:
        return None

    # å…ˆæ¸…ç† think æ ‡ç­¾
    cleaned = strip_think_tags(text)

    # æŸ¥æ‰¾ JSON å¯¹è±¡
    json_start = cleaned.find("{")
    if json_start < 0:
        return None

    # ä»æ‰¾åˆ°çš„ä½ç½®å¼€å§‹ï¼Œå°è¯•æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·
    brace_count = 0
    json_end = -1
    for i, char in enumerate(cleaned[json_start:], start=json_start):
        if char == "{":
            brace_count += 1
        elif char == "}":
            brace_count -= 1
            if brace_count == 0:
                json_end = i + 1
                break

    if json_end > json_start:
        try:
            json_str = cleaned[json_start:json_end]
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass

    return None


def extract_json_array(text: str) -> Optional[List]:
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ JSON æ•°ç»„

    Args:
        text: æ¸…ç†åçš„æ–‡æœ¬

    Returns:
        List æˆ– None
    """
    if not text:
        return None

    # å…ˆæ¸…ç† think æ ‡ç­¾
    cleaned = strip_think_tags(text)

    # æŸ¥æ‰¾ JSON æ•°ç»„
    json_start = cleaned.find("[")
    if json_start < 0:
        return None

    # ä»æ‰¾åˆ°çš„ä½ç½®å¼€å§‹ï¼Œå°è¯•æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·
    bracket_count = 0
    json_end = -1
    for i, char in enumerate(cleaned[json_start:], start=json_start):
        if char == "[":
            bracket_count += 1
        elif char == "]":
            bracket_count -= 1
            if bracket_count == 0:
                json_end = i + 1
                break

    if json_end > json_start:
        try:
            json_str = cleaned[json_start:json_end]
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass

    return None


# ============================================================
# Ollama API å®¢æˆ·ç«¯
# ============================================================


class OllamaClient:
    """Ollama API å®¢æˆ·ç«¯"""

    def __init__(
        self, base_url: str = None, model: str = None, timeout: float = REQUEST_TIMEOUT
    ):
        self.base_url = (base_url or OLLAMA_BASE_URL).rstrip("/")
        self.model = model or DEFAULT_MODEL
        self.timeout = timeout
        self._client = httpx.AsyncClient(timeout=timeout)

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self._client.aclose()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    # ========== åŸºç¡€ API ==========

    async def generate(
        self,
        prompt: str,
        model: str = None,
        system: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        stream: bool = False,
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: ç”¨æˆ·æç¤º
            model: æ¨¡å‹åç§° (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å®ä¾‹é…ç½®)
            system: ç³»ç»Ÿæç¤º (å¯é€‰)
            temperature: æ¸©åº¦ (0-1)
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        url = f"{self.base_url}/api/generate"

        payload = {
            "model": model or self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        if system:
            payload["system"] = system

        response = await self._client.post(url, json=payload)
        response.raise_for_status()

        data = response.json()
        return data.get("response", "")

    async def chat(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        stream: bool = False,
    ) -> str:
        """
        èŠå¤©è¡¥å…¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant/system", "content": "..."}]
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼

        Returns:
            str: AI å›å¤å†…å®¹
        """
        url = f"{self.base_url}/api/chat"

        payload = {
            "model": model or self.model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        response = await self._client.post(url, json=payload)
        response.raise_for_status()

        data = response.json()
        return data.get("message", {}).get("content", "")

    async def embeddings(self, text: str, model: str = None) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬
            model: æ¨¡å‹åç§°

        Returns:
            List[float]: åµŒå…¥å‘é‡
        """
        url = f"{self.base_url}/api/embeddings"

        payload = {"model": model or self.model, "prompt": text}

        response = await self._client.post(url, json=payload)
        response.raise_for_status()

        data = response.json()
        return data.get("embedding", [])

    async def list_models(self) -> List[Dict]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        url = f"{self.base_url}/api/tags"
        response = await self._client.get(url)
        response.raise_for_status()
        data = response.json()
        return data.get("models", [])

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            url = f"{self.base_url}/api/tags"
            response = await self._client.get(url, timeout=10.0)
            return response.status_code == 200
        except Exception:
            return False


# ============================================================
# æ¨æ–‡åˆ†ææœåŠ¡
# ============================================================


class TweetAnalyzer:
    """æ¨æ–‡åˆ†æå™¨ - ä½¿ç”¨ AI åˆ†ææ¨æ–‡å†…å®¹"""

    def __init__(self, client: OllamaClient = None):
        self.client = client or OllamaClient()

    async def analyze_sentiment(self, tweet_text: str) -> Dict[str, Any]:
        """
        åˆ†ææ¨æ–‡æƒ…æ„Ÿ

        Returns:
            {
                "sentiment": "bullish" | "bearish" | "neutral",
                "confidence": 0.0-1.0,
                "reasoning": "..."
            }
        """
        system_prompt = """You are a financial sentiment analyzer. Analyze the given tweet and determine:
1. Sentiment: bullish (positive about markets/stocks), bearish (negative), or neutral
2. Confidence: 0.0 to 1.0
3. Brief reasoning (1-2 sentences)

Respond in JSON format only:
{"sentiment": "bullish|bearish|neutral", "confidence": 0.85, "reasoning": "..."}"""

        try:
            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Analyze this tweet:\n\n{tweet_text}"},
                ],
                temperature=0.3,
                max_tokens=2500,
            )

            # ä½¿ç”¨æ–°çš„ JSON æå–å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç† <think> æ ‡ç­¾ï¼‰
            result = extract_json_object(response)
            if result:
                return result

            return {
                "sentiment": "neutral",
                "confidence": 0.5,
                "reasoning": "Unable to parse response",
            }

        except Exception as e:
            return {
                "sentiment": "neutral",
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
            }

    async def extract_tickers(self, tweet_text: str) -> List[str]:
        """
        ä»æ¨æ–‡ä¸­æå–è‚¡ç¥¨ä»£ç 

        Returns:
            ["AAPL", "TSLA", ...]
        """
        system_prompt = """Extract stock tickers (symbols) mentioned in the tweet.
Look for:
- $SYMBOL format
- Company names that map to tickers
- Common stock abbreviations

Respond with JSON array only: ["AAPL", "TSLA"]
If no tickers found, respond: []"""

        try:
            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": f"Extract tickers from:\n\n{tweet_text}",
                    },
                ],
                temperature=0.1,
                max_tokens=2500,
            )

            # ä½¿ç”¨æ–°çš„ JSON æå–å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç† <think> æ ‡ç­¾ï¼‰
            result = extract_json_array(response)
            if result is not None:
                return result

            return []

        except Exception:
            return []

    async def summarize(self, tweet_text: str, max_length: int = 100) -> str:
        """
        ç”Ÿæˆæ¨æ–‡æ‘˜è¦

        Args:
            tweet_text: æ¨æ–‡åŸæ–‡
            max_length: æ‘˜è¦æœ€å¤§é•¿åº¦

        Returns:
            str: æ‘˜è¦
        """
        system_prompt = f"""Summarize the following tweet in {max_length} characters or less.
Focus on the key financial/market information.
Respond with the summary text only, no quotes or formatting."""

        try:
            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": tweet_text},
                ],
                temperature=0.3,
                max_tokens=2500,
            )
            # æ¸…ç† <think> æ ‡ç­¾åå†æå–æ‘˜è¦
            cleaned = strip_think_tags(response)
            return cleaned.strip()[:max_length]
        except Exception:
            return tweet_text[:max_length]

    async def generate_tags(self, tweet_text: str, max_tags: int = 5) -> List[str]:
        """
        ä¸ºæ¨æ–‡ç”Ÿæˆæ ‡ç­¾

        Returns:
            ["earnings", "tech", "bullish", ...]
        """
        system_prompt = f"""Generate up to {max_tags} relevant tags for this financial tweet.
Tags should be lowercase, single words or short phrases.
Categories: sector (tech, finance, energy), sentiment (bullish, bearish), 
event type (earnings, merger, ipo), asset class (stocks, crypto, bonds).

Respond with JSON array only: ["tag1", "tag2"]"""

        try:
            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Generate tags for:\n\n{tweet_text}"},
                ],
                temperature=0.3,
                max_tokens=2500,
            )

            # ä½¿ç”¨æ–°çš„ JSON æå–å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç† <think> æ ‡ç­¾ï¼‰
            result = extract_json_array(response)
            if result is not None:
                return result[:max_tags]

            return []

        except Exception:
            return []

    async def analyze_trading_signal(
        self, tweet_text: str, tickers: List[str] = None
    ) -> Dict[str, Any]:
        """
        åˆ†ææŠ•èµ„ä¿¡å·

        Returns:
            {
                "action": "buy" | "sell" | "hold" | null,
                "tickers": ["AAPL"],
                "confidence": 0.7
            }
        """
        system_prompt = """Analyze this financial tweet for trading signals.
Determine if there's an actionable trading recommendation.

Consider:
- Is the author suggesting to buy, sell, or hold?
- Which tickers are being recommended?
- How confident is the signal? (0.0-1.0)

If no clear trading signal, return null for action.

Respond in JSON format only:
{"action": "buy"|"sell"|"hold"|null, "tickers": ["AAPL"], "confidence": 0.7}"""

        try:
            context = tweet_text
            if tickers:
                context += f"\n\nDetected tickers: {', '.join(tickers)}"

            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": f"Analyze trading signal:\n\n{context}",
                    },
                ],
                temperature=0.3,
                max_tokens=2500,
            )

            # ä½¿ç”¨æ–°çš„ JSON æå–å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç† <think> æ ‡ç­¾ï¼‰
            result = extract_json_object(response)
            if result:
                return {
                    "action": result.get("action"),
                    "tickers": result.get("tickers", tickers or []),
                    "confidence": result.get("confidence"),
                }

            return {"action": None, "tickers": tickers or [], "confidence": None}

        except Exception as e:
            return {"action": None, "tickers": tickers or [], "confidence": None}

    async def generate_summary_cn(self, tweet_text: str, max_length: int = 100) -> str:
        """
        ç”Ÿæˆä¸­æ–‡æ‘˜è¦
        """
        system_prompt = f"""å°†ä»¥ä¸‹é‡‘èæ¨æ–‡ç¿»è¯‘å¹¶æ€»ç»“ä¸ºä¸­æ–‡ï¼Œä¸è¶…è¿‡{max_length}ä¸ªå­—ç¬¦ã€‚
åªè¿”å›æ‘˜è¦æ–‡æœ¬ï¼Œä¸è¦å¼•å·æˆ–æ ¼å¼ã€‚"""

        try:
            response = await self.client.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": tweet_text},
                ],
                temperature=0.3,
                max_tokens=2000,
            )
            # æ¸…ç† <think> æ ‡ç­¾åå†æå–æ‘˜è¦
            cleaned = strip_think_tags(response)
            return cleaned.strip()[:max_length]
        except Exception:
            return ""

    async def full_analysis(self, tweet_text: str) -> Dict[str, Any]:
        """
        å®Œæ•´åˆ†ææ¨æ–‡

        Returns:
            {
                "sentiment": {"value": "bullish", "confidence": 0.85, "reasoning": "..."},
                "tickers": ["AAPL", "NVDA"],
                "tags": ["tech", "earnings"],
                "trading_signal": {"action": "buy", "tickers": ["AAPL"], "confidence": 0.7},
                "summary": "ä¸­æ–‡æ‘˜è¦",
                "summary_en": "English summary",
                "analyzed_at": "2024-01-15T10:30:00Z",
                "model": "llama3.2"
            }
        """
        import asyncio

        # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡ŒåŸºç¡€åˆ†æ
        sentiment_task = self.analyze_sentiment(tweet_text)
        tickers_task = self.extract_tickers(tweet_text)
        summary_en_task = self.summarize(tweet_text)
        summary_cn_task = self.generate_summary_cn(tweet_text)
        tags_task = self.generate_tags(tweet_text)

        sentiment, tickers, summary_en, summary_cn, tags = await asyncio.gather(
            sentiment_task, tickers_task, summary_en_task, summary_cn_task, tags_task
        )

        # ç¬¬äºŒé˜¶æ®µï¼šåŸºäºæå–çš„ tickers åˆ†æäº¤æ˜“ä¿¡å·
        trading_signal = await self.analyze_trading_signal(tweet_text, tickers)

        return {
            "sentiment": sentiment,
            "tickers": tickers,
            "tags": tags,
            "trading_signal": trading_signal,
            "summary": summary_cn,
            "summary_en": summary_en,
            "analyzed_at": datetime.utcnow().isoformat(),
            "model": self.client.model,
        }


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================


async def get_ai_client() -> OllamaClient:
    """è·å– AI å®¢æˆ·ç«¯å®ä¾‹"""
    return OllamaClient()


async def analyze_tweet(tweet_text: str) -> Dict[str, Any]:
    """
    åˆ†æå•æ¡æ¨æ–‡ (ä¾¿æ·å‡½æ•°)

    Usage:
        result = await analyze_tweet("$AAPL looking strong after earnings!")
    """
    async with OllamaClient() as client:
        analyzer = TweetAnalyzer(client)
        return await analyzer.full_analysis(tweet_text)


async def quick_sentiment(tweet_text: str) -> str:
    """
    å¿«é€Ÿè·å–æƒ…æ„Ÿ (ä¾¿æ·å‡½æ•°)

    Returns:
        "bullish" | "bearish" | "neutral"
    """
    async with OllamaClient() as client:
        analyzer = TweetAnalyzer(client)
        result = await analyzer.analyze_sentiment(tweet_text)
        return result.get("sentiment", "neutral")


# ============================================================
# æ•°æ®åº“æ“ä½œ - ä¿å­˜ AI åˆ†æç»“æœ
# ============================================================


async def save_analysis_to_db(tweet_id: int, analysis: Dict[str, Any]) -> bool:
    """
    å°† AI åˆ†æç»“æœä¿å­˜åˆ°æ•°æ®åº“

    Args:
        tweet_id: æ¨æ–‡ ID
        analysis: full_analysis è¿”å›çš„åˆ†æç»“æœ

    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å› True
    """
    try:
        from app.core.supabase import get_supabase_service

        supabase = get_supabase_service()

        # æ„å»ºæ›´æ–°æ•°æ®
        update_data = {
            # æƒ…æ„Ÿåˆ†æ
            "ai_sentiment": analysis.get("sentiment", {}).get("sentiment"),
            "ai_sentiment_confidence": analysis.get("sentiment", {}).get("confidence"),
            "ai_sentiment_reasoning": analysis.get("sentiment", {}).get("reasoning"),
            # è‚¡ç¥¨ä»£ç å’Œæ ‡ç­¾ (JSONB)
            "ai_tickers": analysis.get("tickers", []),
            "ai_tags": analysis.get("tags", []),
            # æŠ•èµ„ä¿¡å· (JSONB)
            "ai_trading_signal": analysis.get("trading_signal"),
            # æ‘˜è¦
            "ai_summary": analysis.get("summary"),
            "ai_summary_en": analysis.get("summary_en"),
            # å…ƒæ•°æ®
            "ai_analyzed_at": analysis.get("analyzed_at"),
            "ai_model": analysis.get("model"),
        }

        # æ›´æ–°æ•°æ®åº“
        supabase.table("kol_tweets").update(update_data).eq("id", tweet_id).execute()

        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜ AI åˆ†æç»“æœå¤±è´¥ (tweet_id={tweet_id}): {e}")
        return False


async def analyze_and_save_tweet(tweet_id: int, tweet_text: str) -> Dict[str, Any]:
    """
    åˆ†ææ¨æ–‡å¹¶ä¿å­˜åˆ°æ•°æ®åº“

    Args:
        tweet_id: æ¨æ–‡ ID
        tweet_text: æ¨æ–‡æ–‡æœ¬

    Returns:
        Dict: åˆ†æç»“æœ
    """
    async with OllamaClient() as client:
        analyzer = TweetAnalyzer(client)
        analysis = await analyzer.full_analysis(tweet_text)

        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_analysis_to_db(tweet_id, analysis)

        return analysis


async def batch_analyze_tweets(limit: int = 10) -> Dict[str, Any]:
    """
    æ‰¹é‡åˆ†ææœªå¤„ç†çš„æ¨æ–‡

    Args:
        limit: æ¯æ‰¹å¤„ç†çš„æ¨æ–‡æ•°é‡

    Returns:
        Dict: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from app.core.supabase import get_supabase_service

        supabase = get_supabase_service()

        # æŸ¥è¯¢æœªåˆ†æçš„æ¨æ–‡
        result = (
            supabase.table("kol_tweets")
            .select("id, tweet_text")
            .is_("ai_analyzed_at", "null")
            .order("created_at", desc=True)
            .limit(limit)
            .execute()
        )

        tweets = result.data
        if not tweets:
            return {
                "processed": 0,
                "success": 0,
                "failed": 0,
                "message": "æ²¡æœ‰å¾…åˆ†æçš„æ¨æ–‡",
            }

        stats = {"processed": 0, "success": 0, "failed": 0, "results": []}

        async with OllamaClient() as client:
            analyzer = TweetAnalyzer(client)

            for tweet in tweets:
                tweet_id = tweet["id"]
                tweet_text = tweet["tweet_text"]

                try:
                    print(f"ğŸ” åˆ†ææ¨æ–‡ #{tweet_id}: {tweet_text[:50]}...")

                    analysis = await analyzer.full_analysis(tweet_text)
                    saved = await save_analysis_to_db(tweet_id, analysis)

                    stats["processed"] += 1
                    if saved:
                        stats["success"] += 1
                        stats["results"].append(
                            {
                                "tweet_id": tweet_id,
                                "sentiment": analysis.get("sentiment", {}).get(
                                    "sentiment"
                                ),
                                "tickers": analysis.get("tickers", []),
                                "success": True,
                            }
                        )
                        print(
                            f"   âœ… æƒ…æ„Ÿ: {analysis.get('sentiment', {}).get('sentiment')} | "
                            f"è‚¡ç¥¨: {analysis.get('tickers', [])}"
                        )
                    else:
                        stats["failed"] += 1
                        stats["results"].append(
                            {
                                "tweet_id": tweet_id,
                                "success": False,
                                "error": "ä¿å­˜å¤±è´¥",
                            }
                        )

                except Exception as e:
                    stats["processed"] += 1
                    stats["failed"] += 1
                    stats["results"].append(
                        {"tweet_id": tweet_id, "success": False, "error": str(e)}
                    )
                    print(f"   âŒ åˆ†æå¤±è´¥: {e}")

        return stats

    except Exception as e:
        return {"processed": 0, "success": 0, "failed": 0, "error": str(e)}


# ============================================================
# åŒæ­¥ç‰ˆæœ¬ (ç”¨äºéå¼‚æ­¥ç¯å¢ƒ)
# ============================================================


class OllamaClientSync:
    """åŒæ­¥ç‰ˆæœ¬çš„ Ollama å®¢æˆ·ç«¯"""

    def __init__(
        self, base_url: str = None, model: str = None, timeout: float = REQUEST_TIMEOUT
    ):
        self.base_url = (base_url or OLLAMA_BASE_URL).rstrip("/")
        self.model = model or DEFAULT_MODEL
        self.timeout = timeout

    def generate(
        self,
        prompt: str,
        model: str = None,
        system: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """åŒæ­¥ç”Ÿæˆæ–‡æœ¬"""
        url = f"{self.base_url}/api/generate"

        payload = {
            "model": model or self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        if system:
            payload["system"] = system

        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            return data.get("response", "")

    def chat(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """åŒæ­¥èŠå¤©"""
        url = f"{self.base_url}/api/chat"

        payload = {
            "model": model or self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            return data.get("message", {}).get("content", "")

    def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            with httpx.Client(timeout=10.0) as client:
                response = client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except Exception:
            return False


# ============================================================
# æµ‹è¯•
# ============================================================


if __name__ == "__main__":
    import asyncio

    async def test():
        print("ğŸ” æµ‹è¯• Ollama API...")
        print(f"ğŸ“¡ Base URL: {OLLAMA_BASE_URL}")
        print(f"ğŸ¤– Model: {DEFAULT_MODEL}")
        print()

        async with OllamaClient() as client:
            # å¥åº·æ£€æŸ¥
            print("1ï¸âƒ£ å¥åº·æ£€æŸ¥...")
            is_healthy = await client.health_check()
            print(f"   âœ… API çŠ¶æ€: {'æ­£å¸¸' if is_healthy else 'å¼‚å¸¸'}")

            if not is_healthy:
                print("   âŒ API ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥è¿æ¥")
                return

            # åˆ—å‡ºæ¨¡å‹
            print("\n2ï¸âƒ£ å¯ç”¨æ¨¡å‹...")
            models = await client.list_models()
            for m in models:
                print(f"   - {m.get('name', 'unknown')}")

            # æµ‹è¯•ç”Ÿæˆ
            print("\n3ï¸âƒ£ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
            response = await client.generate(
                prompt="Say hello in 5 words or less", temperature=0.5, max_tokens=20
            )
            print(f"   Response: {response}")

            # æµ‹è¯•æ¨æ–‡åˆ†æ
            print("\n4ï¸âƒ£ æµ‹è¯•æ¨æ–‡åˆ†æ...")
            test_tweet = "$AAPL just smashed earnings! Revenue up 15% YoY. Tim Cook is a genius. ğŸš€ğŸ“ˆ"

            analyzer = TweetAnalyzer(client)

            print(f"   Tweet: {test_tweet}")
            print()

            # æƒ…æ„Ÿåˆ†æ
            sentiment = await analyzer.analyze_sentiment(test_tweet)
            print(f"   ğŸ“Š Sentiment: {sentiment}")

            # æå–è‚¡ç¥¨ä»£ç 
            tickers = await analyzer.extract_tickers(test_tweet)
            print(f"   ğŸ’¹ Tickers: {tickers}")

            # ç”Ÿæˆæ ‡ç­¾
            tags = await analyzer.generate_tags(test_tweet)
            print(f"   ğŸ·ï¸ Tags: {tags}")

        print("\nâœ… æµ‹è¯•å®Œæˆ!")

    asyncio.run(test())
